"""
FastAPI AI Chat Service with RAG (Retrieval-Augmented Generation)
Táº¥t cáº£ trong 1 file - Gemini 2.5 Flash + Vector Database
"""
import sys
import io
import os

# Fix Unicode encoding for Windows console - MUST BE FIRST!
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    # Also set console code page to UTF-8
    os.system('chcp 65001 >nul 2>&1')

from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ConfigDict
from typing import List, Optional, Dict
import google.generativeai as genai
import os
from dotenv import load_dotenv
import json
import math
import requests
from datetime import datetime, timedelta
try:
    from youtube_helper import search_youtube_video, get_youtube_watch_url, get_youtube_embed_url
    YOUTUBE_HELPER_AVAILABLE = True
except ImportError:
    YOUTUBE_HELPER_AVAILABLE = False
    print("âš ï¸  YouTube helper not available. Video search will use fallback.")

try:
    from groq_helper import GroqClient
    GROQ_HELPER_AVAILABLE = True
except ImportError:
    GROQ_HELPER_AVAILABLE = False
    print("âš ï¸  Groq helper not available.")

try:
    from agent_features import AgentFeatures
    AGENT_FEATURES_AVAILABLE = True
except ImportError:
    AGENT_FEATURES_AVAILABLE = False
    print("âš ï¸  Agent features not available.")

try:
    from google_cloud_agent import GoogleCloudAgent
    GOOGLE_CLOUD_AGENT_AVAILABLE = True
except ImportError:
    GOOGLE_CLOUD_AGENT_AVAILABLE = False
    print("âš ï¸  Google Cloud Agent not available.")

try:
    from document_intelligence_service import DocumentIntelligence, create_document_intelligence_service
    DOCUMENT_INTELLIGENCE_AVAILABLE = True
except ImportError:
    DOCUMENT_INTELLIGENCE_AVAILABLE = False
    print("âš ï¸  Document Intelligence not available.")

# LangChain Agent
try:
    from langchain_agent_simple import create_simple_langchain_agent, SimpleLangChainAgent
    LANGCHAIN_AGENT_AVAILABLE = True
except ImportError:
    LANGCHAIN_AGENT_AVAILABLE = False
    print("âš ï¸  LangChain Agent not available. Install: pip install langchain langchain-google-genai")

# Image analysis tools for non-vision models (Groq)
# Using OCR.space free API (25,000 requests/month)
IMAGE_OCR_AVAILABLE = True  # Always available via API
IMAGE_CAPTION_AVAILABLE = False
print("âœ… OCR.space API available for Groq image reading")

# ============================================================================
# VECTOR DATABASE CLASS
# ============================================================================

def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """TÃ­nh cosine similarity giá»¯a 2 vectors"""
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(a * a for a in vec1))
    magnitude2 = math.sqrt(sum(b * b for b in vec2))
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    
    return dot_product / (magnitude1 * magnitude2)

class SimpleVectorDB:
    def __init__(self, storage_file: str = "vector_db.json"):
        """Khá»Ÿi táº¡o Simple Vector Database"""
        self.storage_file = storage_file
        self.documents = []
        self.load()
    
    def load(self):
        """Load data tá»« file"""
        if os.path.exists(self.storage_file):
            with open(self.storage_file, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
    
    def save(self):
        """LÆ°u data vÃ o file"""
        with open(self.storage_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
    
    def add_documents(self, documents: List[str], metadatas: List[Dict] = None, ids: List[str] = None):
        """ThÃªm documents vÃ o database"""
        if ids is None:
            start_id = len(self.documents)
            ids = [f"doc_{start_id + i}" for i in range(len(documents))]
        
        if metadatas is None:
            metadatas = [{"source": "manual"} for _ in documents]
        
        # Táº¡o embeddings
        for doc, metadata, doc_id in zip(documents, metadatas, ids):
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=doc,
                task_type="retrieval_document"
            )
            
            self.documents.append({
                "id": doc_id,
                "document": doc,
                "embedding": result['embedding'],
                "metadata": metadata
            })
        
        self.save()
        return {"status": "success", "count": len(documents)}
    
    def search(self, query: str, n_results: int = 5) -> Dict:
        """TÃ¬m kiáº¿m documents tÆ°Æ¡ng tá»±"""
        if not self.documents:
            return {"documents": [], "distances": [], "metadatas": [], "ids": []}
        
        # Táº¡o embedding cho query
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=query,
            task_type="retrieval_query"
        )
        query_embedding = result['embedding']
        
        # TÃ­nh similarity vá»›i táº¥t cáº£ documents
        similarities = []
        for doc in self.documents:
            similarity = cosine_similarity(query_embedding, doc['embedding'])
            similarities.append({
                "document": doc['document'],
                "distance": 1 - similarity,
                "metadata": doc['metadata'],
                "id": doc['id'],
                "similarity": similarity
            })
        
        # Sáº¯p xáº¿p theo similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        top_results = similarities[:n_results]
        
        return {
            "documents": [r['document'] for r in top_results],
            "distances": [r['distance'] for r in top_results],
            "metadatas": [r['metadata'] for r in top_results],
            "ids": [r['id'] for r in top_results]
        }
    
    def delete_all(self):
        """XÃ³a táº¥t cáº£ documents"""
        self.documents = []
        self.save()
        return {"status": "success", "message": "All documents deleted"}
    
    def get_count(self) -> int:
        """Láº¥y sá»‘ lÆ°á»£ng documents"""
        return len(self.documents)
    
    def get_all_documents(self) -> Dict:
        """Láº¥y táº¥t cáº£ documents"""
        return {
            "documents": [doc['document'] for doc in self.documents],
            "metadatas": [doc['metadata'] for doc in self.documents],
            "ids": [doc['id'] for doc in self.documents],
            "count": len(self.documents)
        }

# ============================================================================
# FASTAPI APP SETUP
# ============================================================================

# Load environment variables
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
DEFAULT_AI_MODEL = os.getenv("DEFAULT_AI_MODEL", "gemini")

# Validate API keys
if DEFAULT_AI_MODEL == "gemini":
    if not GEMINI_API_KEY or GEMINI_API_KEY == "your_gemini_api_key_here":
        raise ValueError("âš ï¸  GEMINI_API_KEY khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y trong file .env\nLáº¥y API key táº¡i: https://aistudio.google.com/apikey")
    genai.configure(api_key=GEMINI_API_KEY)
    print(f"âœ… Using Gemini AI")
elif DEFAULT_AI_MODEL == "groq":
    if not GROQ_API_KEY or GROQ_API_KEY == "your_groq_api_key_here":
        raise ValueError("âš ï¸  GROQ_API_KEY khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y trong file .env\nLáº¥y API key táº¡i: https://console.groq.com/")
    print(f"âœ… Using Groq AI")
else:
    # Fallback to Gemini if not specified
    if GEMINI_API_KEY and GEMINI_API_KEY != "your_gemini_api_key_here":
        genai.configure(api_key=GEMINI_API_KEY)
        print(f"âš ï¸  Invalid DEFAULT_AI_MODEL, falling back to Gemini")
    else:
        raise ValueError(f"âš ï¸  No valid API key found")

# Initialize AI clients
groq_client = None
if GROQ_HELPER_AVAILABLE and GROQ_API_KEY and GROQ_API_KEY != "your_groq_api_key_here":
    groq_client = GroqClient(GROQ_API_KEY)
    print("âœ… Groq client initialized")

# Initialize FastAPI app
app = FastAPI(
    title="AI Chat Service with RAG",
    description="API chat vá»›i Gemini 2.5 Flash + Vector Database RAG",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Vector Database
vector_db = SimpleVectorDB(storage_file="knowledge_base.json")

# Initialize Agent Features
if AGENT_FEATURES_AVAILABLE:
    agent_features = AgentFeatures(spring_boot_url="http://localhost:8080")
    print("âœ… Agent Features initialized")
else:
    agent_features = None
    print("âš ï¸  Agent Features not initialized")

# Initialize Google Cloud Agent
if GOOGLE_CLOUD_AGENT_AVAILABLE:
    google_cloud_agent = GoogleCloudAgent(google_cloud_url="http://localhost:8004")
    print("âœ… Google Cloud Agent initialized")
else:
    google_cloud_agent = None
    print("âš ï¸  Google Cloud Agent not initialized")

# Initialize LangChain Agent - DISABLED (khÃ´ng cáº§n thiáº¿t cho dá»± Ã¡n nÃ y)
# Káº¿t luáº­n: LangChain phá»©c táº¡p 8/10, giÃ¡ trá»‹ thá»±c táº¿ tháº¥p
langchain_agent = None
print("â„¹ï¸  LangChain Agent disabled - using direct Gemini API instead")

# Initialize Image Analysis models (lazy loading)
# No longer using EasyOCR or BLIP - using pytesseract instead

def extract_image_content(image_base64: str, image_mime_type: str) -> Dict[str, str]:
    """
    Extract text from image using OCR.space API
    Also provides basic image description for non-text images
    Returns: {
        "description": "Basic image info",
        "text_content": "Extracted text from image",
        "success": True/False
    }
    """
    try:
        import base64
        from PIL import Image
        import io
        
        # Decode image
        image_data = base64.b64decode(image_base64)
        image = Image.open(io.BytesIO(image_data))
        
        # Get image info
        width, height = image.size
        img_format = image.format or "Unknown"
        mode = image.mode  # RGB, RGBA, L (grayscale), etc.
        
        result = {
            "description": f"áº¢nh {img_format}, kÃ­ch thÆ°á»›c {width}x{height} pixels, mode: {mode}",
            "text_content": "",
            "success": False
        }
        
        # Use OCR.space free API with Vietnamese support
        try:
            print("ğŸ” Using OCR.space API for text extraction...")
            import requests
            
            ocr_url = "https://api.ocr.space/parse/image"
            
            # Try Vietnamese first, then English
            for lang in ['vie', 'eng']:
                print(f"   Trying language: {lang}")
                payload = {
                    'base64Image': f'data:{image_mime_type};base64,{image_base64}',
                    'language': lang,
                    'isOverlayRequired': False,
                    'detectOrientation': True,
                    'scale': True,
                    'OCREngine': 2  # Engine 2 for better accuracy
                }
                
                response = requests.post(ocr_url, data=payload, timeout=30)
                ocr_result = response.json()
                
                # Check for processing error
                is_error = ocr_result.get('IsErroredOnProcessing', False)
                if is_error:
                    error_msg = ocr_result.get('ErrorMessage', 'Unknown error')
                    # ErrorMessage can be string or list
                    if isinstance(error_msg, list):
                        error_msg = error_msg[0] if error_msg else 'Unknown error'
                    elif not isinstance(error_msg, str):
                        error_msg = str(error_msg)
                    print(f"   âš ï¸ OCR error ({lang}): {error_msg}")
                    continue
                
                # Extract text from all parsed results
                text_parts = []
                parsed_results = ocr_result.get('ParsedResults', [])
                
                # Ensure parsed_results is a list
                if not isinstance(parsed_results, list):
                    print(f"   âš ï¸ ParsedResults is not a list: {type(parsed_results)}")
                    continue
                
                for parsed_result in parsed_results:
                    # Ensure parsed_result is a dict
                    if not isinstance(parsed_result, dict):
                        continue
                    text = parsed_result.get('ParsedText', '').strip()
                    if text:
                        text_parts.append(text)
                
                full_text = '\n'.join(text_parts)
                
                if full_text and len(full_text) > 5:  # At least some meaningful text
                    result["text_content"] = full_text
                    result["success"] = True
                    print(f"âœ… OCR extracted {len(full_text)} characters ({lang})")
                    break  # Found text, stop trying other languages
            
            # If no text found, provide helpful context
            if not result["success"] or not result["text_content"]:
                result["text_content"] = f"""[KhÃ´ng tÃ¬m tháº¥y text trong áº£nh]

ThÃ´ng tin áº£nh:
- Äá»‹nh dáº¡ng: {img_format}
- KÃ­ch thÆ°á»›c: {width}x{height} pixels
- Cháº¿ Ä‘á»™ mÃ u: {mode}

LÆ°u Ã½: Groq khÃ´ng thá»ƒ phÃ¢n tÃ­ch ná»™i dung hÃ¬nh áº£nh (chá»‰ Ä‘á»c Ä‘Æ°á»£c text).
Náº¿u báº¡n cáº§n phÃ¢n tÃ­ch áº£nh chi tiáº¿t, vui lÃ²ng chuyá»ƒn sang Gemini."""
                result["success"] = True  # Still return success so we can respond
                print(f"â„¹ï¸ No text found in image, returning image info")
                
        except requests.exceptions.Timeout:
            print(f"âš ï¸ OCR timeout")
            result["text_content"] = "[OCR timeout - vui lÃ²ng thá»­ láº¡i]"
        except Exception as e:
            print(f"âš ï¸ OCR error: {e}")
            result["text_content"] = f"[OCR error: {str(e)[:100]}]"
        
        return result
        
    except Exception as e:
        print(f"âŒ Image extraction error: {e}")
        return {
            "description": "Error processing image",
            "text_content": f"[Error: {str(e)[:100]}]",
            "success": False,
            "error": str(e)
        }

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_user_id_from_token(token: str) -> Optional[int]:
    """
    Get user_id from JWT token by calling Spring Boot API
    
    Args:
        token: JWT token string
    
    Returns:
        user_id (int) or None if failed
    """
    if not token:
        return None
    
    try:
        # Call Spring Boot API to get user profile
        headers = {"Authorization": f"Bearer {token}"}
        response = requests.get(
            "http://localhost:8080/api/auth/profile",
            headers=headers,
            timeout=5
        )
        
        if response.status_code == 200:
            user_data = response.json()
            user_id = user_data.get('id')
            print(f"âœ… Got user_id from token: {user_id}")
            return user_id
        else:
            print(f"âš ï¸  Failed to get user from token: {response.status_code}")
            return None
    except Exception as e:
        print(f"âŒ Error getting user_id from token: {e}")
        return None

# ============================================================================
# PYDANTIC MODELS
# ============================================================================

class ChatRequest(BaseModel):
    message: str
    model: str = "gemini-flash-latest"  # Use latest flash model (1,500 requests/day)
    ai_provider: str = "gemini"  # "gemini" or "groq"
    use_rag: bool = True
    image_base64: Optional[str] = None  # Base64 encoded image for vision analysis
    image_mime_type: Optional[str] = None  # e.g., "image/jpeg", "image/png"
    session_id: Optional[int] = None  # Chat session ID for conversation context
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "message": "Giáº£i thÃ­ch vá» AI lÃ  gÃ¬?",
                "model": "gemini-2.5-flash",
                "ai_provider": "gemini",
                "use_rag": True,
                "image_base64": None,
                "image_mime_type": None,
                "session_id": None
            }
        }
    )

class ActionLink(BaseModel):
    type: str  # "youtube", "google", "wikipedia"
    url: str
    title: str
    icon: str

class ToolAction(BaseModel):
    tool: str  # "play_youtube", "search_youtube", "search_google", "open_wikipedia"
    query: str
    url: str
    auto_execute: bool = True
    video_id: Optional[str] = None  # YouTube video ID
    embed_url: Optional[str] = None  # URL Ä‘á»ƒ embed video

class SendEmailRequest(BaseModel):
    """Request model for sending email after user confirmation"""
    to: str
    subject: str
    body: str
    user_id: Optional[int] = None
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "to": "teacher@tvu.edu.vn",
                "subject": "Xin nghá»‰ há»c",
                "body": "KÃ­nh gá»­i tháº§y, em xin phÃ©p nghá»‰ há»c...",
                "user_id": 1
            }
        }
    )

class EmailDraft(BaseModel):
    """Email draft for preview"""
    to: str
    subject: str
    body: str
    user_id: Optional[int] = None
    
    model_config = ConfigDict(
        populate_by_name=True,
        # Ensure snake_case in JSON output
    )

class ChatResponse(BaseModel):
    response: str
    model: str
    context_used: Optional[List[str]] = None
    rag_enabled: bool = False
    suggested_actions: Optional[List[ActionLink]] = None  # Links gá»£i Ã½
    tool_action: Optional[ToolAction] = None  # Action tá»± Ä‘á»™ng thá»±c thi
    email_draft: Optional[EmailDraft] = None  # Email draft for preview
    
    model_config = ConfigDict(
        populate_by_name=True,
        # Ensure snake_case in JSON output
    )

class DocumentRequest(BaseModel):
    documents: List[str]
    metadatas: Optional[List[dict]] = None
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "documents": [
                    "AI (Artificial Intelligence) lÃ  trÃ­ tuá»‡ nhÃ¢n táº¡o.",
                    "Machine Learning lÃ  má»™t nhÃ¡nh cá»§a AI."
                ]
            }
        }
    )

class PromptRAGRequest(BaseModel):
    prompt: str
    category: Optional[str] = "general"
    tags: Optional[List[str]] = None
    
    model_config = ConfigDict(
        json_schema_extra={
            "examples": [
                {
                    "prompt": "Python lÃ  ngÃ´n ngá»¯ láº­p trÃ¬nh dá»… há»c vÃ  máº¡nh máº½.",
                    "category": "programming",
                    "tags": ["python", "programming", "ai"]
                },
                {
                    "prompt": "Machine Learning giÃºp mÃ¡y tÃ­nh há»c tá»« dá»¯ liá»‡u mÃ  khÃ´ng cáº§n láº­p trÃ¬nh cá»¥ thá»ƒ."
                }
            ]
        }
    )

class SimplePromptRequest(BaseModel):
    prompt: str
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "prompt": "FastAPI lÃ  framework Python hiá»‡n Ä‘áº¡i Ä‘á»ƒ xÃ¢y dá»±ng API nhanh vÃ  dá»… sá»­ dá»¥ng."
            }
        }
    )

class SearchRequest(BaseModel):
    query: str
    n_results: int = 5

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/", tags=["Health"])
async def root():
    """Health check endpoint"""
    doc_count = vector_db.get_count()
    return {
        "status": "running",
        "service": "AI Chat Service with RAG",
        "version": "2.0.0",
        "vector_db_documents": doc_count
    }

def detect_tool_intent(message: str) -> Optional[ToolAction]:
    """PhÃ¡t hiá»‡n intent Ä‘á»ƒ tá»± Ä‘á»™ng thá»±c thi tool"""
    message_lower = message.lower()
    
    # Intent: PhÃ¡t video YouTube (tá»± Ä‘á»™ng tÃ¬m vÃ  phÃ¡t)
    # Chá»‰ cáº§n cÃ³ trigger "phÃ¡t", "play", "chÆ¡i", "báº­t" lÃ  Ä‘á»§
    play_triggers = ["phÃ¡t", "play", "chÆ¡i", "báº­t"]
    
    is_play_intent = any(trigger in message_lower for trigger in play_triggers)
    
    if is_play_intent:
        # Extract query - loáº¡i bá» cÃ¡c tá»« trigger
        query = message_lower
        for trigger in play_triggers + ["má»Ÿ", "cho tÃ´i", "cho toi", "má»™t", "mot", "báº¥t ká»³", "bat ky", "video", "youtube", "táº­p"]:
            query = query.replace(trigger, "")
        query = query.strip()
        
        if query and YOUTUBE_HELPER_AVAILABLE:
            # TÃ¬m video trÃªn YouTube
            try:
                video_id = search_youtube_video(query)
                
                if video_id:
                    watch_url = get_youtube_watch_url(video_id, autoplay=True)
                    embed_url = get_youtube_embed_url(video_id, autoplay=True)
                    
                    return ToolAction(
                        tool="play_youtube",
                        query=query,
                        url=watch_url,
                        video_id=video_id,
                        embed_url=embed_url,
                        auto_execute=True
                    )
            except Exception as e:
                print(f"Error searching YouTube: {e}")
                # Fallback to search
                pass
    
    # Intent: Má»Ÿ YouTube search (khÃ´ng tá»± Ä‘á»™ng phÃ¡t)
    youtube_triggers = ["má»Ÿ video", "xem video", "open video", "show video", "youtube", "tÃ¬m video"]
    for trigger in youtube_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").replace("vá»", "").strip()
            if query:
                youtube_url = f"https://www.youtube.com/results?search_query={query.replace(' ', '+')}"
                return ToolAction(
                    tool="search_youtube",
                    query=query,
                    url=youtube_url,
                    auto_execute=True
                )
    
    # Intent: Search Google
    google_triggers = ["tÃ¬m kiáº¿m", "search", "google", "tra google", "tÃ¬m trÃªn google"]
    for trigger in google_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").strip()
            if query:
                google_url = f"https://www.google.com/search?q={query.replace(' ', '+')}"
                return ToolAction(
                    tool="search_google",
                    query=query,
                    url=google_url,
                    auto_execute=True
                )
    
    # Intent: Open Wikipedia
    wiki_triggers = ["wikipedia", "wiki", "tra wikipedia"]
    for trigger in wiki_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").strip()
            if query:
                wiki_url = f"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}"
                return ToolAction(
                    tool="open_wikipedia",
                    query=query,
                    url=wiki_url,
                    auto_execute=True
                )
    
    return None


# ============================================================================
# TEST ENDPOINT - TVU Schedule Direct
# ============================================================================
class TVUTestRequest(BaseModel):
    mssv: str
    password: str
    message: str = "HÃ´m nay tÃ´i há»c gÃ¬?"

@app.post("/api/test/tvu-schedule", tags=["Test"])
async def test_tvu_schedule(request: TVUTestRequest):
    """
    Test endpoint - Láº¥y TKB trá»±c tiáº¿p tá»« TVU (khÃ´ng cáº§n Ä‘Äƒng nháº­p há»‡ thá»‘ng)
    """
    try:
        from tvu_scraper import TVUScraper
        from datetime import datetime, timedelta
        import re
        
        scraper = TVUScraper()
        
        # Login
        if not scraper.login(request.mssv, request.password):
            return {"success": False, "message": "âŒ ÄÄƒng nháº­p TVU tháº¥t báº¡i!"}
        
        # Get schedule
        schedules = scraper.get_schedule()
        
        if not schedules:
            return {"success": False, "message": "ğŸ“… KhÃ´ng tÃ¬m tháº¥y lá»‹ch há»c tuáº§n nÃ y."}
        
        # Filter by day if message mentions specific day
        message_lower = request.message.lower()
        today = datetime.now()
        day_map = {
            'thá»© 2': 'MONDAY', 'thá»© hai': 'MONDAY', 't2': 'MONDAY',
            'thá»© 3': 'TUESDAY', 'thá»© ba': 'TUESDAY', 't3': 'TUESDAY',
            'thá»© 4': 'WEDNESDAY', 'thá»© tÆ°': 'WEDNESDAY', 't4': 'WEDNESDAY',
            'thá»© 5': 'THURSDAY', 'thá»© nÄƒm': 'THURSDAY', 't5': 'THURSDAY',
            'thá»© 6': 'FRIDAY', 'thá»© sÃ¡u': 'FRIDAY', 't6': 'FRIDAY',
            'thá»© 7': 'SATURDAY', 'thá»© báº£y': 'SATURDAY', 't7': 'SATURDAY',
            'chá»§ nháº­t': 'SUNDAY', 'cn': 'SUNDAY'
        }
        
        # Check for relative dates
        target_day = None
        day_label = "tuáº§n nÃ y"
        
        # Try to extract specific date first (DD/MM/YYYY or DD-MM-YYYY)
        date_pattern = r'(?:ngÃ y\s+)?(\d{1,2})[/-](\d{1,2})[/-](\d{4})'
        date_match = re.search(date_pattern, message_lower)
        if date_match:
            try:
                day, month, year = int(date_match.group(1)), int(date_match.group(2)), int(date_match.group(3))
                target_date = datetime(year, month, day)
                target_day = target_date.strftime('%A').upper()
                date_str = target_date.strftime('%d/%m/%Y')
                day_name = target_date.strftime('%A')
                
                # Map to Vietnamese day name
                day_names = {
                    'Monday': 'Thá»© 2',
                    'Tuesday': 'Thá»© 3',
                    'Wednesday': 'Thá»© 4',
                    'Thursday': 'Thá»© 5',
                    'Friday': 'Thá»© 6',
                    'Saturday': 'Thá»© 7',
                    'Sunday': 'Chá»§ nháº­t'
                }
                vn_day = day_names.get(day_name, day_name)
                day_label = f"{vn_day} ({date_str})"
            except (ValueError, OverflowError):
                pass
        
        # HÃ´m qua
        if target_day is None and ('hÃ´m qua' in message_lower or 'hom qua' in message_lower):
            yesterday = today - timedelta(days=1)
            target_day = yesterday.strftime('%A').upper()
            date_str = yesterday.strftime('%d/%m/%Y')
            day_label = f"hÃ´m qua ({date_str})"
        # Mai
        elif target_day is None and 'mai' in message_lower:
            tomorrow = today + timedelta(days=1)
            target_day = tomorrow.strftime('%A').upper()
            date_str = tomorrow.strftime('%d/%m/%Y')
            day_label = f"mai ({date_str})"
        # Má»‘t (2 ngÃ y sau)
        elif target_day is None and ('má»‘t' in message_lower or 'mot' in message_lower):
            two_days = today + timedelta(days=2)
            target_day = two_days.strftime('%A').upper()
            date_str = two_days.strftime('%d/%m/%Y')
            day_label = f"má»‘t ({date_str})"
        # Kia (3 ngÃ y sau)
        elif target_day is None and 'kia' in message_lower:
            three_days = today + timedelta(days=3)
            target_day = three_days.strftime('%A').upper()
            date_str = three_days.strftime('%d/%m/%Y')
            day_label = f"kia ({date_str})"
        # HÃ´m nay
        elif target_day is None and ('hÃ´m nay' in message_lower or 'hom nay' in message_lower or 'today' in message_lower or 'hnay' in message_lower):
            target_day = today.strftime('%A').upper()
            date_str = today.strftime('%d/%m/%Y')
            day_label = f"hÃ´m nay ({date_str})"
        elif target_day is None:
            # Check for specific day name
            for keyword, day in day_map.items():
                if keyword in message_lower:
                    target_day = day
                    day_label = keyword
                    break
        
        # Filter schedules by target day
        if target_day:
            schedules = [s for s in schedules if s.get('day_of_week') == target_day]
        
        if not schedules:
            return {
                "success": True,
                "message": f"ğŸ“… {day_label.capitalize()} báº¡n khÃ´ng cÃ³ lá»›p nÃ o.",
                "schedules": []
            }
        
        # Format response
        message_text = f"ğŸ“… **Lá»‹ch há»c {day_label}:**\n\n"
        for schedule in schedules:
            day_vn = {
                'MONDAY': 'Thá»© 2', 'TUESDAY': 'Thá»© 3', 'WEDNESDAY': 'Thá»© 4',
                'THURSDAY': 'Thá»© 5', 'FRIDAY': 'Thá»© 6', 'SATURDAY': 'Thá»© 7', 'SUNDAY': 'CN'
            }.get(schedule.get('day_of_week', ''), '')
            
            start_time = schedule.get('start_time', '')[:5]
            end_time = schedule.get('end_time', '')[:5]
            
            message_text += f"ğŸ• **{start_time} - {end_time}** ({day_vn})\n"
            message_text += f"   ğŸ“š {schedule.get('subject', 'N/A')}\n"
            message_text += f"   ğŸ« PhÃ²ng {schedule.get('room', 'N/A')}\n"
            if schedule.get('teacher'):
                message_text += f"   ğŸ‘¨â€ğŸ« {schedule['teacher']}\n"
            message_text += "\n"
        
        return {
            "success": True,
            "message": message_text,
            "schedules": schedules
        }
        
    except Exception as e:
        return {"success": False, "message": f"âŒ Lá»—i: {str(e)}"}


@app.post("/api/chat", tags=["Chat"])
async def chat(request: ChatRequest, authorization: Optional[str] = Header(None)):
    """
    Chat vá»›i Gemini AI (cÃ³ há»— trá»£ RAG + Agent Features + Conversation Memory)
    
    - **message**: Tin nháº¯n cá»§a ngÆ°á»i dÃ¹ng
    - **model**: Model Gemini sá»­ dá»¥ng (máº·c Ä‘á»‹nh: gemini-2.5-flash)
    - **use_rag**: Sá»­ dá»¥ng RAG Ä‘á»ƒ tÄƒng cÆ°á»ng context (máº·c Ä‘á»‹nh: true)
    - **session_id**: ID cá»§a chat session Ä‘á»ƒ load conversation history (optional)
    
    Agent Features (tá»± Ä‘á»™ng):
    - Xem thá»i khÃ³a biá»ƒu (tá»± Ä‘á»™ng láº¥y tá»« trang trÆ°á»ng)
    - Xem Ä‘iá»ƒm sá»‘
    - Gá»­i email
    
    Conversation Memory:
    - Náº¿u cÃ³ session_id, AI sáº½ nhá»› toÃ n bá»™ context cá»§a phiÃªn chat
    - Giá»‘ng nhÆ° ChatGPT - khÃ´ng cáº§n láº·p láº¡i thÃ´ng tin
    
    Models Ä‘Æ°á»£c khuyáº¿n nghá»‹:
    - gemini-2.5-flash (Má»šI NHáº¤T - Nhanh, stable)
    - gemini-2.5-pro (Máº¡nh nháº¥t)
    - gemini-flash-latest (LuÃ´n dÃ¹ng version má»›i nháº¥t)
    """
    try:
        # Extract token from Authorization header
        token = None
        user_id = None
        if authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            # Get user_id from token
            user_id = get_user_id_from_token(token)
        
        print(f"\n{'='*60}")
        print(f"ğŸ“¨ NEW CHAT REQUEST")
        print(f"Message: {request.message}")
        print(f"Session ID: {request.session_id}")
        print(f"AI Provider: {request.ai_provider}")
        print(f"Has token: {token is not None}")
        print(f"User ID: {user_id}")
        print(f"AGENT_FEATURES_AVAILABLE: {AGENT_FEATURES_AVAILABLE}")
        print(f"agent_features: {agent_features is not None if 'agent_features' in globals() else 'NOT DEFINED'}")
        
        # Debug email intent detection
        if AGENT_FEATURES_AVAILABLE and agent_features:
            email_intent = agent_features.detect_email_intent(request.message)
            gmail_send_intent = agent_features.detect_gmail_send_intent(request.message)
            print(f"ğŸ” Email Intent: {email_intent}")
            print(f"ğŸ” Gmail Send Intent: {gmail_send_intent}")
        
        print(f"{'='*60}\n")
        conversation_history = []
        if request.session_id:
            try:
                print(f"ğŸ’¬ Loading conversation history for session {request.session_id}...")
                # Call Spring Boot INTERNAL API (no auth required)
                history_response = requests.get(
                    f"http://localhost:8080/api/chat/internal/sessions/{request.session_id}/messages",
                    timeout=5
                )
                
                if history_response.status_code == 200:
                    messages = history_response.json()
                    # Take last 10 messages for context (5 exchanges)
                    recent_messages = messages[-10:] if len(messages) > 10 else messages
                    
                    for msg in recent_messages:
                        role = "user" if msg["sender"] == "USER" else "assistant"
                        conversation_history.append({
                            "role": role,
                            "content": msg["message"]
                        })
                    
                    print(f"âœ… Loaded {len(conversation_history)} messages from session history")
                else:
                    print(f"âš ï¸ Could not load session history: {history_response.status_code}")
            except Exception as e:
                print(f"âš ï¸ Error loading conversation history: {e}")
                # Continue without history - not critical
        
        # ===== DECISION TREE: IMAGE vs AGENTS vs TOOLS =====
        # Priority: Image > Google Cloud Agent > Agent Features > Tools > Normal chat
        
        has_image_input = bool(request.image_base64 and request.image_mime_type)
        
        if has_image_input:
            # ===== HIGHEST PRIORITY: IMAGE VISION =====
            print(f"ğŸ–¼ï¸ IMAGE DETECTED - Skipping ALL agent features!")
            print(f"   MIME type: {request.image_mime_type}")
            print(f"   Base64 length: {len(request.image_base64)}")
            print(f"   Jumping directly to Vision AI processing...")
            # Skip everything, go to vision processing at ~line 900
            
        elif GOOGLE_CLOUD_AGENT_AVAILABLE and google_cloud_agent:
            # Check for Google Cloud intents
            gc_result = google_cloud_agent.handle_google_cloud_request(
                message=request.message,
                token=token or "",
                image_url=None,  # TODO: Extract from message if available
                audio_base64=None  # TODO: Extract from message if available
            )
            
            if gc_result:
                print(f"ğŸŒ Google Cloud intent detected and handled")
                # Safely convert to string
                response_text = gc_result.get('message', '')
                if not isinstance(response_text, str):
                    response_text = str(response_text) if not isinstance(response_text, list) else '\n'.join(str(x) for x in response_text)
                
                return ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False
                ).model_dump()
        
        # AGENT FEATURES - Check intents (schedule, grades, email)
        # ONLY run if we haven't returned yet (no Google Cloud intent) AND no image
        if not has_image_input and AGENT_FEATURES_AVAILABLE and agent_features:
            # ===== CHECK EMAIL INTENT FIRST (cao nháº¥t) =====
            # Email patterns ráº¥t cá»¥ thá»ƒ nÃªn Æ°u tiÃªn trÆ°á»›c
            # Email draft generation KHÃ”NG cáº§n token
            if agent_features.detect_email_intent(request.message):
                print(f"âœ… ğŸ“§ Detected email intent in: {request.message}")
                print(f"Token: {token is not None}, User ID: {user_id}")
                
                # Always use handle_gmail_send for send intent - it handles both auth and no-auth cases
                if agent_features.detect_gmail_send_intent(request.message):
                    print(f"ğŸ“§ Detected SEND intent - calling handle_gmail_send with user_id: {user_id}")
                    result = agent_features.handle_gmail_send(request.message, token or "", user_id=user_id)
                elif token and user_id:
                    # For read/search - need authentication
                    if agent_features.detect_gmail_read_intent(request.message):
                        print(f"ğŸ“§ Using Gmail OAuth API for READ - User ID: {user_id}")
                        result = agent_features.handle_gmail_read(request.message, token, user_id=user_id)
                    elif agent_features.detect_gmail_search_intent(request.message):
                        print(f"ğŸ“§ Using Gmail OAuth API for SEARCH - User ID: {user_id}")
                        result = agent_features.handle_gmail_search(request.message, token, user_id=user_id)
                    else:
                        result = agent_features.handle_gmail_request(request.message, token, user_id=user_id)
                else:
                    result = {
                        "success": False,
                        "message": "ğŸ“§ Vui lÃ²ng cung cáº¥p Ä‘á»‹a chá»‰ email ngÆ°á»i nháº­n trong cÃ¢u lá»‡nh.\n\nVÃ­ dá»¥: 'gá»­i mail xin nghá»‰ há»c Ä‘áº¿n teacher@tvu.edu.vn'"
                    }
                
                # Safely convert result['message'] to string
                response_text = result.get('message', '')
                if not isinstance(response_text, str):
                    if isinstance(response_text, list):
                        response_text = '\n'.join(str(item) for item in response_text)
                    else:
                        response_text = str(response_text)
                
                # Extract email_draft if present
                email_draft_data = result.get('email_draft')
                email_draft = None
                if email_draft_data:
                    print(f"âœ… Email draft found: {email_draft_data}")
                    email_draft = EmailDraft(**email_draft_data)
                    print(f"âœ… EmailDraft object created: {email_draft}")
                    print(f"âœ… EmailDraft dict: {email_draft.model_dump()}")
                else:
                    print(f"âš ï¸ No email_draft in result. Result keys: {result.keys()}")
                
                chat_response = ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False,
                    email_draft=email_draft
                )
                print(f"ğŸ“§ ChatResponse created with email_draft: {chat_response.email_draft is not None}")
                
                # Serialize to dict to ensure email_draft is included
                response_dict = chat_response.model_dump()
                print(f"ğŸ“§ ChatResponse dict: {response_dict}")
                print(f"ğŸ“§ email_draft in dict: {response_dict.get('email_draft')}")
                
                # Ensure email_draft is in response even if None
                if 'email_draft' not in response_dict:
                    response_dict['email_draft'] = None
                    print(f"âš ï¸ Added email_draft=None to response_dict")
                
                return response_dict
            
            # ===== CHECK SCHEDULE INTENT ===== (Cáº¦N token)
            # Check for schedule intent
            if token and agent_features.detect_schedule_intent(request.message):
                print(f"ğŸ“… Detected schedule intent in: {request.message}")
                result = agent_features.get_schedule(token, message=request.message, force_sync=False)
                
                # Safely convert to string
                response_text = result.get('message', '')
                if not isinstance(response_text, str):
                    response_text = str(response_text) if not isinstance(response_text, list) else '\n'.join(str(x) for x in response_text)
                
                return ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False
                ).model_dump()
            
            # ===== CHECK CALENDAR SYNC INTENT ===== (Cáº¦N token + user_id)
            # Check for calendar sync intent
            if token and user_id and agent_features.detect_calendar_sync_intent(request.message):
                print(f"ğŸ”„ Detected calendar sync intent in: {request.message}")
                result = agent_features.sync_schedule_to_calendar(
                    token=token,
                    user_id=user_id,
                    week=None,  # Use current week
                    hoc_ky=None  # Use current semester
                )
                
                # Safely convert to string
                response_text = result.get('message', '')
                if not isinstance(response_text, str):
                    response_text = str(response_text) if not isinstance(response_text, list) else '\n'.join(str(x) for x in response_text)
                
                return ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False
                ).model_dump()
            
            # ===== CHECK GRADE INTENT ===== (Cáº¦N token)
            # Check for grade intent
            if token and agent_features.detect_grade_intent(request.message):
                print(f"ğŸ“Š Detected grade intent in: {request.message}")
                result = agent_features.get_grades(token)
                
                # Safely convert to string
                response_text = result.get('message', '')
                if not isinstance(response_text, str):
                    response_text = str(response_text) if not isinstance(response_text, list) else '\n'.join(str(x) for x in response_text)
                
                return ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False
                ).model_dump()
        
        # Detect tool action (YouTube, Google, Wikipedia) - ONLY if NO image
        tool_action = None
        if not has_image_input:
            print(f"ğŸ” Detecting tool intent for message: {request.message}")
            tool_action = detect_tool_intent(request.message)
            if tool_action:
                print(f"âœ… Tool action detected: {tool_action.tool} - {tool_action.query}")
                print(f"   URL: {tool_action.url}")
            else:
                print(f"âŒ No tool action detected")
        
        if tool_action:
            # AI xÃ¡c nháº­n action
            tool_messages = {
                "play_youtube": f"ğŸ¬ Äang phÃ¡t video YouTube vá» '{tool_action.query}'...\n\nVideo sáº½ tá»± Ä‘á»™ng phÃ¡t trong giÃ¢y lÃ¡t! ğŸ¥",
                "search_youtube": f"ğŸ¥ Äang má»Ÿ YouTube Ä‘á»ƒ xem video vá» '{tool_action.query}'...",
                "search_google": f"ğŸ” Äang tÃ¬m kiáº¿m trÃªn Google vá» '{tool_action.query}'...",
                "open_wikipedia": f"ğŸ“– Äang má»Ÿ Wikipedia vá» '{tool_action.query}'..."
            }
            
            confirmation = tool_messages.get(tool_action.tool, "Äang thá»±c hiá»‡n...")
            
            return ChatResponse(
                response=confirmation,
                model=request.model,
                tool_action=tool_action,
                rag_enabled=False
            ).model_dump()
        
        # System prompt - Personality cá»§a AI
        system_prompt = """ğŸ“ Báº¡n lÃ  AI Learning Assistant - Trá»£ lÃ½ há»c táº­p thÃ´ng minh vÃ  thÃ¢n thiá»‡n!

**Vai trÃ² cá»§a báº¡n:**
- GiÃ¡o viÃªn áº£o kiÃªn nháº«n, nhiá»‡t tÃ¬nh ğŸ‘¨â€ğŸ«
- Giáº£i thÃ­ch kiáº¿n thá»©c rÃµ rÃ ng, dá»… hiá»ƒu
- Khuyáº¿n khÃ­ch há»c sinh tÆ° duy vÃ  Ä‘áº·t cÃ¢u há»i
- LuÃ´n tÃ­ch cá»±c vÃ  Ä‘á»™ng viÃªn
- Nhá»› context cá»§a cuá»™c trÃ² chuyá»‡n (nhÆ° ChatGPT)

**Phong cÃ¡ch giao tiáº¿p:**
- ThÃ¢n thiá»‡n, gáº§n gÅ©i nhÆ° ngÆ°á»i báº¡n ğŸ˜Š
- Sá»­ dá»¥ng emoji phÃ¹ há»£p Ä‘á»ƒ sinh Ä‘á»™ng: ğŸ“š âœ¨ ğŸ’¡ ğŸ¯ âœ…
- Chia nhá» kiáº¿n thá»©c phá»©c táº¡p thÃ nh cÃ¡c pháº§n dá»… hiá»ƒu
- ÄÆ°a ra vÃ­ dá»¥ thá»±c táº¿, gáº§n gÅ©i vá»›i cuá»™c sá»‘ng

**CÃ¡ch tráº£ lá»i:**
1. TÃ³m táº¯t ngáº¯n gá»n cÃ¢u há»i (náº¿u cáº§n)
2. Giáº£i thÃ­ch chi tiáº¿t vá»›i cáº¥u trÃºc rÃµ rÃ ng
3. ÄÆ°a ra 1-2 vÃ­ dá»¥ minh há»a
4. Há»i láº¡i xem cÃ²n tháº¯c máº¯c gÃ¬ khÃ´ng

**LÆ°u Ã½:**
- Náº¿u khÃ´ng cháº¯c cháº¯n, hÃ£y thá»«a nháº­n vÃ  Ä‘á» xuáº¥t tÃ¬m hiá»ƒu thÃªm
- Khuyáº¿n khÃ­ch há»c sinh tá»± suy nghÄ© trÆ°á»›c khi Ä‘Æ°a ra Ä‘Ã¡p Ã¡n
- Sá»­ dá»¥ng ngÃ´n ngá»¯ phÃ¹ há»£p vá»›i trÃ¬nh Ä‘á»™ há»c sinh
- Nhá»› thÃ´ng tin tá»« cÃ¡c tin nháº¯n trÆ°á»›c trong phiÃªn chat nÃ y
"""
        
        context_docs = []
        prompt = request.message
        
        # Build conversation context if available
        conversation_context = ""
        if conversation_history:
            print(f"ğŸ“ Building conversation context from {len(conversation_history)} messages...")
            conversation_context = "\n\n**Lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n:**\n"
            for msg in conversation_history:
                role_label = "Há»c sinh" if msg["role"] == "user" else "AI"
                conversation_context += f"{role_label}: {msg['content']}\n"
            conversation_context += "\n"
        
        # Náº¿u báº­t RAG, tÃ¬m kiáº¿m context tá»« vector DB
        if request.use_rag and vector_db.get_count() > 0:
            search_results = vector_db.search(request.message, n_results=3)
            context_docs = search_results['documents']
            
            if context_docs:
                context_text = "\n\n".join([f"ğŸ“š TÃ i liá»‡u {i+1}: {doc}" for i, doc in enumerate(context_docs)])
                prompt = f"""{system_prompt}

{conversation_context}**TÃ i liá»‡u tham kháº£o tá»« khÃ³a há»c:**
{context_text}

**CÃ¢u há»i cá»§a há»c sinh:**
{request.message}

HÃ£y tráº£ lá»i dá»±a trÃªn lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n, tÃ i liá»‡u vÃ  kiáº¿n thá»©c cá»§a báº¡n. Náº¿u tÃ i liá»‡u khÃ´ng Ä‘á»§ thÃ´ng tin, hÃ£y bá»• sung tá»« kiáº¿n thá»©c chung."""
            else:
                prompt = f"""{system_prompt}

{conversation_context}**CÃ¢u há»i cá»§a há»c sinh:**
{request.message}

HÃ£y tráº£ lá»i dá»±a trÃªn lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n vÃ  kiáº¿n thá»©c cá»§a báº¡n."""
        else:
            prompt = f"""{system_prompt}

{conversation_context}**CÃ¢u há»i cá»§a há»c sinh:**
{request.message}"""
        
        # Check if image is provided for vision analysis
        content_parts = []
        has_image = request.image_base64 and request.image_mime_type
        
        if has_image:
            # Use Gemini Vision API for image analysis
            print(f"ğŸ–¼ï¸ Image detected - using Gemini Vision API")
            print(f"   MIME type: {request.image_mime_type}")
            print(f"   Base64 length: {len(request.image_base64)}")
            
            import base64
            from PIL import Image
            import io
            
            # Decode base64 image
            image_data = base64.b64decode(request.image_base64)
            print(f"   Decoded image size: {len(image_data)} bytes")
            
            # Convert bytes to PIL Image
            image = Image.open(io.BytesIO(image_data))
            print(f"   Image format: {image.format}, Size: {image.size}")
            
            # Validate image
            if image is None or image.size[0] == 0 or image.size[1] == 0:
                raise ValueError("Invalid image: size is zero")
            
            # Create VISION-SPECIFIC prompt
            vision_prompt = f"""Báº N LÃ€ GEMINI - AI VISION MODEL Vá»šI KHáº¢ NÄ‚NG NHÃŒN THáº¤Y HÃŒNH áº¢NH!

ğŸ–¼ï¸ **THá»°C TRáº NG:** 
- Há»c sinh ÄÃƒ Gá»¬I CHO Báº N Má»˜T HÃŒNH áº¢NH
- HÃ¬nh áº£nh Ä‘ang á»Ÿ ngay phÃ­a sau tin nháº¯n nÃ y
- Báº N CÃ“ Äáº¦Y Äá»¦ KHáº¢ NÄ‚NG NHÃŒN THáº¤Y VÃ€ PHÃ‚N TÃCH áº¢NH

**TUYá»†T Äá»I KHÃ”NG ÄÆ¯á»¢C:**
âŒ NÃ³i ráº±ng báº¡n khÃ´ng thá»ƒ xem áº£nh
âŒ NÃ³i ráº±ng báº¡n chá»‰ xá»­ lÃ½ vÄƒn báº£n
âŒ YÃªu cáº§u há»c sinh mÃ´ táº£ láº¡i áº£nh
âŒ Bá» qua ná»™i dung trong áº£nh

**NHIá»†M Vá»¤ Báº®T BUá»˜C:**
1. ğŸ‘€ NHÃŒN VÃ€O áº¢NH - Báº¡n CÃ“ THá»‚ vÃ  PHáº¢I LÃ€M Ä‘iá»u nÃ y
2. ğŸ“ MÃ” Táº¢ chi tiáº¿t nhá»¯ng gÃ¬ báº¡n tháº¥y
3. ğŸ“– Äá»ŒC má»i text, sá»‘ liá»‡u, cÃ´ng thá»©c trong áº£nh
4. ğŸ’¡ TRáº¢ Lá»œI cÃ¢u há»i dá»±a trÃªn ná»™i dung áº£nh

**YÃŠU Cáº¦U/CÃ‚U Há»I Cá»¦A Há»ŒC SINH:**
{request.message if request.message.strip() else "PhÃ¢n tÃ­ch vÃ  mÃ´ táº£ chi tiáº¿t nhá»¯ng gÃ¬ báº¡n tháº¥y trong áº£nh nÃ y"}

**Báº®T Äáº¦U NGAY:** HÃ£y mÃ´ táº£ nhá»¯ng gÃ¬ báº¡n NHÃŒN THáº¤Y trong áº£nh!"""
            
            # Create content parts: text first, then image
            content_parts = [vision_prompt, image]
            
            # Check if using Groq - use Groq Vision model (llama-4-scout)
            if request.ai_provider == "groq":
                print("ğŸ–¼ï¸ Groq vá»›i áº£nh - sá»­ dá»¥ng Llama 4 Scout Vision model...")
                
                # Use Groq Vision directly - no need for OCR
                vision_prompt = f"""Báº¡n lÃ  AI Learning Assistant thÃ´ng minh vá»›i kháº£ nÄƒng nhÃ¬n vÃ  phÃ¢n tÃ­ch hÃ¬nh áº£nh.

**NHIá»†M Vá»¤:**
1. ğŸ‘€ NhÃ¬n vÃ o áº£nh vÃ  mÃ´ táº£ chi tiáº¿t nhá»¯ng gÃ¬ báº¡n tháº¥y
2. ğŸ“– Äá»c táº¥t cáº£ text, sá»‘ liá»‡u, cÃ´ng thá»©c trong áº£nh (náº¿u cÃ³)
3. ğŸ’¡ Tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn ná»™i dung áº£nh

**CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:**
{request.message if request.message.strip() else "HÃ£y phÃ¢n tÃ­ch vÃ  mÃ´ táº£ chi tiáº¿t ná»™i dung trong áº£nh nÃ y"}

**HÃ£y tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, thÃ¢n thiá»‡n vÃ  chi tiáº¿t.**"""
                
                content_parts = [vision_prompt]  # Will be handled specially for Groq
                print(f"âœ… Groq Vision prompt ready")
        else:
            content_parts = [prompt]
        
        # Generate response based on AI provider
        ai_response = ""
        actual_model = request.model
        
        print(f"ğŸ“ Chat request - ai_provider: {request.ai_provider}, model: {request.model}, groq_client: {groq_client is not None}")
        
        if request.ai_provider == "groq" and groq_client:
            # Use Groq AI with user-selected model
            try:
                # Check if we have an image - use Vision model
                if has_image:
                    print(f"ğŸ–¼ï¸ Using Groq Vision model for image analysis")
                    
                    vision_prompt = request.message if request.message.strip() else "HÃ£y phÃ¢n tÃ­ch vÃ  mÃ´ táº£ chi tiáº¿t ná»™i dung trong áº£nh nÃ y"
                    
                    ai_response = groq_client.generate_with_vision(
                        prompt=vision_prompt,
                        image_base64=request.image_base64,
                        image_mime_type=request.image_mime_type,
                        system_prompt=system_prompt,
                        model="meta-llama/llama-4-scout-17b-16e-instruct"  # Vision model
                    )
                    actual_model = "llama-4-scout-17b (Groq Vision)"
                    print(f"âœ… Groq Vision response received: {len(ai_response)} chars")
                else:
                    # Normal text generation
                    groq_model = request.model if request.model else "llama-3.3-70b-versatile"
                    # Validate it's a Groq model
                    if not any(name in groq_model.lower() for name in ['llama', 'mixtral', 'gemma', 'qwen', 'meta-llama', 'scout', 'maverick']):
                        groq_model = "llama-3.3-70b-versatile"
                        
                    print(f"ğŸš€ Using Groq model: {groq_model}")
                    
                    # Use content_parts[0] which may contain context
                    groq_final_prompt = content_parts[0] if isinstance(content_parts[0], str) else request.message
                    
                    # Debug: Check if conversation context is in prompt
                    if conversation_history:
                        print(f"ğŸ“ DEBUG: Groq prompt includes {len(conversation_history)} messages of context")
                        print(f"ğŸ“ DEBUG: Prompt preview: {groq_final_prompt[:200]}...")
                    else:
                        print(f"âš ï¸ DEBUG: No conversation history for Groq")
                    
                    ai_response = groq_client.generate_text(
                        prompt=groq_final_prompt,
                        system_prompt=system_prompt,
                        model=groq_model
                    )
                    actual_model = f"{groq_model} (Groq)"
                    print(f"âœ… Groq response received: {len(ai_response)} chars")
            except Exception as e:
                print(f"âš ï¸ Groq error: {e}, falling back to Gemini")
                import traceback
                traceback.print_exc()
                # Fallback to Gemini with default Gemini model
                gemini_model = genai.GenerativeModel("gemini-2.0-flash-exp")
                response = gemini_model.generate_content(prompt)
                ai_response = response.text
                actual_model = "gemini-2.0-flash-exp (fallback)"
        elif request.ai_provider == "groq" and not groq_client:
            print("âŒ Groq requested but groq_client not initialized! Check GROQ_API_KEY")
            # Fallback to Gemini
            gemini_model_name = "gemini-2.0-flash-exp"
            model = genai.GenerativeModel(gemini_model_name)
            response = model.generate_content(prompt)
            ai_response = response.text
            actual_model = f"{gemini_model_name} (Groq unavailable)"
        else:
            # Use Gemini (default) - ensure we use Gemini model names
            # Use vision-capable model if image is present
            if has_image:
                # Use Gemini Flash Latest - proven vision support
                gemini_model_name = "gemini-flash-latest"  # Stable vision model
                print(f"ğŸ–¼ï¸ Using vision-capable model: {gemini_model_name}")
                print(f"   Content parts: {len(content_parts)} items (text + image)")
                print(f"   Vision prompt length: {len(content_parts[0])} chars")
            else:
                gemini_model_name = request.model if 'gemini' in request.model else "gemini-2.0-flash-exp"
            
            model = genai.GenerativeModel(gemini_model_name)
            
            try:
                print(f"ğŸ“¤ Sending to Gemini...")
                response = model.generate_content(content_parts)
                ai_response = response.text
                actual_model = gemini_model_name
                print(f"âœ… Gemini response received: {len(ai_response)} chars")
                
                # Debug: Check if response mentions inability to see
                if has_image and any(word in ai_response.lower() for word in ['khÃ´ng thá»ƒ xem', 'khÃ´ng xem Ä‘Æ°á»£c', 'chá»‰ xá»­ lÃ½ vÄƒn báº£n', 'khÃ´ng nhÃ¬n tháº¥y']):
                    print(f"âš ï¸ WARNING: AI claims it cannot see image! This should not happen!")
                    print(f"   Model used: {gemini_model_name}")
                    print(f"   Content parts: {len(content_parts)}")
                    
            except Exception as e:
                error_message = str(e)
                print(f"âŒ Gemini API Error: {error_message}")
                
                # Check for quota exceeded
                if "quota" in error_message.lower() or "429" in error_message:
                    ai_response = """âš ï¸ **Gemini API Quota Exceeded**

Xin lá»—i! API key cá»§a Gemini Ä‘Ã£ vÆ°á»£t quÃ¡ giá»›i háº¡n sá»­ dá»¥ng miá»…n phÃ­.

**Giáº£i phÃ¡p:**
1. ğŸ”‘ Äá»£i 1 phÃºt vÃ  thá»­ láº¡i (rate limit reset)
2. ğŸ†• Táº¡o API key má»›i táº¡i: https://ai.google.dev/
3. ğŸ’³ Upgrade lÃªn Gemini API tráº£ phÃ­ Ä‘á»ƒ cÃ³ quota cao hÆ¡n

**ThÃ´ng tin lá»—i:** ÄÃ£ vÆ°á»£t quota requests hoáº·c tokens cho model."""
                else:
                    ai_response = f"âš ï¸ Lá»—i khi xá»­ lÃ½: {error_message[:200]}"
                    
                actual_model = f"{gemini_model_name} (error)"
        
        # Táº¡o suggested actions (YouTube, Google Search)
        suggested_actions = []
        
        # Táº¡o search query tá»« cÃ¢u há»i
        search_query = request.message.replace("?", "").strip()
        
        # YouTube link
        youtube_query = search_query.replace(" ", "+")
        suggested_actions.append(ActionLink(
            type="youtube",
            url=f"https://www.youtube.com/results?search_query={youtube_query}",
            title=f"Xem video vá»: {search_query[:50]}",
            icon="ğŸ¥"
        ))
        
        # Google Search link
        google_query = search_query.replace(" ", "+")
        suggested_actions.append(ActionLink(
            type="google",
            url=f"https://www.google.com/search?q={google_query}",
            title=f"TÃ¬m trÃªn Google: {search_query[:50]}",
            icon="ğŸ”"
        ))
        
        # Wikipedia link (náº¿u lÃ  cÃ¢u há»i vá» khÃ¡i niá»‡m)
        if any(word in request.message.lower() for word in ["lÃ  gÃ¬", "what is", "Ä‘á»‹nh nghÄ©a", "khÃ¡i niá»‡m"]):
            wiki_query = search_query.replace(" ", "_")
            suggested_actions.append(ActionLink(
                type="wikipedia",
                url=f"https://en.wikipedia.org/wiki/{wiki_query}",
                title=f"Wikipedia: {search_query[:50]}",
                icon="ğŸ“–"
            ))
        
        return ChatResponse(
            response=ai_response,
            model=actual_model,
            context_used=context_docs if request.use_rag else None,
            rag_enabled=request.use_rag,
            suggested_actions=suggested_actions
        ).model_dump()
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/api/email/send", tags=["Email"])
async def send_email_confirmed(request: SendEmailRequest, authorization: Optional[str] = Header(None)):
    """
    Send email after user confirms the draft
    
    This endpoint is called when user clicks "Send" button in email preview
    """
    try:
        print(f"ğŸ“§ /api/email/send called")
        print(f"ğŸ“§ Authorization header: {authorization[:50] if authorization else 'None'}...")
        print(f"ğŸ“§ Request user_id: {request.user_id}")
        
        # Priority: use user_id from request first, then try token
        user_id = request.user_id
        
        if not user_id and authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            print(f"ğŸ“§ Extracting user_id from token...")
            user_id = get_user_id_from_token(token)
            print(f"ğŸ“§ Got user_id from token: {user_id}")
        
        # If still no user_id, return clear error
        if not user_id:
            print(f"âŒ User not authenticated - no user_id found")
            raise HTTPException(
                status_code=401, 
                detail="KhÃ´ng thá»ƒ xÃ¡c thá»±c ngÆ°á»i dÃ¹ng. Vui lÃ²ng Ä‘Äƒng nháº­p láº¡i!"
            )
        
        print(f"âœ… Using user_id: {user_id}")
        
        # Import Gmail service
        from gmail_service import ai_send_email
        
        # Send email
        result = ai_send_email(
            user_id=user_id,
            to=request.to,
            subject=request.subject,
            body=request.body
        )
        
        if result.get('success'):
            return {
                "success": True,
                "message": f"âœ… Email Ä‘Ã£ gá»­i thÃ nh cÃ´ng tá»›i {request.to}!",
                "sent_at": datetime.now().strftime('%H:%M %d/%m/%Y')
            }
        else:
            if result.get('need_auth'):
                raise HTTPException(
                    status_code=401, 
                    detail="Cáº§n káº¿t ná»‘i Google Account trong Settings"
                )
            raise HTTPException(
                status_code=400,
                detail=result.get("error", "KhÃ´ng thá»ƒ gá»­i email")
            )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/api/rag/prompt/auto", tags=["RAG - Knowledge Base"])
async def add_prompt_auto(request: SimplePromptRequest):
    """
    ThÃªm prompt vá»›i AI tá»± Ä‘á»™ng sinh category vÃ  tags
    
    - **prompt**: Ná»™i dung kiáº¿n thá»©c cáº§n thÃªm
    
    AI sáº½ tá»± Ä‘á»™ng phÃ¢n tÃ­ch vÃ  sinh category + tags phÃ¹ há»£p
    """
    try:
        # DÃ¹ng Gemini Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  sinh metadata
        analysis_prompt = f"""PhÃ¢n tÃ­ch vÄƒn báº£n sau vÃ  tráº£ vá» JSON:

VÄƒn báº£n: "{request.prompt}"

Tráº£ vá» JSON vá»›i format:
{{
  "category": "tÃªn_danh_má»¥c",
  "tags": ["tag1", "tag2", "tag3"],
  "summary": "tÃ³m táº¯t ngáº¯n gá»n"
}}

Categories: programming, ai, machine-learning, education, science, business, health, technology, math, language, general

Chá»‰ tráº£ vá» JSON."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(analysis_prompt)
        
        # Parse JSON
        import re
        import json as json_lib
        json_text = response.text.strip()
        json_match = re.search(r'\{[^}]+\}', json_text, re.DOTALL)
        
        if json_match:
            metadata_ai = json_lib.loads(json_match.group())
            category = metadata_ai.get("category", "general")
            tags = metadata_ai.get("tags", [])
            summary = metadata_ai.get("summary", "")
        else:
            category = "general"
            tags = []
            summary = ""
        
        metadata = {
            "category": category,
            "tags": tags,
            "type": "prompt",
            "summary": summary
        }
        
        result = vector_db.add_documents(
            documents=[request.prompt],
            metadatas=[metadata]
        )
        
        return {
            "status": "success",
            "message": "ÄÃ£ thÃªm prompt vá»›i metadata tá»± Ä‘á»™ng",
            "prompt": request.prompt,
            "category": category,
            "tags": tags,
            "summary": summary,
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/api/rag/prompt", tags=["RAG - Knowledge Base"])
async def add_rag_prompt(request: PromptRAGRequest):
    """
    ThÃªm prompt/kiáº¿n thá»©c vÃ o RAG Knowledge Base
    
    - **prompt**: Ná»™i dung kiáº¿n thá»©c cáº§n thÃªm
    - **category**: Danh má»¥c (tá»± Ä‘á»™ng sinh náº¿u Ä‘á»ƒ trá»‘ng)
    - **tags**: CÃ¡c tag (tá»± Ä‘á»™ng sinh náº¿u Ä‘á»ƒ trá»‘ng)
    
    AI sáº½ tá»± Ä‘á»™ng phÃ¢n tÃ­ch vÃ  sinh category + tags náº¿u khÃ´ng cung cáº¥p
    """
    try:
        # Náº¿u khÃ´ng cÃ³ category hoáº·c tags, dÃ¹ng AI Ä‘á»ƒ sinh tá»± Ä‘á»™ng
        category = request.category
        tags = request.tags if request.tags else []
        
        if category == "general" or not tags:
            # DÃ¹ng Gemini Ä‘á»ƒ phÃ¢n tÃ­ch vÃ  sinh metadata
            analysis_prompt = f"""PhÃ¢n tÃ­ch vÄƒn báº£n sau vÃ  tráº£ vá» JSON vá»›i format chÃ­nh xÃ¡c:

VÄƒn báº£n: "{request.prompt}"

HÃ£y phÃ¢n tÃ­ch vÃ  tráº£ vá» JSON vá»›i format:
{{
  "category": "tÃªn_danh_má»¥c",
  "tags": ["tag1", "tag2", "tag3"]
}}

CÃ¡c category phá»• biáº¿n: programming, ai, machine-learning, education, science, business, health, technology, math, language

Chá»‰ tráº£ vá» JSON, khÃ´ng thÃªm text khÃ¡c."""

            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(analysis_prompt)
            
            try:
                # Parse JSON tá»« response
                import re
                json_text = response.text.strip()
                # TÃ¬m JSON trong response
                json_match = re.search(r'\{[^}]+\}', json_text)
                if json_match:
                    import json as json_lib
                    metadata_ai = json_lib.loads(json_match.group())
                    
                    if category == "general":
                        category = metadata_ai.get("category", "general")
                    
                    if not tags:
                        tags = metadata_ai.get("tags", [])
            except:
                # Náº¿u parse lá»—i, giá»¯ nguyÃªn giÃ¡ trá»‹ máº·c Ä‘á»‹nh
                pass
        
        metadata = {
            "category": category,
            "tags": tags,
            "type": "prompt"
        }
        
        result = vector_db.add_documents(
            documents=[request.prompt],
            metadatas=[metadata]
        )
        
        return {
            "status": "success",
            "message": "ÄÃ£ thÃªm prompt vÃ o RAG knowledge base",
            "prompt": request.prompt,
            "category": category,
            "tags": tags,
            "auto_generated": request.category == "general" or not request.tags,
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/api/documents/add", tags=["RAG - Knowledge Base"])
async def add_documents(request: DocumentRequest):
    """ThÃªm nhiá»u documents vÃ o Vector Database"""
    try:
        result = vector_db.add_documents(
            documents=request.documents,
            metadatas=request.metadatas
        )
        return {
            "status": "success",
            "message": f"ÄÃ£ thÃªm {result['count']} documents",
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/api/documents/search", tags=["RAG - Knowledge Base"])
async def search_documents(request: SearchRequest):
    """TÃ¬m kiáº¿m documents tÆ°Æ¡ng tá»± trong Vector Database"""
    try:
        results = vector_db.search(request.query, request.n_results)
        return {
            "query": request.query,
            "results": [
                {
                    "document": doc,
                    "distance": dist,
                    "metadata": meta,
                    "id": doc_id
                }
                for doc, dist, meta, doc_id in zip(
                    results['documents'],
                    results['distances'],
                    results['metadatas'],
                    results['ids']
                )
            ],
            "count": len(results['documents'])
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.get("/api/documents", tags=["RAG - Knowledge Base"])
async def get_all_documents():
    """Láº¥y táº¥t cáº£ documents trong Vector Database"""
    try:
        return vector_db.get_all_documents()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.delete("/api/documents", tags=["RAG - Knowledge Base"])
async def delete_all_documents():
    """XÃ³a táº¥t cáº£ documents trong Vector Database"""
    try:
        return vector_db.delete_all()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.get("/api/documents/count", tags=["RAG - Knowledge Base"])
async def get_document_count():
    """Láº¥y sá»‘ lÆ°á»£ng documents trong Vector Database"""
    return {"count": vector_db.get_count()}

@app.get("/api/rag/stats", tags=["RAG - Knowledge Base"])
async def get_rag_stats():
    """Láº¥y thá»‘ng kÃª vá» RAG Knowledge Base"""
    try:
        all_docs = vector_db.get_all_documents()
        
        # Thá»‘ng kÃª theo category
        categories = {}
        for meta in all_docs['metadatas']:
            cat = meta.get('category', 'unknown')
            categories[cat] = categories.get(cat, 0) + 1
        
        return {
            "total_documents": all_docs['count'],
            "categories": categories,
            "status": "active"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.get("/api/models", tags=["Models"])
async def list_models():
    """Liá»‡t kÃª cÃ¡c model Gemini cÃ³ sáºµn"""
    try:
        models = []
        for model in genai.list_models():
            if 'generateContent' in model.supported_generation_methods:
                models.append({
                    "name": model.name,
                    "display_name": model.display_name,
                    "description": model.description
                })
        return {"models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.get("/api/models/groq", tags=["Models"])
async def list_groq_models():
    """
    Liá»‡t kÃª cÃ¡c Groq models cÃ³ sáºµn tá»« API
    
    Fetches models tá»« Groq API: https://api.groq.com/openai/v1/models
    Falls back to hardcoded list náº¿u API fail
    """
    try:
        print(f"ğŸ“‹ GET /api/models/groq - groq_client initialized: {groq_client is not None}")
        
        # Try to get models from Groq API
        if groq_client:
            models = groq_client.get_models_from_api()
            print(f"âœ… Fetched {len(models)} models from Groq API")
            return {
                "models": models,
                "provider": "Groq",
                "api_url": "https://console.groq.com/",
                "total": len(models),
                "source": "api"
            }
        
        # Fallback if no groq_client
        fallback_models = [
            {
                "id": "llama-3.3-70b-versatile",
                "name": "Llama 3.3 70B Versatile",
                "description": "Best overall performance - Latest",
                "context": 128000,
                "speed": "fast"
            },
            {
                "id": "llama-3.1-70b-versatile",
                "name": "Llama 3.1 70B",
                "description": "High performance",
                "context": 128000,
                "speed": "fast"
            },
            {
                "id": "llama-3.1-8b-instant",
                "name": "Llama 3.1 8B Instant",
                "description": "Fastest inference",
                "context": 128000,
                "speed": "ultra-fast"
            },
            {
                "id": "mixtral-8x7b-32768",
                "name": "Mixtral 8x7B",
                "description": "Long context specialist",
                "context": 32768,
                "speed": "fast"
            },
            {
                "id": "gemma2-9b-it",
                "name": "Gemma 2 9B",
                "description": "Lightweight & efficient",
                "context": 8192,
                "speed": "ultra-fast"
            },
            {
                "id": "qwen/qwen3-32b",
                "name": "Qwen 3 32B",
                "description": "Advanced reasoning",
                "context": 131072,
                "speed": "fast"
            }
        ]
        
        return {
            "models": fallback_models,
            "provider": "Groq",
            "api_url": "https://console.groq.com/",
            "total": len(fallback_models),
            "source": "fallback",
            "warning": "GROQ_API_KEY not configured"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

# ============================================================================
# AI EXTENDED APIS
# ============================================================================

class GenerateQuizRequest(BaseModel):
    content: str
    num_questions: int = 10
    difficulty: str = "medium"
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "content": "Python lÃ  ngÃ´n ngá»¯ láº­p trÃ¬nh báº­c cao...",
                "num_questions": 10,
                "difficulty": "medium"
            }
        }
    )

class QuizQuestion(BaseModel):
    question: str
    a: str
    b: str
    c: str
    d: str
    correct: str

class GenerateQuizResponse(BaseModel):
    questions: List[QuizQuestion]

class SummarizeRequest(BaseModel):
    content: str
    max_length: Optional[int] = 200

class SummarizeResponse(BaseModel):
    summary: str
    original_length: int
    summary_length: int

class ExplainRequest(BaseModel):
    question: str
    context: Optional[str] = None

class ExplainResponse(BaseModel):
    explanation: str
    examples: Optional[List[str]] = None

class IngestRequest(BaseModel):
    file_url: str
    title: Optional[str] = None

class IngestResponse(BaseModel):
    status: str
    message: str
    documents_added: int

@app.post("/api/ai/generate-quiz", response_model=GenerateQuizResponse, tags=["AI - Extended"])
async def generate_quiz(request: GenerateQuizRequest):
    """Táº¡o cÃ¢u há»i tráº¯c nghiá»‡m tá»± Ä‘á»™ng tá»« ná»™i dung bÃ i há»c"""
    try:
        import re
        import json as json_lib
        
        if request.num_questions < 1 or request.num_questions > 50:
            raise HTTPException(status_code=400, detail="Sá»‘ cÃ¢u há»i pháº£i tá»« 1-50")
        
        difficulty_map = {
            "easy": "dá»…, cÆ¡ báº£n",
            "medium": "trung bÃ¬nh",
            "hard": "khÃ³, nÃ¢ng cao"
        }
        
        difficulty_desc = difficulty_map.get(request.difficulty.lower(), "trung bÃ¬nh")
        
        prompt = f"""Dá»±a trÃªn ná»™i dung sau, hÃ£y táº¡o {request.num_questions} cÃ¢u há»i tráº¯c nghiá»‡m vá»›i Ä‘á»™ khÃ³ {difficulty_desc}.

Ná»™i dung:
{request.content}

YÃªu cáº§u:
- Táº¡o Ä‘Ãºng {request.num_questions} cÃ¢u há»i
- Má»—i cÃ¢u cÃ³ 4 Ä‘Ã¡p Ã¡n A, B, C, D
- Chá»‰ 1 Ä‘Ã¡p Ã¡n Ä‘Ãºng
- CÃ¢u há»i pháº£i liÃªn quan trá»±c tiáº¿p Ä‘áº¿n ná»™i dung
- Tráº£ vá» JSON array vá»›i format:

[
  {{
    "question": "CÃ¢u há»i?",
    "a": "ÄÃ¡p Ã¡n A",
    "b": "ÄÃ¡p Ã¡n B",
    "c": "ÄÃ¡p Ã¡n C",
    "d": "ÄÃ¡p Ã¡n D",
    "correct": "A"
  }}
]

CHá»ˆ TRáº¢ Vá»€ JSON, KHÃ”NG THÃŠM TEXT KHÃC."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        json_text = response.text.strip()
        json_match = re.search(r'\[.*\]', json_text, re.DOTALL)
        if not json_match:
            raise HTTPException(status_code=500, detail="KhÃ´ng thá»ƒ parse JSON tá»« AI response")
        
        questions_data = json_lib.loads(json_match.group())
        
        questions = []
        for q in questions_data:
            questions.append(QuizQuestion(
                question=q['question'],
                a=q['a'],
                b=q['b'],
                c=q['c'],
                d=q['d'],
                correct=q['correct'].upper()
            ))
        
        return GenerateQuizResponse(questions=questions)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/api/ai/summarize", response_model=SummarizeResponse, tags=["AI - Extended"])
async def summarize(request: SummarizeRequest):
    """TÃ³m táº¯t vÄƒn báº£n"""
    try:
        prompt = f"""HÃ£y tÃ³m táº¯t vÄƒn báº£n sau trong khoáº£ng {request.max_length} tá»«:

{request.content}

YÃªu cáº§u:
- Giá»¯ láº¡i Ã½ chÃ­nh
- Ngáº¯n gá»n, sÃºc tÃ­ch
- Dá»… hiá»ƒu
- KhÃ´ng thÃªm thÃ´ng tin ngoÃ i vÄƒn báº£n gá»‘c"""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        summary = response.text.strip()
        
        return SummarizeResponse(
            summary=summary,
            original_length=len(request.content.split()),
            summary_length=len(summary.split())
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/api/ai/explain", response_model=ExplainResponse, tags=["AI - Extended"])
async def explain(request: ExplainRequest):
    """Giáº£i thÃ­ch nhÆ° má»™t giÃ¡o viÃªn"""
    try:
        import re
        
        context_text = f"\nNgá»¯ cáº£nh: {request.context}" if request.context else ""
        
        prompt = f"""Báº¡n lÃ  má»™t giÃ¡o viÃªn giá»i. HÃ£y giáº£i thÃ­ch cÃ¢u há»i sau má»™t cÃ¡ch dá»… hiá»ƒu, chi tiáº¿t:{context_text}

CÃ¢u há»i: {request.question}

YÃªu cáº§u:
- Giáº£i thÃ­ch rÃµ rÃ ng, dá»… hiá»ƒu
- Sá»­ dá»¥ng vÃ­ dá»¥ cá»¥ thá»ƒ
- Chia nhá» thÃ nh cÃ¡c bÆ°á»›c náº¿u cáº§n
- Giá»ng Ä‘iá»‡u thÃ¢n thiá»‡n, khuyáº¿n khÃ­ch há»c táº­p

Sau pháº§n giáº£i thÃ­ch, hÃ£y Ä‘Æ°a ra 2-3 vÃ­ dá»¥ minh há»a."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        explanation = response.text.strip()
        
        examples = []
        if "VÃ­ dá»¥" in explanation or "Example" in explanation:
            parts = re.split(r'VÃ­ dá»¥ \d+:|Example \d+:', explanation)
            if len(parts) > 1:
                examples = [ex.strip() for ex in parts[1:]]
        
        return ExplainResponse(
            explanation=explanation,
            examples=examples if examples else None
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

@app.post("/api/ai/ingest", response_model=IngestResponse, tags=["AI - Extended"])
async def ingest_document(request: IngestRequest):
    """Ingest tÃ i liá»‡u vÃ o RAG Vector Database"""
    try:
        # Simplified: chá»‰ xá»­ lÃ½ text content
        # Trong production cáº§n thÃªm PDF, DOC parsing
        
        # Giáº£ láº­p extract text
        text_content = f"Ná»™i dung tá»« {request.file_url}"
        
        # Chia nhá» thÃ nh chunks
        words = text_content.split()
        chunk_size = 500
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
        
        # ThÃªm vÃ o vector DB
        metadatas = [{
            "source": request.file_url,
            "title": request.title or "Untitled",
            "type": "document"
        } for _ in chunks]
        
        result = vector_db.add_documents(
            documents=chunks,
            metadatas=metadatas
        )
        
        return IngestResponse(
            status="success",
            message=f"ÄÃ£ ingest {len(chunks)} chunks vÃ o RAG database",
            documents_added=len(chunks)
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")

# ============================================================================
# CREDENTIAL MANAGER INTEGRATION
# ============================================================================

# Import credential API router - DISABLED due to heavy dependencies
# To enable: pip install sentence-transformers (takes long time)
CREDENTIAL_API_AVAILABLE = False
print("âš ï¸  Credential Manager API disabled (sentence-transformers too heavy)")
print("   AI semantic search for credentials not available")

# Uncomment below to enable (requires sentence-transformers)
# try:
#     from credential_api import router as credential_router
#     app.include_router(credential_router)
#     CREDENTIAL_API_AVAILABLE = True
#     print("âœ… Credential Manager API loaded")
# except Exception as e:
#     print(f"âš ï¸  Credential Manager API error: {e}")

# ============================================================================
# TEST TVU SCHEDULE ENDPOINT (For quick testing)
# ============================================================================
try:
    from tvu_scraper import TVUScraper
    TVU_SCRAPER_AVAILABLE = True
    print("âœ… TVU Scraper loaded")
except ImportError as e:
    TVU_SCRAPER_AVAILABLE = False
    print(f"âš ï¸  TVU Scraper not available: {e}")

class TVUTestRequest(BaseModel):
    mssv: str
    password: str
    message: str = "HÃ´m nay tÃ´i há»c gÃ¬?"

@app.post("/api/test/tvu-schedule", tags=["Test - TVU"])
async def test_tvu_schedule(request: TVUTestRequest):
    """
    ğŸ§ª Test endpoint - Láº¥y thá»i khÃ³a biá»ƒu TVU trá»±c tiáº¿p (khÃ´ng cáº§n Ä‘Äƒng nháº­p há»‡ thá»‘ng)
    
    - **mssv**: MÃ£ sá»‘ sinh viÃªn TVU
    - **password**: Máº­t kháº©u
    - **message**: CÃ¢u há»i (vd: "HÃ´m nay tÃ´i há»c gÃ¬?", "tuáº§n nÃ y há»c gÃ¬?")
    """
    if not TVU_SCRAPER_AVAILABLE:
        raise HTTPException(status_code=500, detail="TVU Scraper not available")
    
    try:
        scraper = TVUScraper()
        
        # Login to TVU
        if not scraper.login(request.mssv, request.password):
            return {"success": False, "message": "âŒ ÄÄƒng nháº­p TVU tháº¥t báº¡i. Kiá»ƒm tra láº¡i MSSV vÃ  máº­t kháº©u."}
        
        # Get schedule
        schedules = scraper.get_schedule()
        
        if not schedules:
            return {"success": True, "message": "ğŸ“… KhÃ´ng cÃ³ lá»‹ch há»c tuáº§n nÃ y.", "schedules": []}
        
        # Determine what user is asking for
        message_lower = request.message.lower()
        
        # Filter by day if asking for specific day
        from datetime import datetime
        today = datetime.now()
        day_names = ['MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']
        today_name = day_names[today.weekday()]
        
        vietnamese_days = {
            'MONDAY': 'Thá»© 2',
            'TUESDAY': 'Thá»© 3', 
            'WEDNESDAY': 'Thá»© 4',
            'THURSDAY': 'Thá»© 5',
            'FRIDAY': 'Thá»© 6',
            'SATURDAY': 'Thá»© 7',
            'SUNDAY': 'Chá»§ nháº­t'
        }
        
        # Check if asking for today
        if 'hÃ´m nay' in message_lower or 'today' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == today_name]
            day_label = f"hÃ´m nay ({vietnamese_days[today_name]})"
        # Check for specific day
        elif 'thá»© 2' in message_lower or 'thá»© hai' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'MONDAY']
            day_label = 'Thá»© 2'
        elif 'thá»© 3' in message_lower or 'thá»© ba' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'TUESDAY']
            day_label = 'Thá»© 3'
        elif 'thá»© 4' in message_lower or 'thá»© tÆ°' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'WEDNESDAY']
            day_label = 'Thá»© 4'
        elif 'thá»© 5' in message_lower or 'thá»© nÄƒm' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'THURSDAY']
            day_label = 'Thá»© 5'
        elif 'thá»© 6' in message_lower or 'thá»© sÃ¡u' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'FRIDAY']
            day_label = 'Thá»© 6'
        elif 'thá»© 7' in message_lower or 'thá»© báº£y' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'SATURDAY']
            day_label = 'Thá»© 7'
        elif 'chá»§ nháº­t' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'SUNDAY']
            day_label = 'Chá»§ nháº­t'
        else:
            day_label = 'tuáº§n nÃ y'
        
        # Format response
        if not schedules:
            return {
                "success": True,
                "message": f"ğŸ“… {day_label.capitalize()} báº¡n khÃ´ng cÃ³ lá»›p nÃ o.",
                "schedules": []
            }
        
        # Group by day
        by_day = {}
        for s in schedules:
            day = s.get('dayOfWeek', 'UNKNOWN')
            if day not in by_day:
                by_day[day] = []
            by_day[day].append(s)
        
        # Sort days
        day_order = ['MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']
        
        message_text = f"ğŸ“… **Lá»‹ch há»c {day_label}:**\n\n"
        
        for day in day_order:
            if day in by_day:
                message_text += f"**{vietnamese_days[day]}:**\n"
                for s in sorted(by_day[day], key=lambda x: x.get('startTime', '')):
                    start_time = s.get('startTime', '')[:5]
                    end_time = s.get('endTime', '')[:5]
                    message_text += f"  ğŸ• {start_time} - {end_time}\n"
                    message_text += f"  ğŸ“š {s.get('subject', 'N/A')}\n"
                    message_text += f"  ğŸ« PhÃ²ng: {s.get('room', 'N/A')}\n"
                    if s.get('teacher'):
                        message_text += f"  ğŸ‘¨â€ğŸ« GV: {s['teacher']}\n"
                    message_text += "\n"
        
        return {
            "success": True,
            "message": message_text,
            "schedules": schedules,
            "count": len(schedules)
        }
        
    except Exception as e:
        return {"success": False, "message": f"âŒ Lá»—i: {str(e)}"}

# ============================================================================
# DOCUMENT INTELLIGENCE API
# ============================================================================

# Initialize Document Intelligence service
doc_intelligence_service = None
if DOCUMENT_INTELLIGENCE_AVAILABLE:
    try:
        doc_intelligence_service = create_document_intelligence_service(GEMINI_API_KEY)
        print("âœ… Document Intelligence initialized")
    except Exception as e:
        print(f"âš ï¸ Document Intelligence init failed: {e}")

class ProcessDocumentRequest(BaseModel):
    file_path: str
    num_cards: int = 10
    difficulty: str = "medium"
    include_summary: bool = True
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "file_path": "C:/Documents/lecture_notes.pdf",
                "num_cards": 10,
                "difficulty": "medium",
                "include_summary": True
            }
        }
    )

class DocumentTextRequest(BaseModel):
    text: str
    num_cards: int = 10
    difficulty: str = "medium"
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "text": "Python lÃ  ngÃ´n ngá»¯ láº­p trÃ¬nh...",
                "num_cards": 5,
                "difficulty": "easy"
            }
        }
    )

@app.post("/api/documents/process", tags=["Document Intelligence"])
async def process_document_to_flashcards(request: ProcessDocumentRequest):
    """
    ğŸ“„ Upload PDF/DOCX/TXT â†’ AI tá»± Ä‘á»™ng táº¡o Flashcards
    
    **TÃ­nh nÄƒng:**
    - TrÃ­ch xuáº¥t text tá»« PDF, DOCX, TXT, áº£nh (OCR)
    - AI tÃ³m táº¯t ná»™i dung
    - TrÃ­ch xuáº¥t key concepts
    - Tá»± Ä‘á»™ng táº¡o flashcards
    
    **Parameters:**
    - file_path: ÄÆ°á»ng dáº«n file (local path hoáº·c URL)
    - num_cards: Sá»‘ lÆ°á»£ng flashcards cáº§n táº¡o (default: 10)
    - difficulty: Äá»™ khÃ³ (easy/medium/hard)
    - include_summary: CÃ³ táº¡o summary khÃ´ng (default: true)
    
    **Returns:**
    ```json
    {
      "success": true,
      "file_name": "lecture_notes.pdf",
      "summary": "TÃ³m táº¯t ná»™i dung...",
      "key_concepts": ["Concept 1", "Concept 2", ...],
      "flashcards": [
        {
          "question": "CÃ¢u há»i?",
          "answer": "CÃ¢u tráº£ lá»i",
          "hint": "Gá»£i Ã½...",
          "explanation": "Giáº£i thÃ­ch chi tiáº¿t..."
        }
      ],
      "num_flashcards": 10
    }
    ```
    """
    if not DOCUMENT_INTELLIGENCE_AVAILABLE or not doc_intelligence_service:
        raise HTTPException(
            status_code=503,
            detail="Document Intelligence service not available. Please install dependencies: pip install pdfplumber PyPDF2 python-docx"
        )
    
    try:
        # Process document
        result = doc_intelligence_service.process_document_to_flashcards(
            file_path=request.file_path,
            num_cards=request.num_cards,
            difficulty=request.difficulty,
            include_summary=request.include_summary
        )
        
        if not result.get("success"):
            raise HTTPException(status_code=400, detail=result.get("error", "Unknown error"))
        
        return result
        
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing document: {str(e)}")

@app.post("/api/documents/text-to-flashcards", tags=["Document Intelligence"])
async def text_to_flashcards(request: DocumentTextRequest):
    """
    ğŸ“ Text â†’ AI táº¡o Flashcards
    
    Paste text trá»±c tiáº¿p, AI sáº½ táº¡o flashcards
    
    **Parameters:**
    - text: Ná»™i dung cáº§n táº¡o flashcards
    - num_cards: Sá»‘ lÆ°á»£ng flashcards
    - difficulty: Äá»™ khÃ³ (easy/medium/hard)
    
    **Use cases:**
    - Copy-paste tá»« lecture slides
    - Paste tá»« website/blog
    - Nháº­p text tá»± viáº¿t
    """
    if not DOCUMENT_INTELLIGENCE_AVAILABLE or not doc_intelligence_service:
        raise HTTPException(
            status_code=503,
            detail="Document Intelligence service not available"
        )
    
    try:
        # Validate text length
        if len(request.text) < 50:
            raise HTTPException(
                status_code=400,
                detail="Text quÃ¡ ngáº¯n. Cáº§n Ã­t nháº¥t 50 kÃ½ tá»± Ä‘á»ƒ táº¡o flashcards."
            )
        
        # Generate flashcards from text
        flashcards = doc_intelligence_service.generate_flashcards_from_text(
            text=request.text,
            num_cards=request.num_cards,
            difficulty=request.difficulty
        )
        
        if not flashcards:
            raise HTTPException(
                status_code=500,
                detail="KhÃ´ng thá»ƒ táº¡o flashcards. Vui lÃ²ng thá»­ láº¡i hoáº·c thay Ä‘á»•i ná»™i dung."
            )
        
        return {
            "success": True,
            "text_length": len(request.text),
            "flashcards": flashcards,
            "num_flashcards": len(flashcards)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.post("/api/documents/summarize", tags=["Document Intelligence"])
async def summarize_document(request: ProcessDocumentRequest):
    """
    ğŸ“„ TÃ³m táº¯t Document (PDF/DOCX/TXT)
    
    Upload file, AI sáº½ tÃ³m táº¯t ná»™i dung chÃ­nh
    """
    if not DOCUMENT_INTELLIGENCE_AVAILABLE or not doc_intelligence_service:
        raise HTTPException(
            status_code=503,
            detail="Document Intelligence service not available"
        )
    
    try:
        # Extract text
        text = doc_intelligence_service.extract_text(request.file_path)
        
        if not text or len(text) < 100:
            raise HTTPException(
                status_code=400,
                detail="Document quÃ¡ ngáº¯n hoáº·c khÃ´ng cÃ³ ná»™i dung vÄƒn báº£n"
            )
        
        # Summarize
        summary = doc_intelligence_service.summarize_document(text, max_length=500)
        
        return {
            "success": True,
            "file_name": Path(request.file_path).name,
            "original_length": len(text),
            "summary": summary,
            "summary_length": len(summary)
        }
        
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.get("/api/documents/capabilities", tags=["Document Intelligence"])
async def get_document_capabilities():
    """
    â„¹ï¸ Kiá»ƒm tra kháº£ nÄƒng xá»­ lÃ½ documents
    
    Returns thÃ´ng tin vá» cÃ¡c loáº¡i file Ä‘Æ°á»£c há»— trá»£
    """
    capabilities = {
        "service_available": DOCUMENT_INTELLIGENCE_AVAILABLE and doc_intelligence_service is not None,
        "supported_formats": [],
        "features": []
    }
    
    if DOCUMENT_INTELLIGENCE_AVAILABLE:
        try:
            from document_intelligence_service import (
                PDFPLUMBER_AVAILABLE,
                PYPDF2_AVAILABLE,
                DOCX_AVAILABLE,
                OCR_AVAILABLE
            )
            
            if PDFPLUMBER_AVAILABLE or PYPDF2_AVAILABLE:
                capabilities["supported_formats"].append("PDF (.pdf)")
            if DOCX_AVAILABLE:
                capabilities["supported_formats"].append("Word (.docx)")
            if OCR_AVAILABLE:
                capabilities["supported_formats"].append("Images (.png, .jpg, .jpeg) with OCR")
            
            capabilities["supported_formats"].append("Text (.txt)")
            
            capabilities["features"] = [
                "Auto-generate flashcards from documents",
                "Document summarization",
                "Key concepts extraction",
                "Text extraction from multiple formats"
            ]
            
            if OCR_AVAILABLE:
                capabilities["features"].append("OCR for scanned documents/images")
            
        except ImportError:
            pass
    
    return capabilities

# ============================================================================
# LANGCHAIN AGENT ENDPOINTS
# ============================================================================

class LangChainChatRequest(BaseModel):
    """Request model for LangChain agent chat"""
    message: str
    user_id: Optional[int] = None
    reset_memory: bool = False
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "message": "Xin chÃ o!",
                "user_id": 1,
                "reset_memory": False
            }
        }
    )

class LangChainChatResponse(BaseModel):
    """Response model for LangChain agent"""
    success: bool
    response: str
    agent_type: str = "langchain_simple"
    error: Optional[str] = None

@app.post("/api/chat/langchain", response_model=LangChainChatResponse, tags=["LangChain Agent"])
async def chat_with_langchain_agent(
    request: LangChainChatRequest,
    authorization: Optional[str] = Header(None)
):
    """Chat vá»›i LangChain AI Agent"""
    
    if not LANGCHAIN_AGENT_AVAILABLE or not langchain_agent:
        raise HTTPException(status_code=503, detail="LangChain Agent not available")
    
    try:
        user_id = request.user_id
        if not user_id and authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            user_id = get_user_id_from_token(token)
        
        if request.reset_memory:
            langchain_agent.reset_memory()
        
        result = langchain_agent.chat(message=request.message, user_id=user_id)
        return LangChainChatResponse(**result)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.post("/api/chat/langchain/reset", tags=["LangChain Agent"])
async def reset_langchain_memory():
    """Reset memory"""
    if not LANGCHAIN_AGENT_AVAILABLE or not langchain_agent:
        raise HTTPException(status_code=503, detail="Not available")
    
    langchain_agent.reset_memory()
    return {"success": True, "message": "Memory reset"}

@app.get("/api/chat/langchain/status", tags=["LangChain Agent"])
async def get_langchain_status():
    """Check status"""
    if not LANGCHAIN_AGENT_AVAILABLE or not langchain_agent:
        return {"available": False}
    
    return {
        "available": True,
        "memory_enabled": True,
        "llm_model": "gemini-2.0-flash-exp"
    }

# ============================================================================
# CALENDAR SYNC ENDPOINTS
# ============================================================================

class CalendarSyncRequest(BaseModel):
    """Request model for syncing schedule to calendar"""
    week: Optional[int] = None
    hoc_ky: Optional[str] = None
    user_id: Optional[int] = None  # User ID Ä‘á»ƒ láº¥y credentials vÃ  táº¡o events
    reminder_email: Optional[int] = None  # PhÃºt trÆ°á»›c Ä‘á»ƒ gá»­i email (vd: 30, 60, 1440)
    reminder_popup: Optional[int] = None  # PhÃºt trÆ°á»›c Ä‘á»ƒ hiá»‡n popup
    notification_email: Optional[str] = None  # Email tÃ¹y chá»‰nh Ä‘á»ƒ nháº­n thÃ´ng bÃ¡o
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "week": 5,
                "hoc_ky": "20251",
                "user_id": 1,
                "reminder_email": 30,
                "reminder_popup": 15,
                "notification_email": "myemail@gmail.com"
            }
        }
    )

@app.post("/api/calendar/sync-schedule", tags=["Calendar Sync"])
async def sync_schedule_to_calendar(
    request: CalendarSyncRequest,
    authorization: Optional[str] = Header(None)
):
    """
    ğŸ”„ Äá»“ng bá»™ Thá»i KhÃ³a Biá»ƒu lÃªn Google Calendar
    
    Tá»± Ä‘á»™ng láº¥y TKB tá»« TVU Portal vÃ  táº¡o events trÃªn Google Calendar
    
    **YÃªu cáº§u:**
    - ÄÃ£ káº¿t ná»‘i Google Account (OAuth)
    - ÄÃ£ cáº¥u hÃ¬nh tÃ i khoáº£n TVU trong Settings
    
    **Parameters:**
    - week: Tuáº§n há»c (optional, máº·c Ä‘á»‹nh tuáº§n hiá»‡n táº¡i)
    - hoc_ky: Há»c ká»³ (optional, máº·c Ä‘á»‹nh há»c ká»³ hiá»‡n táº¡i)
    - user_id: User ID (optional, náº¿u khÃ´ng cÃ³ sáº½ láº¥y tá»« token)
    
    **Returns:**
    - success: True/False
    - message: ThÃ´ng bÃ¡o káº¿t quáº£
    - events_created: Sá»‘ events Ä‘Ã£ táº¡o
    """
    if not AGENT_FEATURES_AVAILABLE or not agent_features:
        raise HTTPException(
            status_code=503,
            detail="Agent features not available"
        )
    
    try:
        # Get user_id - tá»« request body hoáº·c tá»« token
        user_id = request.user_id
        token = None
        
        # Náº¿u cÃ³ authorization header, láº¥y token vÃ  user_id tá»« Ä‘Ã³
        if authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            if not user_id:
                user_id = get_user_id_from_token(token)
        
        # Náº¿u váº«n khÃ´ng cÃ³ user_id, bÃ¡o lá»—i
        if not user_id:
            raise HTTPException(
                status_code=400,
                detail="user_id is required - please provide in request body or login"
            )
        
        print(f"ğŸ”„ Syncing schedule for user_id: {user_id}")
        
        # Call sync function - truyá»n user_id Ä‘á»ƒ láº¥y credentials
        result = agent_features.sync_schedule_to_calendar(
            token=token or "",  # Token cÃ³ thá»ƒ rá»—ng, function sáº½ dÃ¹ng user_id
            user_id=user_id,
            week=request.week,
            hoc_ky=request.hoc_ky,
            reminder_email=request.reminder_email,
            reminder_popup=request.reminder_popup,
            notification_email=request.notification_email
        )
        
        if result.get("success"):
            return result
        else:
            raise HTTPException(
                status_code=400,
                detail=result.get("message", "Sync failed")
            )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    print("=" * 60)
    print("ğŸš€ Starting AI Chat Service with RAG")
    print("=" * 60)
    print(f"ğŸ“ Server: http://localhost:{port}")
    print(f"ğŸ“š Swagger UI: http://localhost:{port}/docs")
    print(f"ğŸ“Š Vector DB Documents: {vector_db.get_count()}")
    if CREDENTIAL_API_AVAILABLE:
        print(f"ğŸ” Credential Manager: Enabled")
    print("=" * 60)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )
