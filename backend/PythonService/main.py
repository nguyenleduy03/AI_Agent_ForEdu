"""
FastAPI AI Chat Service with RAG (Retrieval-Augmented Generation)
T·∫•t c·∫£ trong 1 file - Gemini 2.5 Flash + Vector Database
"""
from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ConfigDict
from typing import List, Optional, Dict
import google.generativeai as genai
import os
from dotenv import load_dotenv
import json
import math
try:
    from youtube_helper import search_youtube_video, get_youtube_watch_url, get_youtube_embed_url
    YOUTUBE_HELPER_AVAILABLE = True
except ImportError:
    YOUTUBE_HELPER_AVAILABLE = False
    print("‚ö†Ô∏è  YouTube helper not available. Video search will use fallback.")

try:
    from agent_features import AgentFeatures
    AGENT_FEATURES_AVAILABLE = True
except ImportError:
    AGENT_FEATURES_AVAILABLE = False
    print("‚ö†Ô∏è  Agent features not available.")

try:
    from google_cloud_agent import GoogleCloudAgent
    GOOGLE_CLOUD_AGENT_AVAILABLE = True
except ImportError:
    GOOGLE_CLOUD_AGENT_AVAILABLE = False
    print("‚ö†Ô∏è  Google Cloud Agent not available.")

# ============================================================================
# VECTOR DATABASE CLASS
# ============================================================================

def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """T√≠nh cosine similarity gi·ªØa 2 vectors"""
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(a * a for a in vec1))
    magnitude2 = math.sqrt(sum(b * b for b in vec2))
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    
    return dot_product / (magnitude1 * magnitude2)

class SimpleVectorDB:
    def __init__(self, storage_file: str = "vector_db.json"):
        """Kh·ªüi t·∫°o Simple Vector Database"""
        self.storage_file = storage_file
        self.documents = []
        self.load()
    
    def load(self):
        """Load data t·ª´ file"""
        if os.path.exists(self.storage_file):
            with open(self.storage_file, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
    
    def save(self):
        """L∆∞u data v√†o file"""
        with open(self.storage_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
    
    def add_documents(self, documents: List[str], metadatas: List[Dict] = None, ids: List[str] = None):
        """Th√™m documents v√†o database"""
        if ids is None:
            start_id = len(self.documents)
            ids = [f"doc_{start_id + i}" for i in range(len(documents))]
        
        if metadatas is None:
            metadatas = [{"source": "manual"} for _ in documents]
        
        # T·∫°o embeddings
        for doc, metadata, doc_id in zip(documents, metadatas, ids):
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=doc,
                task_type="retrieval_document"
            )
            
            self.documents.append({
                "id": doc_id,
                "document": doc,
                "embedding": result['embedding'],
                "metadata": metadata
            })
        
        self.save()
        return {"status": "success", "count": len(documents)}
    
    def search(self, query: str, n_results: int = 5) -> Dict:
        """T√¨m ki·∫øm documents t∆∞∆°ng t·ª±"""
        if not self.documents:
            return {"documents": [], "distances": [], "metadatas": [], "ids": []}
        
        # T·∫°o embedding cho query
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=query,
            task_type="retrieval_query"
        )
        query_embedding = result['embedding']
        
        # T√≠nh similarity v·ªõi t·∫•t c·∫£ documents
        similarities = []
        for doc in self.documents:
            similarity = cosine_similarity(query_embedding, doc['embedding'])
            similarities.append({
                "document": doc['document'],
                "distance": 1 - similarity,
                "metadata": doc['metadata'],
                "id": doc['id'],
                "similarity": similarity
            })
        
        # S·∫Øp x·∫øp theo similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        top_results = similarities[:n_results]
        
        return {
            "documents": [r['document'] for r in top_results],
            "distances": [r['distance'] for r in top_results],
            "metadatas": [r['metadata'] for r in top_results],
            "ids": [r['id'] for r in top_results]
        }
    
    def delete_all(self):
        """X√≥a t·∫•t c·∫£ documents"""
        self.documents = []
        self.save()
        return {"status": "success", "message": "All documents deleted"}
    
    def get_count(self) -> int:
        """L·∫•y s·ªë l∆∞·ª£ng documents"""
        return len(self.documents)
    
    def get_all_documents(self) -> Dict:
        """L·∫•y t·∫•t c·∫£ documents"""
        return {
            "documents": [doc['document'] for doc in self.documents],
            "metadatas": [doc['metadata'] for doc in self.documents],
            "ids": [doc['id'] for doc in self.documents],
            "count": len(self.documents)
        }

# ============================================================================
# FASTAPI APP SETUP
# ============================================================================

# Load environment variables
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY or GEMINI_API_KEY == "your_gemini_api_key_here":
    raise ValueError("‚ö†Ô∏è  GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env\nL·∫•y API key t·∫°i: https://aistudio.google.com/apikey")

genai.configure(api_key=GEMINI_API_KEY)

# Initialize FastAPI app
app = FastAPI(
    title="AI Chat Service with RAG",
    description="API chat v·ªõi Gemini 2.5 Flash + Vector Database RAG",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Vector Database
vector_db = SimpleVectorDB(storage_file="knowledge_base.json")

# Initialize Agent Features
if AGENT_FEATURES_AVAILABLE:
    agent_features = AgentFeatures(spring_boot_url="http://localhost:8080")
    print("‚úÖ Agent Features initialized")
else:
    agent_features = None
    print("‚ö†Ô∏è  Agent Features not initialized")

# Initialize Google Cloud Agent
if GOOGLE_CLOUD_AGENT_AVAILABLE:
    google_cloud_agent = GoogleCloudAgent(google_cloud_url="http://localhost:8002")
    print("‚úÖ Google Cloud Agent initialized")
else:
    google_cloud_agent = None
    print("‚ö†Ô∏è  Google Cloud Agent not initialized")

# ============================================================================
# PYDANTIC MODELS
# ============================================================================

class ChatRequest(BaseModel):
    message: str
    model: str = "gemini-flash-latest"  # Use latest flash model (1,500 requests/day)
    use_rag: bool = True
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "message": "Gi·∫£i th√≠ch v·ªÅ AI l√† g√¨?",
                "model": "gemini-2.5-flash",
                "use_rag": True
            }
        }
    )

class ActionLink(BaseModel):
    type: str  # "youtube", "google", "wikipedia"
    url: str
    title: str
    icon: str

class ToolAction(BaseModel):
    tool: str  # "play_youtube", "search_youtube", "search_google", "open_wikipedia"
    query: str
    url: str
    auto_execute: bool = True
    video_id: Optional[str] = None  # YouTube video ID
    embed_url: Optional[str] = None  # URL ƒë·ªÉ embed video

class ChatResponse(BaseModel):
    response: str
    model: str
    context_used: Optional[List[str]] = None
    rag_enabled: bool = False
    suggested_actions: Optional[List[ActionLink]] = None  # Links g·ª£i √Ω
    tool_action: Optional[ToolAction] = None  # Action t·ª± ƒë·ªông th·ª±c thi

class DocumentRequest(BaseModel):
    documents: List[str]
    metadatas: Optional[List[dict]] = None
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "documents": [
                    "AI (Artificial Intelligence) l√† tr√≠ tu·ªá nh√¢n t·∫°o.",
                    "Machine Learning l√† m·ªôt nh√°nh c·ªßa AI."
                ]
            }
        }
    )

class PromptRAGRequest(BaseModel):
    prompt: str
    category: Optional[str] = "general"
    tags: Optional[List[str]] = None
    
    model_config = ConfigDict(
        json_schema_extra={
            "examples": [
                {
                    "prompt": "Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh d·ªÖ h·ªçc v√† m·∫°nh m·∫Ω.",
                    "category": "programming",
                    "tags": ["python", "programming", "ai"]
                },
                {
                    "prompt": "Machine Learning gi√∫p m√°y t√≠nh h·ªçc t·ª´ d·ªØ li·ªáu m√† kh√¥ng c·∫ßn l·∫≠p tr√¨nh c·ª• th·ªÉ."
                }
            ]
        }
    )

class SimplePromptRequest(BaseModel):
    prompt: str
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "prompt": "FastAPI l√† framework Python hi·ªán ƒë·∫°i ƒë·ªÉ x√¢y d·ª±ng API nhanh v√† d·ªÖ s·ª≠ d·ª•ng."
            }
        }
    )

class SearchRequest(BaseModel):
    query: str
    n_results: int = 5

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/", tags=["Health"])
async def root():
    """Health check endpoint"""
    doc_count = vector_db.get_count()
    return {
        "status": "running",
        "service": "AI Chat Service with RAG",
        "version": "2.0.0",
        "vector_db_documents": doc_count
    }

def detect_tool_intent(message: str) -> Optional[ToolAction]:
    """Ph√°t hi·ªán intent ƒë·ªÉ t·ª± ƒë·ªông th·ª±c thi tool"""
    message_lower = message.lower()
    
    # Intent: Ph√°t video YouTube (t·ª± ƒë·ªông t√¨m v√† ph√°t)
    # Ch·ªâ c·∫ßn c√≥ trigger "ph√°t", "play", "ch∆°i", "b·∫≠t" l√† ƒë·ªß
    play_triggers = ["ph√°t", "play", "ch∆°i", "b·∫≠t"]
    
    is_play_intent = any(trigger in message_lower for trigger in play_triggers)
    
    if is_play_intent:
        # Extract query - lo·∫°i b·ªè c√°c t·ª´ trigger
        query = message_lower
        for trigger in play_triggers + ["m·ªü", "cho t√¥i", "cho toi", "m·ªôt", "mot", "b·∫•t k·ª≥", "bat ky", "video", "youtube", "t·∫≠p"]:
            query = query.replace(trigger, "")
        query = query.strip()
        
        if query and YOUTUBE_HELPER_AVAILABLE:
            # T√¨m video tr√™n YouTube
            try:
                video_id = search_youtube_video(query)
                
                if video_id:
                    watch_url = get_youtube_watch_url(video_id, autoplay=True)
                    embed_url = get_youtube_embed_url(video_id, autoplay=True)
                    
                    return ToolAction(
                        tool="play_youtube",
                        query=query,
                        url=watch_url,
                        video_id=video_id,
                        embed_url=embed_url,
                        auto_execute=True
                    )
            except Exception as e:
                print(f"Error searching YouTube: {e}")
                # Fallback to search
                pass
    
    # Intent: M·ªü YouTube search (kh√¥ng t·ª± ƒë·ªông ph√°t)
    youtube_triggers = ["m·ªü video", "xem video", "open video", "show video", "youtube", "t√¨m video"]
    for trigger in youtube_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").replace("v·ªÅ", "").strip()
            if query:
                youtube_url = f"https://www.youtube.com/results?search_query={query.replace(' ', '+')}"
                return ToolAction(
                    tool="search_youtube",
                    query=query,
                    url=youtube_url,
                    auto_execute=True
                )
    
    # Intent: Search Google
    google_triggers = ["t√¨m ki·∫øm", "search", "google", "tra google", "t√¨m tr√™n google"]
    for trigger in google_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").strip()
            if query:
                google_url = f"https://www.google.com/search?q={query.replace(' ', '+')}"
                return ToolAction(
                    tool="search_google",
                    query=query,
                    url=google_url,
                    auto_execute=True
                )
    
    # Intent: Open Wikipedia
    wiki_triggers = ["wikipedia", "wiki", "tra wikipedia"]
    for trigger in wiki_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").strip()
            if query:
                wiki_url = f"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}"
                return ToolAction(
                    tool="open_wikipedia",
                    query=query,
                    url=wiki_url,
                    auto_execute=True
                )
    
    return None

@app.post("/api/chat", response_model=ChatResponse, tags=["Chat"])
async def chat(request: ChatRequest, authorization: Optional[str] = Header(None)):
    """
    Chat v·ªõi Gemini AI (c√≥ h·ªó tr·ª£ RAG + Agent Features)
    
    - **message**: Tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
    - **model**: Model Gemini s·ª≠ d·ª•ng (m·∫∑c ƒë·ªãnh: gemini-2.5-flash)
    - **use_rag**: S·ª≠ d·ª•ng RAG ƒë·ªÉ tƒÉng c∆∞·ªùng context (m·∫∑c ƒë·ªãnh: true)
    
    Agent Features (t·ª± ƒë·ªông):
    - Xem th·ªùi kh√≥a bi·ªÉu (t·ª± ƒë·ªông l·∫•y t·ª´ trang tr∆∞·ªùng)
    - Xem ƒëi·ªÉm s·ªë
    - G·ª≠i email
    
    Models ƒë∆∞·ª£c khuy·∫øn ngh·ªã:
    - gemini-2.5-flash (M·ªöI NH·∫§T - Nhanh, stable)
    - gemini-2.5-pro (M·∫°nh nh·∫•t)
    - gemini-flash-latest (Lu√¥n d√πng version m·ªõi nh·∫•t)
    """
    try:
        # Extract token from Authorization header
        token = None
        if authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
        
        # GOOGLE CLOUD AGENT - Check intents FIRST
        if GOOGLE_CLOUD_AGENT_AVAILABLE and google_cloud_agent:
            # Check for Google Cloud intents
            gc_result = google_cloud_agent.handle_google_cloud_request(
                message=request.message,
                token=token or "",
                image_url=None,  # TODO: Extract from message if available
                audio_base64=None  # TODO: Extract from message if available
            )
            
            if gc_result:
                print(f"üåê Google Cloud intent detected and handled")
                return ChatResponse(
                    response=gc_result['message'],
                    model=request.model,
                    rag_enabled=False
                )
        
        # AGENT FEATURES - Check intents (schedule, grades, email)
        if AGENT_FEATURES_AVAILABLE and agent_features and token:
            # Check for schedule intent
            if agent_features.detect_schedule_intent(request.message):
                print(f"üîç Detected schedule intent in: {request.message}")
                result = agent_features.get_schedule(token, message=request.message, force_sync=False)
                
                return ChatResponse(
                    response=result['message'],
                    model=request.model,
                    rag_enabled=False
                )
            
            # Check for grade intent
            if agent_features.detect_grade_intent(request.message):
                print(f"üîç Detected grade intent in: {request.message}")
                result = agent_features.get_grades(token)
                
                return ChatResponse(
                    response=result['message'],
                    model=request.model,
                    rag_enabled=False
                )
            
            # Check for email intent
            if agent_features.detect_email_intent(request.message):
                print(f"üîç Detected email intent in: {request.message}")
                gemini_model = genai.GenerativeModel(request.model)
                result = agent_features.handle_email_request(request.message, token, gemini_model)
                
                return ChatResponse(
                    response=result['message'],
                    model=request.model,
                    rag_enabled=False
                )
        
        # Detect tool action (YouTube, Google, Wikipedia)
        tool_action = detect_tool_intent(request.message)
        
        if tool_action:
            # AI x√°c nh·∫≠n action
            tool_messages = {
                "play_youtube": f"üé¨ ƒêang ph√°t video YouTube v·ªÅ '{tool_action.query}'...\n\nVideo s·∫Ω t·ª± ƒë·ªông ph√°t trong gi√¢y l√°t! üé•",
                "search_youtube": f"üé• ƒêang m·ªü YouTube ƒë·ªÉ xem video v·ªÅ '{tool_action.query}'...",
                "search_google": f"üîç ƒêang t√¨m ki·∫øm tr√™n Google v·ªÅ '{tool_action.query}'...",
                "open_wikipedia": f"üìñ ƒêang m·ªü Wikipedia v·ªÅ '{tool_action.query}'..."
            }
            
            confirmation = tool_messages.get(tool_action.tool, "ƒêang th·ª±c hi·ªán...")
            
            return ChatResponse(
                response=confirmation,
                model=request.model,
                tool_action=tool_action,
                rag_enabled=False
            )
        
        # System prompt - Personality c·ªßa AI
        system_prompt = """üéì B·∫°n l√† AI Learning Assistant - Tr·ª£ l√Ω h·ªçc t·∫≠p th√¥ng minh v√† th√¢n thi·ªán!

**Vai tr√≤ c·ªßa b·∫°n:**
- Gi√°o vi√™n ·∫£o ki√™n nh·∫´n, nhi·ªát t√¨nh üë®‚Äçüè´
- Gi·∫£i th√≠ch ki·∫øn th·ª©c r√µ r√†ng, d·ªÖ hi·ªÉu
- Khuy·∫øn kh√≠ch h·ªçc sinh t∆∞ duy v√† ƒë·∫∑t c√¢u h·ªèi
- Lu√¥n t√≠ch c·ª±c v√† ƒë·ªông vi√™n

**Phong c√°ch giao ti·∫øp:**
- Th√¢n thi·ªán, g·∫ßn g≈©i nh∆∞ ng∆∞·ªùi b·∫°n üòä
- S·ª≠ d·ª•ng emoji ph√π h·ª£p ƒë·ªÉ sinh ƒë·ªông: üìö ‚ú® üí° üéØ ‚úÖ
- Chia nh·ªè ki·∫øn th·ª©c ph·ª©c t·∫°p th√†nh c√°c ph·∫ßn d·ªÖ hi·ªÉu
- ƒê∆∞a ra v√≠ d·ª• th·ª±c t·∫ø, g·∫ßn g≈©i v·ªõi cu·ªôc s·ªëng

**C√°ch tr·∫£ l·ªùi:**
1. T√≥m t·∫Øt ng·∫Øn g·ªçn c√¢u h·ªèi (n·∫øu c·∫ßn)
2. Gi·∫£i th√≠ch chi ti·∫øt v·ªõi c·∫•u tr√∫c r√µ r√†ng
3. ƒê∆∞a ra 1-2 v√≠ d·ª• minh h·ªça
4. H·ªèi l·∫°i xem c√≤n th·∫Øc m·∫Øc g√¨ kh√¥ng

**L∆∞u √Ω:**
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y th·ª´a nh·∫≠n v√† ƒë·ªÅ xu·∫•t t√¨m hi·ªÉu th√™m
- Khuy·∫øn kh√≠ch h·ªçc sinh t·ª± suy nghƒ© tr∆∞·ªõc khi ƒë∆∞a ra ƒë√°p √°n
- S·ª≠ d·ª•ng ng√¥n ng·ªØ ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô h·ªçc sinh
"""
        
        context_docs = []
        prompt = request.message
        
        # N·∫øu b·∫≠t RAG, t√¨m ki·∫øm context t·ª´ vector DB
        if request.use_rag and vector_db.get_count() > 0:
            search_results = vector_db.search(request.message, n_results=3)
            context_docs = search_results['documents']
            
            if context_docs:
                context_text = "\n\n".join([f"üìö T√†i li·ªáu {i+1}: {doc}" for i, doc in enumerate(context_docs)])
                prompt = f"""{system_prompt}

**T√†i li·ªáu tham kh·∫£o t·ª´ kh√≥a h·ªçc:**
{context_text}

**C√¢u h·ªèi c·ªßa h·ªçc sinh:**
{request.message}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu v√† ki·∫øn th·ª©c c·ªßa b·∫°n. N·∫øu t√†i li·ªáu kh√¥ng ƒë·ªß th√¥ng tin, h√£y b·ªï sung t·ª´ ki·∫øn th·ª©c chung."""
            else:
                prompt = f"""{system_prompt}

**C√¢u h·ªèi c·ªßa h·ªçc sinh:**
{request.message}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n."""
        else:
            prompt = f"""{system_prompt}

**C√¢u h·ªèi c·ªßa h·ªçc sinh:**
{request.message}"""
        
        # Generate response v·ªõi Gemini
        model = genai.GenerativeModel(request.model)
        response = model.generate_content(prompt)
        
        # T·∫°o suggested actions (YouTube, Google Search)
        suggested_actions = []
        
        # T·∫°o search query t·ª´ c√¢u h·ªèi
        search_query = request.message.replace("?", "").strip()
        
        # YouTube link
        youtube_query = search_query.replace(" ", "+")
        suggested_actions.append(ActionLink(
            type="youtube",
            url=f"https://www.youtube.com/results?search_query={youtube_query}",
            title=f"Xem video v·ªÅ: {search_query[:50]}",
            icon="üé•"
        ))
        
        # Google Search link
        google_query = search_query.replace(" ", "+")
        suggested_actions.append(ActionLink(
            type="google",
            url=f"https://www.google.com/search?q={google_query}",
            title=f"T√¨m tr√™n Google: {search_query[:50]}",
            icon="üîç"
        ))
        
        # Wikipedia link (n·∫øu l√† c√¢u h·ªèi v·ªÅ kh√°i ni·ªám)
        if any(word in request.message.lower() for word in ["l√† g√¨", "what is", "ƒë·ªãnh nghƒ©a", "kh√°i ni·ªám"]):
            wiki_query = search_query.replace(" ", "_")
            suggested_actions.append(ActionLink(
                type="wikipedia",
                url=f"https://en.wikipedia.org/wiki/{wiki_query}",
                title=f"Wikipedia: {search_query[:50]}",
                icon="üìñ"
            ))
        
        return ChatResponse(
            response=response.text,
            model=request.model,
            context_used=context_docs if request.use_rag else None,
            rag_enabled=request.use_rag,
            suggested_actions=suggested_actions
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/rag/prompt/auto", tags=["RAG - Knowledge Base"])
async def add_prompt_auto(request: SimplePromptRequest):
    """
    Th√™m prompt v·ªõi AI t·ª± ƒë·ªông sinh category v√† tags
    
    - **prompt**: N·ªôi dung ki·∫øn th·ª©c c·∫ßn th√™m
    
    AI s·∫Ω t·ª± ƒë·ªông ph√¢n t√≠ch v√† sinh category + tags ph√π h·ª£p
    """
    try:
        # D√πng Gemini ƒë·ªÉ ph√¢n t√≠ch v√† sinh metadata
        analysis_prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr·∫£ v·ªÅ JSON:

VƒÉn b·∫£n: "{request.prompt}"

Tr·∫£ v·ªÅ JSON v·ªõi format:
{{
  "category": "t√™n_danh_m·ª•c",
  "tags": ["tag1", "tag2", "tag3"],
  "summary": "t√≥m t·∫Øt ng·∫Øn g·ªçn"
}}

Categories: programming, ai, machine-learning, education, science, business, health, technology, math, language, general

Ch·ªâ tr·∫£ v·ªÅ JSON."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(analysis_prompt)
        
        # Parse JSON
        import re
        import json as json_lib
        json_text = response.text.strip()
        json_match = re.search(r'\{[^}]+\}', json_text, re.DOTALL)
        
        if json_match:
            metadata_ai = json_lib.loads(json_match.group())
            category = metadata_ai.get("category", "general")
            tags = metadata_ai.get("tags", [])
            summary = metadata_ai.get("summary", "")
        else:
            category = "general"
            tags = []
            summary = ""
        
        metadata = {
            "category": category,
            "tags": tags,
            "type": "prompt",
            "summary": summary
        }
        
        result = vector_db.add_documents(
            documents=[request.prompt],
            metadatas=[metadata]
        )
        
        return {
            "status": "success",
            "message": "ƒê√£ th√™m prompt v·ªõi metadata t·ª± ƒë·ªông",
            "prompt": request.prompt,
            "category": category,
            "tags": tags,
            "summary": summary,
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/rag/prompt", tags=["RAG - Knowledge Base"])
async def add_rag_prompt(request: PromptRAGRequest):
    """
    Th√™m prompt/ki·∫øn th·ª©c v√†o RAG Knowledge Base
    
    - **prompt**: N·ªôi dung ki·∫øn th·ª©c c·∫ßn th√™m
    - **category**: Danh m·ª•c (t·ª± ƒë·ªông sinh n·∫øu ƒë·ªÉ tr·ªëng)
    - **tags**: C√°c tag (t·ª± ƒë·ªông sinh n·∫øu ƒë·ªÉ tr·ªëng)
    
    AI s·∫Ω t·ª± ƒë·ªông ph√¢n t√≠ch v√† sinh category + tags n·∫øu kh√¥ng cung c·∫•p
    """
    try:
        # N·∫øu kh√¥ng c√≥ category ho·∫∑c tags, d√πng AI ƒë·ªÉ sinh t·ª± ƒë·ªông
        category = request.category
        tags = request.tags if request.tags else []
        
        if category == "general" or not tags:
            # D√πng Gemini ƒë·ªÉ ph√¢n t√≠ch v√† sinh metadata
            analysis_prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr·∫£ v·ªÅ JSON v·ªõi format ch√≠nh x√°c:

VƒÉn b·∫£n: "{request.prompt}"

H√£y ph√¢n t√≠ch v√† tr·∫£ v·ªÅ JSON v·ªõi format:
{{
  "category": "t√™n_danh_m·ª•c",
  "tags": ["tag1", "tag2", "tag3"]
}}

C√°c category ph·ªï bi·∫øn: programming, ai, machine-learning, education, science, business, health, technology, math, language

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m text kh√°c."""

            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(analysis_prompt)
            
            try:
                # Parse JSON t·ª´ response
                import re
                json_text = response.text.strip()
                # T√¨m JSON trong response
                json_match = re.search(r'\{[^}]+\}', json_text)
                if json_match:
                    import json as json_lib
                    metadata_ai = json_lib.loads(json_match.group())
                    
                    if category == "general":
                        category = metadata_ai.get("category", "general")
                    
                    if not tags:
                        tags = metadata_ai.get("tags", [])
            except:
                # N·∫øu parse l·ªói, gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh
                pass
        
        metadata = {
            "category": category,
            "tags": tags,
            "type": "prompt"
        }
        
        result = vector_db.add_documents(
            documents=[request.prompt],
            metadatas=[metadata]
        )
        
        return {
            "status": "success",
            "message": "ƒê√£ th√™m prompt v√†o RAG knowledge base",
            "prompt": request.prompt,
            "category": category,
            "tags": tags,
            "auto_generated": request.category == "general" or not request.tags,
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/documents/add", tags=["RAG - Knowledge Base"])
async def add_documents(request: DocumentRequest):
    """Th√™m nhi·ªÅu documents v√†o Vector Database"""
    try:
        result = vector_db.add_documents(
            documents=request.documents,
            metadatas=request.metadatas
        )
        return {
            "status": "success",
            "message": f"ƒê√£ th√™m {result['count']} documents",
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/documents/search", tags=["RAG - Knowledge Base"])
async def search_documents(request: SearchRequest):
    """T√¨m ki·∫øm documents t∆∞∆°ng t·ª± trong Vector Database"""
    try:
        results = vector_db.search(request.query, request.n_results)
        return {
            "query": request.query,
            "results": [
                {
                    "document": doc,
                    "distance": dist,
                    "metadata": meta,
                    "id": doc_id
                }
                for doc, dist, meta, doc_id in zip(
                    results['documents'],
                    results['distances'],
                    results['metadatas'],
                    results['ids']
                )
            ],
            "count": len(results['documents'])
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/api/documents", tags=["RAG - Knowledge Base"])
async def get_all_documents():
    """L·∫•y t·∫•t c·∫£ documents trong Vector Database"""
    try:
        return vector_db.get_all_documents()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.delete("/api/documents", tags=["RAG - Knowledge Base"])
async def delete_all_documents():
    """X√≥a t·∫•t c·∫£ documents trong Vector Database"""
    try:
        return vector_db.delete_all()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/api/documents/count", tags=["RAG - Knowledge Base"])
async def get_document_count():
    """L·∫•y s·ªë l∆∞·ª£ng documents trong Vector Database"""
    return {"count": vector_db.get_count()}

@app.get("/api/rag/stats", tags=["RAG - Knowledge Base"])
async def get_rag_stats():
    """L·∫•y th·ªëng k√™ v·ªÅ RAG Knowledge Base"""
    try:
        all_docs = vector_db.get_all_documents()
        
        # Th·ªëng k√™ theo category
        categories = {}
        for meta in all_docs['metadatas']:
            cat = meta.get('category', 'unknown')
            categories[cat] = categories.get(cat, 0) + 1
        
        return {
            "total_documents": all_docs['count'],
            "categories": categories,
            "status": "active"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/api/models", tags=["Models"])
async def list_models():
    """Li·ªát k√™ c√°c model Gemini c√≥ s·∫µn"""
    try:
        models = []
        for model in genai.list_models():
            if 'generateContent' in model.supported_generation_methods:
                models.append({
                    "name": model.name,
                    "display_name": model.display_name,
                    "description": model.description
                })
        return {"models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

# ============================================================================
# AI EXTENDED APIS
# ============================================================================

class GenerateQuizRequest(BaseModel):
    content: str
    num_questions: int = 10
    difficulty: str = "medium"
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "content": "Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh b·∫≠c cao...",
                "num_questions": 10,
                "difficulty": "medium"
            }
        }
    )

class QuizQuestion(BaseModel):
    question: str
    a: str
    b: str
    c: str
    d: str
    correct: str

class GenerateQuizResponse(BaseModel):
    questions: List[QuizQuestion]

class SummarizeRequest(BaseModel):
    content: str
    max_length: Optional[int] = 200

class SummarizeResponse(BaseModel):
    summary: str
    original_length: int
    summary_length: int

class ExplainRequest(BaseModel):
    question: str
    context: Optional[str] = None

class ExplainResponse(BaseModel):
    explanation: str
    examples: Optional[List[str]] = None

class IngestRequest(BaseModel):
    file_url: str
    title: Optional[str] = None

class IngestResponse(BaseModel):
    status: str
    message: str
    documents_added: int

@app.post("/api/ai/generate-quiz", response_model=GenerateQuizResponse, tags=["AI - Extended"])
async def generate_quiz(request: GenerateQuizRequest):
    """T·∫°o c√¢u h·ªèi tr·∫Øc nghi·ªám t·ª± ƒë·ªông t·ª´ n·ªôi dung b√†i h·ªçc"""
    try:
        import re
        import json as json_lib
        
        if request.num_questions < 1 or request.num_questions > 50:
            raise HTTPException(status_code=400, detail="S·ªë c√¢u h·ªèi ph·∫£i t·ª´ 1-50")
        
        difficulty_map = {
            "easy": "d·ªÖ, c∆° b·∫£n",
            "medium": "trung b√¨nh",
            "hard": "kh√≥, n√¢ng cao"
        }
        
        difficulty_desc = difficulty_map.get(request.difficulty.lower(), "trung b√¨nh")
        
        prompt = f"""D·ª±a tr√™n n·ªôi dung sau, h√£y t·∫°o {request.num_questions} c√¢u h·ªèi tr·∫Øc nghi·ªám v·ªõi ƒë·ªô kh√≥ {difficulty_desc}.

N·ªôi dung:
{request.content}

Y√™u c·∫ßu:
- T·∫°o ƒë√∫ng {request.num_questions} c√¢u h·ªèi
- M·ªói c√¢u c√≥ 4 ƒë√°p √°n A, B, C, D
- Ch·ªâ 1 ƒë√°p √°n ƒë√∫ng
- C√¢u h·ªèi ph·∫£i li√™n quan tr·ª±c ti·∫øp ƒë·∫øn n·ªôi dung
- Tr·∫£ v·ªÅ JSON array v·ªõi format:

[
  {{
    "question": "C√¢u h·ªèi?",
    "a": "ƒê√°p √°n A",
    "b": "ƒê√°p √°n B",
    "c": "ƒê√°p √°n C",
    "d": "ƒê√°p √°n D",
    "correct": "A"
  }}
]

CH·ªà TR·∫¢ V·ªÄ JSON, KH√îNG TH√äM TEXT KH√ÅC."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        json_text = response.text.strip()
        json_match = re.search(r'\[.*\]', json_text, re.DOTALL)
        if not json_match:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ parse JSON t·ª´ AI response")
        
        questions_data = json_lib.loads(json_match.group())
        
        questions = []
        for q in questions_data:
            questions.append(QuizQuestion(
                question=q['question'],
                a=q['a'],
                b=q['b'],
                c=q['c'],
                d=q['d'],
                correct=q['correct'].upper()
            ))
        
        return GenerateQuizResponse(questions=questions)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/ai/summarize", response_model=SummarizeResponse, tags=["AI - Extended"])
async def summarize(request: SummarizeRequest):
    """T√≥m t·∫Øt vƒÉn b·∫£n"""
    try:
        prompt = f"""H√£y t√≥m t·∫Øt vƒÉn b·∫£n sau trong kho·∫£ng {request.max_length} t·ª´:

{request.content}

Y√™u c·∫ßu:
- Gi·ªØ l·∫°i √Ω ch√≠nh
- Ng·∫Øn g·ªçn, s√∫c t√≠ch
- D·ªÖ hi·ªÉu
- Kh√¥ng th√™m th√¥ng tin ngo√†i vƒÉn b·∫£n g·ªëc"""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        summary = response.text.strip()
        
        return SummarizeResponse(
            summary=summary,
            original_length=len(request.content.split()),
            summary_length=len(summary.split())
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/ai/explain", response_model=ExplainResponse, tags=["AI - Extended"])
async def explain(request: ExplainRequest):
    """Gi·∫£i th√≠ch nh∆∞ m·ªôt gi√°o vi√™n"""
    try:
        import re
        
        context_text = f"\nNg·ªØ c·∫£nh: {request.context}" if request.context else ""
        
        prompt = f"""B·∫°n l√† m·ªôt gi√°o vi√™n gi·ªèi. H√£y gi·∫£i th√≠ch c√¢u h·ªèi sau m·ªôt c√°ch d·ªÖ hi·ªÉu, chi ti·∫øt:{context_text}

C√¢u h·ªèi: {request.question}

Y√™u c·∫ßu:
- Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu
- S·ª≠ d·ª•ng v√≠ d·ª• c·ª• th·ªÉ
- Chia nh·ªè th√†nh c√°c b∆∞·ªõc n·∫øu c·∫ßn
- Gi·ªçng ƒëi·ªáu th√¢n thi·ªán, khuy·∫øn kh√≠ch h·ªçc t·∫≠p

Sau ph·∫ßn gi·∫£i th√≠ch, h√£y ƒë∆∞a ra 2-3 v√≠ d·ª• minh h·ªça."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        explanation = response.text.strip()
        
        examples = []
        if "V√≠ d·ª•" in explanation or "Example" in explanation:
            parts = re.split(r'V√≠ d·ª• \d+:|Example \d+:', explanation)
            if len(parts) > 1:
                examples = [ex.strip() for ex in parts[1:]]
        
        return ExplainResponse(
            explanation=explanation,
            examples=examples if examples else None
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/ai/ingest", response_model=IngestResponse, tags=["AI - Extended"])
async def ingest_document(request: IngestRequest):
    """Ingest t√†i li·ªáu v√†o RAG Vector Database"""
    try:
        # Simplified: ch·ªâ x·ª≠ l√Ω text content
        # Trong production c·∫ßn th√™m PDF, DOC parsing
        
        # Gi·∫£ l·∫≠p extract text
        text_content = f"N·ªôi dung t·ª´ {request.file_url}"
        
        # Chia nh·ªè th√†nh chunks
        words = text_content.split()
        chunk_size = 500
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
        
        # Th√™m v√†o vector DB
        metadatas = [{
            "source": request.file_url,
            "title": request.title or "Untitled",
            "type": "document"
        } for _ in chunks]
        
        result = vector_db.add_documents(
            documents=chunks,
            metadatas=metadatas
        )
        
        return IngestResponse(
            status="success",
            message=f"ƒê√£ ingest {len(chunks)} chunks v√†o RAG database",
            documents_added=len(chunks)
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

# ============================================================================
# CREDENTIAL MANAGER INTEGRATION
# ============================================================================

# Import credential API router
try:
    from credential_api import router as credential_router
    app.include_router(credential_router)
    CREDENTIAL_API_AVAILABLE = True
    print("‚úÖ Credential Manager API loaded")
except ImportError as e:
    CREDENTIAL_API_AVAILABLE = False
    print(f"‚ö†Ô∏è  Credential Manager API not available: {e}")
    print("   System will work without AI semantic search for credentials")
    print("   To enable: pip install chromadb sentence-transformers")

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    print("=" * 60)
    print("üöÄ Starting AI Chat Service with RAG")
    print("=" * 60)
    print(f"üìç Server: http://localhost:{port}")
    print(f"üìö Swagger UI: http://localhost:{port}/docs")
    print(f"üìä Vector DB Documents: {vector_db.get_count()}")
    if CREDENTIAL_API_AVAILABLE:
        print(f"üîê Credential Manager: Enabled")
    print("=" * 60)
    uvicorn.run("main:app", host="0.0.0.0", port=port, reload=True)
