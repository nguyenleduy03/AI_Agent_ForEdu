"""
FastAPI AI Chat Service with RAG (Retrieval-Augmented Generation)
T·∫•t c·∫£ trong 1 file - Gemini 2.5 Flash + Vector Database
"""
import sys
import io
import os

# Fix Unicode encoding for Windows console - MUST BE FIRST!
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    # Also set console code page to UTF-8
    os.system('chcp 65001 >nul 2>&1')

from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, ConfigDict
from typing import List, Optional, Dict
import google.generativeai as genai
import os
from dotenv import load_dotenv
import json
import math
import requests
from datetime import datetime, timedelta
try:
    from youtube_helper import search_youtube_video, get_youtube_watch_url, get_youtube_embed_url
    YOUTUBE_HELPER_AVAILABLE = True
except ImportError:
    YOUTUBE_HELPER_AVAILABLE = False
    print("‚ö†Ô∏è  YouTube helper not available. Video search will use fallback.")

try:
    from groq_helper import GroqClient
    GROQ_HELPER_AVAILABLE = True
except ImportError:
    GROQ_HELPER_AVAILABLE = False
    print("‚ö†Ô∏è  Groq helper not available.")

try:
    from agent_features import AgentFeatures
    AGENT_FEATURES_AVAILABLE = True
except ImportError:
    AGENT_FEATURES_AVAILABLE = False
    print("‚ö†Ô∏è  Agent features not available.")

try:
    from google_cloud_agent import GoogleCloudAgent
    GOOGLE_CLOUD_AGENT_AVAILABLE = True
except ImportError:
    GOOGLE_CLOUD_AGENT_AVAILABLE = False
    print("‚ö†Ô∏è  Google Cloud Agent not available.")

try:
    from document_intelligence_service import DocumentIntelligence, create_document_intelligence_service
    DOCUMENT_INTELLIGENCE_AVAILABLE = True
except ImportError:
    DOCUMENT_INTELLIGENCE_AVAILABLE = False
    print("‚ö†Ô∏è  Document Intelligence not available.")

# LangChain Agent
try:
    from langchain_agent_simple import create_simple_langchain_agent, SimpleLangChainAgent
    LANGCHAIN_AGENT_AVAILABLE = True
except ImportError:
    LANGCHAIN_AGENT_AVAILABLE = False
    print("‚ö†Ô∏è  LangChain Agent not available. Install: pip install langchain langchain-google-genai")

# Image analysis tools for non-vision models (Groq)
# Using OCR.space free API (25,000 requests/month)
IMAGE_OCR_AVAILABLE = True  # Always available via API
IMAGE_CAPTION_AVAILABLE = False
print("‚úÖ OCR.space API available for Groq image reading")

# ============================================================================
# VECTOR DATABASE CLASS
# ============================================================================

def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """T√≠nh cosine similarity gi·ªØa 2 vectors"""
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    magnitude1 = math.sqrt(sum(a * a for a in vec1))
    magnitude2 = math.sqrt(sum(b * b for b in vec2))
    
    if magnitude1 == 0 or magnitude2 == 0:
        return 0.0
    
    return dot_product / (magnitude1 * magnitude2)

class SimpleVectorDB:
    def __init__(self, storage_file: str = "vector_db.json"):
        """Kh·ªüi t·∫°o Simple Vector Database"""
        self.storage_file = storage_file
        self.documents = []
        self.load()
    
    def load(self):
        """Load data t·ª´ file"""
        if os.path.exists(self.storage_file):
            with open(self.storage_file, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
    
    def save(self):
        """L∆∞u data v√†o file"""
        with open(self.storage_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
    
    def add_documents(self, documents: List[str], metadatas: List[Dict] = None, ids: List[str] = None):
        """Th√™m documents v√†o database"""
        if ids is None:
            start_id = len(self.documents)
            ids = [f"doc_{start_id + i}" for i in range(len(documents))]
        
        if metadatas is None:
            metadatas = [{"source": "manual"} for _ in documents]
        
        # T·∫°o embeddings
        for doc, metadata, doc_id in zip(documents, metadatas, ids):
            result = genai.embed_content(
                model="models/text-embedding-004",
                content=doc,
                task_type="retrieval_document"
            )
            
            self.documents.append({
                "id": doc_id,
                "document": doc,
                "embedding": result['embedding'],
                "metadata": metadata
            })
        
        self.save()
        return {"status": "success", "count": len(documents)}
    
    def search(self, query: str, n_results: int = 5) -> Dict:
        """T√¨m ki·∫øm documents t∆∞∆°ng t·ª±"""
        if not self.documents:
            return {"documents": [], "distances": [], "metadatas": [], "ids": []}
        
        # T·∫°o embedding cho query
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=query,
            task_type="retrieval_query"
        )
        query_embedding = result['embedding']
        
        # T√≠nh similarity v·ªõi t·∫•t c·∫£ documents
        similarities = []
        for doc in self.documents:
            similarity = cosine_similarity(query_embedding, doc['embedding'])
            similarities.append({
                "document": doc['document'],
                "distance": 1 - similarity,
                "metadata": doc['metadata'],
                "id": doc['id'],
                "similarity": similarity
            })
        
        # S·∫Øp x·∫øp theo similarity
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        top_results = similarities[:n_results]
        
        return {
            "documents": [r['document'] for r in top_results],
            "distances": [r['distance'] for r in top_results],
            "metadatas": [r['metadata'] for r in top_results],
            "ids": [r['id'] for r in top_results]
        }
    
    def delete_all(self):
        """X√≥a t·∫•t c·∫£ documents"""
        self.documents = []
        self.save()
        return {"status": "success", "message": "All documents deleted"}
    
    def get_count(self) -> int:
        """L·∫•y s·ªë l∆∞·ª£ng documents"""
        return len(self.documents)
    
    def get_all_documents(self) -> Dict:
        """L·∫•y t·∫•t c·∫£ documents"""
        return {
            "documents": [doc['document'] for doc in self.documents],
            "metadatas": [doc['metadata'] for doc in self.documents],
            "ids": [doc['id'] for doc in self.documents],
            "count": len(self.documents)
        }

# ============================================================================
# FASTAPI APP SETUP
# ============================================================================

# Load environment variables
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
DEFAULT_AI_MODEL = os.getenv("DEFAULT_AI_MODEL", "gemini")

# Validate API keys
if DEFAULT_AI_MODEL == "gemini":
    if not GEMINI_API_KEY or GEMINI_API_KEY == "your_gemini_api_key_here":
        raise ValueError("‚ö†Ô∏è  GEMINI_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env\nL·∫•y API key t·∫°i: https://aistudio.google.com/apikey")
    genai.configure(api_key=GEMINI_API_KEY)
    print(f"‚úÖ Using Gemini AI")
elif DEFAULT_AI_MODEL == "groq":
    if not GROQ_API_KEY or GROQ_API_KEY == "your_groq_api_key_here":
        raise ValueError("‚ö†Ô∏è  GROQ_API_KEY kh√¥ng ƒë∆∞·ª£c t√¨m th·∫•y trong file .env\nL·∫•y API key t·∫°i: https://console.groq.com/")
    print(f"‚úÖ Using Groq AI")
else:
    # Fallback to Gemini if not specified
    if GEMINI_API_KEY and GEMINI_API_KEY != "your_gemini_api_key_here":
        genai.configure(api_key=GEMINI_API_KEY)
        print(f"‚ö†Ô∏è  Invalid DEFAULT_AI_MODEL, falling back to Gemini")
    else:
        raise ValueError(f"‚ö†Ô∏è  No valid API key found")

# Initialize AI clients
groq_client = None
if GROQ_HELPER_AVAILABLE and GROQ_API_KEY and GROQ_API_KEY != "your_groq_api_key_here":
    groq_client = GroqClient(GROQ_API_KEY)
    print("‚úÖ Groq client initialized")

# Initialize FastAPI app
app = FastAPI(
    title="AI Chat Service with RAG",
    description="API chat v·ªõi Gemini 2.5 Flash + Vector Database RAG",
    version="2.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Vector Database
vector_db = SimpleVectorDB(storage_file="knowledge_base.json")

# Initialize Agent Features
if AGENT_FEATURES_AVAILABLE:
    agent_features = AgentFeatures(spring_boot_url="http://localhost:8080")
    print("‚úÖ Agent Features initialized")
else:
    agent_features = None
    print("‚ö†Ô∏è  Agent Features not initialized")

# Initialize Google Cloud Agent
if GOOGLE_CLOUD_AGENT_AVAILABLE:
    google_cloud_agent = GoogleCloudAgent(google_cloud_url="http://localhost:8004")
    print("‚úÖ Google Cloud Agent initialized")
else:
    google_cloud_agent = None
    print("‚ö†Ô∏è  Google Cloud Agent not initialized")

# Initialize LangChain Agent - DISABLED (kh√¥ng c·∫ßn thi·∫øt cho d·ª± √°n n√†y)
# K·∫øt lu·∫≠n: LangChain ph·ª©c t·∫°p 8/10, gi√° tr·ªã th·ª±c t·∫ø th·∫•p
langchain_agent = None
print("‚ÑπÔ∏è  LangChain Agent disabled - using direct Gemini API instead")

# Initialize Image Analysis models (lazy loading)
# No longer using EasyOCR or BLIP - using pytesseract instead

def extract_image_content(image_base64: str, image_mime_type: str) -> Dict[str, str]:
    """
    Extract text from image using OCR.space API
    Also provides basic image description for non-text images
    Returns: {
        "description": "Basic image info",
        "text_content": "Extracted text from image",
        "success": True/False
    }
    """
    try:
        import base64
        from PIL import Image
        import io
        
        # Decode image
        image_data = base64.b64decode(image_base64)
        image = Image.open(io.BytesIO(image_data))
        
        # Get image info
        width, height = image.size
        img_format = image.format or "Unknown"
        mode = image.mode  # RGB, RGBA, L (grayscale), etc.
        
        result = {
            "description": f"·∫¢nh {img_format}, k√≠ch th∆∞·ªõc {width}x{height} pixels, mode: {mode}",
            "text_content": "",
            "success": False
        }
        
        # Use OCR.space free API with Vietnamese support
        try:
            print("üîç Using OCR.space API for text extraction...")
            import requests
            
            ocr_url = "https://api.ocr.space/parse/image"
            
            # Try Vietnamese first, then English
            for lang in ['vie', 'eng']:
                print(f"   Trying language: {lang}")
                payload = {
                    'base64Image': f'data:{image_mime_type};base64,{image_base64}',
                    'language': lang,
                    'isOverlayRequired': False,
                    'detectOrientation': True,
                    'scale': True,
                    'OCREngine': 2  # Engine 2 for better accuracy
                }
                
                response = requests.post(ocr_url, data=payload, timeout=30)
                ocr_result = response.json()
                
                # Check for processing error
                is_error = ocr_result.get('IsErroredOnProcessing', False)
                if is_error:
                    error_msg = ocr_result.get('ErrorMessage', 'Unknown error')
                    # ErrorMessage can be string or list
                    if isinstance(error_msg, list):
                        error_msg = error_msg[0] if error_msg else 'Unknown error'
                    elif not isinstance(error_msg, str):
                        error_msg = str(error_msg)
                    print(f"   ‚ö†Ô∏è OCR error ({lang}): {error_msg}")
                    continue
                
                # Extract text from all parsed results
                text_parts = []
                parsed_results = ocr_result.get('ParsedResults', [])
                
                # Ensure parsed_results is a list
                if not isinstance(parsed_results, list):
                    print(f"   ‚ö†Ô∏è ParsedResults is not a list: {type(parsed_results)}")
                    continue
                
                for parsed_result in parsed_results:
                    # Ensure parsed_result is a dict
                    if not isinstance(parsed_result, dict):
                        continue
                    text = parsed_result.get('ParsedText', '').strip()
                    if text:
                        text_parts.append(text)
                
                full_text = '\n'.join(text_parts)
                
                if full_text and len(full_text) > 5:  # At least some meaningful text
                    result["text_content"] = full_text
                    result["success"] = True
                    print(f"‚úÖ OCR extracted {len(full_text)} characters ({lang})")
                    break  # Found text, stop trying other languages
            
            # If no text found, provide helpful context
            if not result["success"] or not result["text_content"]:
                result["text_content"] = f"""[Kh√¥ng t√¨m th·∫•y text trong ·∫£nh]

Th√¥ng tin ·∫£nh:
- ƒê·ªãnh d·∫°ng: {img_format}
- K√≠ch th∆∞·ªõc: {width}x{height} pixels
- Ch·∫ø ƒë·ªô m√†u: {mode}

L∆∞u √Ω: Groq kh√¥ng th·ªÉ ph√¢n t√≠ch n·ªôi dung h√¨nh ·∫£nh (ch·ªâ ƒë·ªçc ƒë∆∞·ª£c text).
N·∫øu b·∫°n c·∫ßn ph√¢n t√≠ch ·∫£nh chi ti·∫øt, vui l√≤ng chuy·ªÉn sang Gemini."""
                result["success"] = True  # Still return success so we can respond
                print(f"‚ÑπÔ∏è No text found in image, returning image info")
                
        except requests.exceptions.Timeout:
            print(f"‚ö†Ô∏è OCR timeout")
            result["text_content"] = "[OCR timeout - vui l√≤ng th·ª≠ l·∫°i]"
        except Exception as e:
            print(f"‚ö†Ô∏è OCR error: {e}")
            result["text_content"] = f"[OCR error: {str(e)[:100]}]"
        
        return result
        
    except Exception as e:
        print(f"‚ùå Image extraction error: {e}")
        return {
            "description": "Error processing image",
            "text_content": f"[Error: {str(e)[:100]}]",
            "success": False,
            "error": str(e)
        }

# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_user_id_from_token(token: str) -> Optional[int]:
    """
    Get user_id from JWT token by calling Spring Boot API
    
    Args:
        token: JWT token string
    
    Returns:
        user_id (int) or None if failed
    """
    if not token:
        return None
    
    try:
        # Call Spring Boot API to get user profile
        headers = {"Authorization": f"Bearer {token}"}
        response = requests.get(
            "http://localhost:8080/api/auth/profile",
            headers=headers,
            timeout=5
        )
        
        if response.status_code == 200:
            user_data = response.json()
            user_id = user_data.get('id')
            print(f"‚úÖ Got user_id from token: {user_id}")
            return user_id
        else:
            print(f"‚ö†Ô∏è  Failed to get user from token: {response.status_code}")
            return None
    except Exception as e:
        print(f"‚ùå Error getting user_id from token: {e}")
        return None

# ============================================================================
# PYDANTIC MODELS
# ============================================================================

class ChatRequest(BaseModel):
    message: str
    model: str = "gemini-flash-latest"  # Use latest flash model (1,500 requests/day)
    ai_provider: str = "gemini"  # "gemini" or "groq"
    use_rag: bool = True
    image_base64: Optional[str] = None  # Base64 encoded image for vision analysis
    image_mime_type: Optional[str] = None  # e.g., "image/jpeg", "image/png"
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "message": "Gi·∫£i th√≠ch v·ªÅ AI l√† g√¨?",
                "model": "gemini-2.5-flash",
                "ai_provider": "gemini",
                "use_rag": True,
                "image_base64": None,
                "image_mime_type": None
            }
        }
    )

class ActionLink(BaseModel):
    type: str  # "youtube", "google", "wikipedia"
    url: str
    title: str
    icon: str

class ToolAction(BaseModel):
    tool: str  # "play_youtube", "search_youtube", "search_google", "open_wikipedia"
    query: str
    url: str
    auto_execute: bool = True
    video_id: Optional[str] = None  # YouTube video ID
    embed_url: Optional[str] = None  # URL ƒë·ªÉ embed video

class SendEmailRequest(BaseModel):
    """Request model for sending email after user confirmation"""
    to: str
    subject: str
    body: str
    user_id: Optional[int] = None
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "to": "teacher@tvu.edu.vn",
                "subject": "Xin ngh·ªâ h·ªçc",
                "body": "K√≠nh g·ª≠i th·∫ßy, em xin ph√©p ngh·ªâ h·ªçc...",
                "user_id": 1
            }
        }
    )

class EmailDraft(BaseModel):
    """Email draft for preview"""
    to: str
    subject: str
    body: str
    user_id: Optional[int] = None
    
    model_config = ConfigDict(
        populate_by_name=True,
        # Ensure snake_case in JSON output
    )

class ChatResponse(BaseModel):
    response: str
    model: str
    context_used: Optional[List[str]] = None
    rag_enabled: bool = False
    suggested_actions: Optional[List[ActionLink]] = None  # Links g·ª£i √Ω
    tool_action: Optional[ToolAction] = None  # Action t·ª± ƒë·ªông th·ª±c thi
    email_draft: Optional[EmailDraft] = None  # Email draft for preview
    
    model_config = ConfigDict(
        populate_by_name=True,
        # Ensure snake_case in JSON output
    )

class DocumentRequest(BaseModel):
    documents: List[str]
    metadatas: Optional[List[dict]] = None
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "documents": [
                    "AI (Artificial Intelligence) l√† tr√≠ tu·ªá nh√¢n t·∫°o.",
                    "Machine Learning l√† m·ªôt nh√°nh c·ªßa AI."
                ]
            }
        }
    )

class PromptRAGRequest(BaseModel):
    prompt: str
    category: Optional[str] = "general"
    tags: Optional[List[str]] = None
    
    model_config = ConfigDict(
        json_schema_extra={
            "examples": [
                {
                    "prompt": "Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh d·ªÖ h·ªçc v√† m·∫°nh m·∫Ω.",
                    "category": "programming",
                    "tags": ["python", "programming", "ai"]
                },
                {
                    "prompt": "Machine Learning gi√∫p m√°y t√≠nh h·ªçc t·ª´ d·ªØ li·ªáu m√† kh√¥ng c·∫ßn l·∫≠p tr√¨nh c·ª• th·ªÉ."
                }
            ]
        }
    )

class SimplePromptRequest(BaseModel):
    prompt: str
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "prompt": "FastAPI l√† framework Python hi·ªán ƒë·∫°i ƒë·ªÉ x√¢y d·ª±ng API nhanh v√† d·ªÖ s·ª≠ d·ª•ng."
            }
        }
    )

class SearchRequest(BaseModel):
    query: str
    n_results: int = 5

# ============================================================================
# API ENDPOINTS
# ============================================================================

@app.get("/", tags=["Health"])
async def root():
    """Health check endpoint"""
    doc_count = vector_db.get_count()
    return {
        "status": "running",
        "service": "AI Chat Service with RAG",
        "version": "2.0.0",
        "vector_db_documents": doc_count
    }

def detect_tool_intent(message: str) -> Optional[ToolAction]:
    """Ph√°t hi·ªán intent ƒë·ªÉ t·ª± ƒë·ªông th·ª±c thi tool"""
    message_lower = message.lower()
    
    # Intent: Ph√°t video YouTube (t·ª± ƒë·ªông t√¨m v√† ph√°t)
    # Ch·ªâ c·∫ßn c√≥ trigger "ph√°t", "play", "ch∆°i", "b·∫≠t" l√† ƒë·ªß
    play_triggers = ["ph√°t", "play", "ch∆°i", "b·∫≠t"]
    
    is_play_intent = any(trigger in message_lower for trigger in play_triggers)
    
    if is_play_intent:
        # Extract query - lo·∫°i b·ªè c√°c t·ª´ trigger
        query = message_lower
        for trigger in play_triggers + ["m·ªü", "cho t√¥i", "cho toi", "m·ªôt", "mot", "b·∫•t k·ª≥", "bat ky", "video", "youtube", "t·∫≠p"]:
            query = query.replace(trigger, "")
        query = query.strip()
        
        if query and YOUTUBE_HELPER_AVAILABLE:
            # T√¨m video tr√™n YouTube
            try:
                video_id = search_youtube_video(query)
                
                if video_id:
                    watch_url = get_youtube_watch_url(video_id, autoplay=True)
                    embed_url = get_youtube_embed_url(video_id, autoplay=True)
                    
                    return ToolAction(
                        tool="play_youtube",
                        query=query,
                        url=watch_url,
                        video_id=video_id,
                        embed_url=embed_url,
                        auto_execute=True
                    )
            except Exception as e:
                print(f"Error searching YouTube: {e}")
                # Fallback to search
                pass
    
    # Intent: M·ªü YouTube search (kh√¥ng t·ª± ƒë·ªông ph√°t)
    youtube_triggers = ["m·ªü video", "xem video", "open video", "show video", "youtube", "t√¨m video"]
    for trigger in youtube_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").replace("v·ªÅ", "").strip()
            if query:
                youtube_url = f"https://www.youtube.com/results?search_query={query.replace(' ', '+')}"
                return ToolAction(
                    tool="search_youtube",
                    query=query,
                    url=youtube_url,
                    auto_execute=True
                )
    
    # Intent: Search Google
    google_triggers = ["t√¨m ki·∫øm", "search", "google", "tra google", "t√¨m tr√™n google"]
    for trigger in google_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").strip()
            if query:
                google_url = f"https://www.google.com/search?q={query.replace(' ', '+')}"
                return ToolAction(
                    tool="search_google",
                    query=query,
                    url=google_url,
                    auto_execute=True
                )
    
    # Intent: Open Wikipedia
    wiki_triggers = ["wikipedia", "wiki", "tra wikipedia"]
    for trigger in wiki_triggers:
        if trigger in message_lower:
            query = message_lower.replace(trigger, "").strip()
            if query:
                wiki_url = f"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}"
                return ToolAction(
                    tool="open_wikipedia",
                    query=query,
                    url=wiki_url,
                    auto_execute=True
                )
    
    return None


# ============================================================================
# TEST ENDPOINT - TVU Schedule Direct
# ============================================================================
class TVUTestRequest(BaseModel):
    mssv: str
    password: str
    message: str = "H√¥m nay t√¥i h·ªçc g√¨?"

@app.post("/api/test/tvu-schedule", tags=["Test"])
async def test_tvu_schedule(request: TVUTestRequest):
    """
    Test endpoint - L·∫•y TKB tr·ª±c ti·∫øp t·ª´ TVU (kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p h·ªá th·ªëng)
    """
    try:
        from tvu_scraper import TVUScraper
        from datetime import datetime, timedelta
        import re
        
        scraper = TVUScraper()
        
        # Login
        if not scraper.login(request.mssv, request.password):
            return {"success": False, "message": "‚ùå ƒêƒÉng nh·∫≠p TVU th·∫•t b·∫°i!"}
        
        # Get schedule
        schedules = scraper.get_schedule()
        
        if not schedules:
            return {"success": False, "message": "üìÖ Kh√¥ng t√¨m th·∫•y l·ªãch h·ªçc tu·∫ßn n√†y."}
        
        # Filter by day if message mentions specific day
        message_lower = request.message.lower()
        today = datetime.now()
        day_map = {
            'th·ª© 2': 'MONDAY', 'th·ª© hai': 'MONDAY', 't2': 'MONDAY',
            'th·ª© 3': 'TUESDAY', 'th·ª© ba': 'TUESDAY', 't3': 'TUESDAY',
            'th·ª© 4': 'WEDNESDAY', 'th·ª© t∆∞': 'WEDNESDAY', 't4': 'WEDNESDAY',
            'th·ª© 5': 'THURSDAY', 'th·ª© nƒÉm': 'THURSDAY', 't5': 'THURSDAY',
            'th·ª© 6': 'FRIDAY', 'th·ª© s√°u': 'FRIDAY', 't6': 'FRIDAY',
            'th·ª© 7': 'SATURDAY', 'th·ª© b·∫£y': 'SATURDAY', 't7': 'SATURDAY',
            'ch·ªß nh·∫≠t': 'SUNDAY', 'cn': 'SUNDAY'
        }
        
        # Check for relative dates
        target_day = None
        day_label = "tu·∫ßn n√†y"
        
        # Try to extract specific date first (DD/MM/YYYY or DD-MM-YYYY)
        date_pattern = r'(?:ng√†y\s+)?(\d{1,2})[/-](\d{1,2})[/-](\d{4})'
        date_match = re.search(date_pattern, message_lower)
        if date_match:
            try:
                day, month, year = int(date_match.group(1)), int(date_match.group(2)), int(date_match.group(3))
                target_date = datetime(year, month, day)
                target_day = target_date.strftime('%A').upper()
                date_str = target_date.strftime('%d/%m/%Y')
                day_name = target_date.strftime('%A')
                
                # Map to Vietnamese day name
                day_names = {
                    'Monday': 'Th·ª© 2',
                    'Tuesday': 'Th·ª© 3',
                    'Wednesday': 'Th·ª© 4',
                    'Thursday': 'Th·ª© 5',
                    'Friday': 'Th·ª© 6',
                    'Saturday': 'Th·ª© 7',
                    'Sunday': 'Ch·ªß nh·∫≠t'
                }
                vn_day = day_names.get(day_name, day_name)
                day_label = f"{vn_day} ({date_str})"
            except (ValueError, OverflowError):
                pass
        
        # H√¥m qua
        if target_day is None and ('h√¥m qua' in message_lower or 'hom qua' in message_lower):
            yesterday = today - timedelta(days=1)
            target_day = yesterday.strftime('%A').upper()
            date_str = yesterday.strftime('%d/%m/%Y')
            day_label = f"h√¥m qua ({date_str})"
        # Mai
        elif target_day is None and 'mai' in message_lower:
            tomorrow = today + timedelta(days=1)
            target_day = tomorrow.strftime('%A').upper()
            date_str = tomorrow.strftime('%d/%m/%Y')
            day_label = f"mai ({date_str})"
        # M·ªët (2 ng√†y sau)
        elif target_day is None and ('m·ªët' in message_lower or 'mot' in message_lower):
            two_days = today + timedelta(days=2)
            target_day = two_days.strftime('%A').upper()
            date_str = two_days.strftime('%d/%m/%Y')
            day_label = f"m·ªët ({date_str})"
        # Kia (3 ng√†y sau)
        elif target_day is None and 'kia' in message_lower:
            three_days = today + timedelta(days=3)
            target_day = three_days.strftime('%A').upper()
            date_str = three_days.strftime('%d/%m/%Y')
            day_label = f"kia ({date_str})"
        # H√¥m nay
        elif target_day is None and ('h√¥m nay' in message_lower or 'hom nay' in message_lower or 'today' in message_lower or 'hnay' in message_lower):
            target_day = today.strftime('%A').upper()
            date_str = today.strftime('%d/%m/%Y')
            day_label = f"h√¥m nay ({date_str})"
        elif target_day is None:
            # Check for specific day name
            for keyword, day in day_map.items():
                if keyword in message_lower:
                    target_day = day
                    day_label = keyword
                    break
        
        # Filter schedules by target day
        if target_day:
            schedules = [s for s in schedules if s.get('day_of_week') == target_day]
        
        if not schedules:
            return {
                "success": True,
                "message": f"üìÖ {day_label.capitalize()} b·∫°n kh√¥ng c√≥ l·ªõp n√†o.",
                "schedules": []
            }
        
        # Format response
        message_text = f"üìÖ **L·ªãch h·ªçc {day_label}:**\n\n"
        for schedule in schedules:
            day_vn = {
                'MONDAY': 'Th·ª© 2', 'TUESDAY': 'Th·ª© 3', 'WEDNESDAY': 'Th·ª© 4',
                'THURSDAY': 'Th·ª© 5', 'FRIDAY': 'Th·ª© 6', 'SATURDAY': 'Th·ª© 7', 'SUNDAY': 'CN'
            }.get(schedule.get('day_of_week', ''), '')
            
            start_time = schedule.get('start_time', '')[:5]
            end_time = schedule.get('end_time', '')[:5]
            
            message_text += f"üïê **{start_time} - {end_time}** ({day_vn})\n"
            message_text += f"   üìö {schedule.get('subject', 'N/A')}\n"
            message_text += f"   üè´ Ph√≤ng {schedule.get('room', 'N/A')}\n"
            if schedule.get('teacher'):
                message_text += f"   üë®‚Äçüè´ {schedule['teacher']}\n"
            message_text += "\n"
        
        return {
            "success": True,
            "message": message_text,
            "schedules": schedules
        }
        
    except Exception as e:
        return {"success": False, "message": f"‚ùå L·ªói: {str(e)}"}


@app.post("/api/chat", tags=["Chat"])
async def chat(request: ChatRequest, authorization: Optional[str] = Header(None)):
    """
    Chat v·ªõi Gemini AI (c√≥ h·ªó tr·ª£ RAG + Agent Features)
    
    - **message**: Tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
    - **model**: Model Gemini s·ª≠ d·ª•ng (m·∫∑c ƒë·ªãnh: gemini-2.5-flash)
    - **use_rag**: S·ª≠ d·ª•ng RAG ƒë·ªÉ tƒÉng c∆∞·ªùng context (m·∫∑c ƒë·ªãnh: true)
    
    Agent Features (t·ª± ƒë·ªông):
    - Xem th·ªùi kh√≥a bi·ªÉu (t·ª± ƒë·ªông l·∫•y t·ª´ trang tr∆∞·ªùng)
    - Xem ƒëi·ªÉm s·ªë
    - G·ª≠i email
    
    Models ƒë∆∞·ª£c khuy·∫øn ngh·ªã:
    - gemini-2.5-flash (M·ªöI NH·∫§T - Nhanh, stable)
    - gemini-2.5-pro (M·∫°nh nh·∫•t)
    - gemini-flash-latest (Lu√¥n d√πng version m·ªõi nh·∫•t)
    """
    try:
        # Extract token from Authorization header
        token = None
        user_id = None
        if authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            # Get user_id from token
            user_id = get_user_id_from_token(token)
        
        print(f"\n{'='*60}")
        print(f"üì® NEW CHAT REQUEST")
        print(f"Message: {request.message}")
        print(f"AI Provider: {request.ai_provider}")
        print(f"Has token: {token is not None}")
        print(f"User ID: {user_id}")
        print(f"AGENT_FEATURES_AVAILABLE: {AGENT_FEATURES_AVAILABLE}")
        print(f"agent_features: {agent_features is not None if 'agent_features' in globals() else 'NOT DEFINED'}")
        
        # Debug email intent detection
        if AGENT_FEATURES_AVAILABLE and agent_features:
            email_intent = agent_features.detect_email_intent(request.message)
            gmail_send_intent = agent_features.detect_gmail_send_intent(request.message)
            print(f"üîç Email Intent: {email_intent}")
            print(f"üîç Gmail Send Intent: {gmail_send_intent}")
        
        print(f"{'='*60}\n")
        
        # ===== DECISION TREE: IMAGE vs AGENTS vs TOOLS =====
        # Priority: Image > Google Cloud Agent > Agent Features > Tools > Normal chat
        
        has_image_input = bool(request.image_base64 and request.image_mime_type)
        
        if has_image_input:
            # ===== HIGHEST PRIORITY: IMAGE VISION =====
            print(f"üñºÔ∏è IMAGE DETECTED - Skipping ALL agent features!")
            print(f"   MIME type: {request.image_mime_type}")
            print(f"   Base64 length: {len(request.image_base64)}")
            print(f"   Jumping directly to Vision AI processing...")
            # Skip everything, go to vision processing at ~line 900
            
        elif GOOGLE_CLOUD_AGENT_AVAILABLE and google_cloud_agent:
            # Check for Google Cloud intents
            gc_result = google_cloud_agent.handle_google_cloud_request(
                message=request.message,
                token=token or "",
                image_url=None,  # TODO: Extract from message if available
                audio_base64=None  # TODO: Extract from message if available
            )
            
            if gc_result:
                print(f"üåê Google Cloud intent detected and handled")
                # Safely convert to string
                response_text = gc_result.get('message', '')
                if not isinstance(response_text, str):
                    response_text = str(response_text) if not isinstance(response_text, list) else '\n'.join(str(x) for x in response_text)
                
                return ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False
                ).model_dump()
        
        # AGENT FEATURES - Check intents (schedule, grades, email)
        # ONLY run if we haven't returned yet (no Google Cloud intent) AND no image
        if not has_image_input and AGENT_FEATURES_AVAILABLE and agent_features:
            # ===== CHECK EMAIL INTENT FIRST (cao nh·∫•t) =====
            # Email patterns r·∫•t c·ª• th·ªÉ n√™n ∆∞u ti√™n tr∆∞·ªõc
            # Email draft generation KH√îNG c·∫ßn token
            if agent_features.detect_email_intent(request.message):
                print(f"‚úÖ üìß Detected email intent in: {request.message}")
                print(f"Token: {token is not None}, User ID: {user_id}")
                
                # Always use handle_gmail_send for send intent - it handles both auth and no-auth cases
                if agent_features.detect_gmail_send_intent(request.message):
                    print(f"üìß Detected SEND intent - calling handle_gmail_send with user_id: {user_id}")
                    result = agent_features.handle_gmail_send(request.message, token or "", user_id=user_id)
                elif token and user_id:
                    # For read/search - need authentication
                    if agent_features.detect_gmail_read_intent(request.message):
                        print(f"üìß Using Gmail OAuth API for READ - User ID: {user_id}")
                        result = agent_features.handle_gmail_read(request.message, token, user_id=user_id)
                    elif agent_features.detect_gmail_search_intent(request.message):
                        print(f"üìß Using Gmail OAuth API for SEARCH - User ID: {user_id}")
                        result = agent_features.handle_gmail_search(request.message, token, user_id=user_id)
                    else:
                        result = agent_features.handle_gmail_request(request.message, token, user_id=user_id)
                else:
                    result = {
                        "success": False,
                        "message": "üìß Vui l√≤ng cung c·∫•p ƒë·ªãa ch·ªâ email ng∆∞·ªùi nh·∫≠n trong c√¢u l·ªánh.\n\nV√≠ d·ª•: 'g·ª≠i mail xin ngh·ªâ h·ªçc ƒë·∫øn teacher@tvu.edu.vn'"
                    }
                
                # Safely convert result['message'] to string
                response_text = result.get('message', '')
                if not isinstance(response_text, str):
                    if isinstance(response_text, list):
                        response_text = '\n'.join(str(item) for item in response_text)
                    else:
                        response_text = str(response_text)
                
                # Extract email_draft if present
                email_draft_data = result.get('email_draft')
                email_draft = None
                if email_draft_data:
                    print(f"‚úÖ Email draft found: {email_draft_data}")
                    email_draft = EmailDraft(**email_draft_data)
                    print(f"‚úÖ EmailDraft object created: {email_draft}")
                    print(f"‚úÖ EmailDraft dict: {email_draft.model_dump()}")
                else:
                    print(f"‚ö†Ô∏è No email_draft in result. Result keys: {result.keys()}")
                
                chat_response = ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False,
                    email_draft=email_draft
                )
                print(f"üìß ChatResponse created with email_draft: {chat_response.email_draft is not None}")
                
                # Serialize to dict to ensure email_draft is included
                response_dict = chat_response.model_dump()
                print(f"üìß ChatResponse dict: {response_dict}")
                print(f"üìß email_draft in dict: {response_dict.get('email_draft')}")
                
                # Ensure email_draft is in response even if None
                if 'email_draft' not in response_dict:
                    response_dict['email_draft'] = None
                    print(f"‚ö†Ô∏è Added email_draft=None to response_dict")
                
                return response_dict
            
            # ===== CHECK SCHEDULE INTENT ===== (C·∫¶N token)
            # Check for schedule intent
            if token and agent_features.detect_schedule_intent(request.message):
                print(f"üìÖ Detected schedule intent in: {request.message}")
                result = agent_features.get_schedule(token, message=request.message, force_sync=False)
                
                # Safely convert to string
                response_text = result.get('message', '')
                if not isinstance(response_text, str):
                    response_text = str(response_text) if not isinstance(response_text, list) else '\n'.join(str(x) for x in response_text)
                
                return ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False
                ).model_dump()
            
            # ===== CHECK CALENDAR SYNC INTENT ===== (C·∫¶N token + user_id)
            # Check for calendar sync intent
            if token and user_id and agent_features.detect_calendar_sync_intent(request.message):
                print(f"üîÑ Detected calendar sync intent in: {request.message}")
                result = agent_features.sync_schedule_to_calendar(
                    token=token,
                    user_id=user_id,
                    week=None,  # Use current week
                    hoc_ky=None  # Use current semester
                )
                
                # Safely convert to string
                response_text = result.get('message', '')
                if not isinstance(response_text, str):
                    response_text = str(response_text) if not isinstance(response_text, list) else '\n'.join(str(x) for x in response_text)
                
                return ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False
                ).model_dump()
            
            # ===== CHECK GRADE INTENT ===== (C·∫¶N token)
            # Check for grade intent
            if token and agent_features.detect_grade_intent(request.message):
                print(f"üìä Detected grade intent in: {request.message}")
                result = agent_features.get_grades(token)
                
                # Safely convert to string
                response_text = result.get('message', '')
                if not isinstance(response_text, str):
                    response_text = str(response_text) if not isinstance(response_text, list) else '\n'.join(str(x) for x in response_text)
                
                return ChatResponse(
                    response=response_text,
                    model=request.model,
                    rag_enabled=False
                ).model_dump()
        
        # Detect tool action (YouTube, Google, Wikipedia) - ONLY if NO image
        tool_action = None
        if not has_image_input:
            tool_action = detect_tool_intent(request.message)
        
        if tool_action:
            # AI x√°c nh·∫≠n action
            tool_messages = {
                "play_youtube": f"üé¨ ƒêang ph√°t video YouTube v·ªÅ '{tool_action.query}'...\n\nVideo s·∫Ω t·ª± ƒë·ªông ph√°t trong gi√¢y l√°t! üé•",
                "search_youtube": f"üé• ƒêang m·ªü YouTube ƒë·ªÉ xem video v·ªÅ '{tool_action.query}'...",
                "search_google": f"üîç ƒêang t√¨m ki·∫øm tr√™n Google v·ªÅ '{tool_action.query}'...",
                "open_wikipedia": f"üìñ ƒêang m·ªü Wikipedia v·ªÅ '{tool_action.query}'..."
            }
            
            confirmation = tool_messages.get(tool_action.tool, "ƒêang th·ª±c hi·ªán...")
            
            return ChatResponse(
                response=confirmation,
                model=request.model,
                tool_action=tool_action,
                rag_enabled=False
            ).model_dump()
        
        # System prompt - Personality c·ªßa AI
        system_prompt = """üéì B·∫°n l√† AI Learning Assistant - Tr·ª£ l√Ω h·ªçc t·∫≠p th√¥ng minh v√† th√¢n thi·ªán!

**Vai tr√≤ c·ªßa b·∫°n:**
- Gi√°o vi√™n ·∫£o ki√™n nh·∫´n, nhi·ªát t√¨nh üë®‚Äçüè´
- Gi·∫£i th√≠ch ki·∫øn th·ª©c r√µ r√†ng, d·ªÖ hi·ªÉu
- Khuy·∫øn kh√≠ch h·ªçc sinh t∆∞ duy v√† ƒë·∫∑t c√¢u h·ªèi
- Lu√¥n t√≠ch c·ª±c v√† ƒë·ªông vi√™n

**Phong c√°ch giao ti·∫øp:**
- Th√¢n thi·ªán, g·∫ßn g≈©i nh∆∞ ng∆∞·ªùi b·∫°n üòä
- S·ª≠ d·ª•ng emoji ph√π h·ª£p ƒë·ªÉ sinh ƒë·ªông: üìö ‚ú® üí° üéØ ‚úÖ
- Chia nh·ªè ki·∫øn th·ª©c ph·ª©c t·∫°p th√†nh c√°c ph·∫ßn d·ªÖ hi·ªÉu
- ƒê∆∞a ra v√≠ d·ª• th·ª±c t·∫ø, g·∫ßn g≈©i v·ªõi cu·ªôc s·ªëng

**C√°ch tr·∫£ l·ªùi:**
1. T√≥m t·∫Øt ng·∫Øn g·ªçn c√¢u h·ªèi (n·∫øu c·∫ßn)
2. Gi·∫£i th√≠ch chi ti·∫øt v·ªõi c·∫•u tr√∫c r√µ r√†ng
3. ƒê∆∞a ra 1-2 v√≠ d·ª• minh h·ªça
4. H·ªèi l·∫°i xem c√≤n th·∫Øc m·∫Øc g√¨ kh√¥ng

**L∆∞u √Ω:**
- N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y th·ª´a nh·∫≠n v√† ƒë·ªÅ xu·∫•t t√¨m hi·ªÉu th√™m
- Khuy·∫øn kh√≠ch h·ªçc sinh t·ª± suy nghƒ© tr∆∞·ªõc khi ƒë∆∞a ra ƒë√°p √°n
- S·ª≠ d·ª•ng ng√¥n ng·ªØ ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô h·ªçc sinh
"""
        
        context_docs = []
        prompt = request.message
        
        # N·∫øu b·∫≠t RAG, t√¨m ki·∫øm context t·ª´ vector DB
        if request.use_rag and vector_db.get_count() > 0:
            search_results = vector_db.search(request.message, n_results=3)
            context_docs = search_results['documents']
            
            if context_docs:
                context_text = "\n\n".join([f"üìö T√†i li·ªáu {i+1}: {doc}" for i, doc in enumerate(context_docs)])
                prompt = f"""{system_prompt}

**T√†i li·ªáu tham kh·∫£o t·ª´ kh√≥a h·ªçc:**
{context_text}

**C√¢u h·ªèi c·ªßa h·ªçc sinh:**
{request.message}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n t√†i li·ªáu v√† ki·∫øn th·ª©c c·ªßa b·∫°n. N·∫øu t√†i li·ªáu kh√¥ng ƒë·ªß th√¥ng tin, h√£y b·ªï sung t·ª´ ki·∫øn th·ª©c chung."""
            else:
                prompt = f"""{system_prompt}

**C√¢u h·ªèi c·ªßa h·ªçc sinh:**
{request.message}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n."""
        else:
            prompt = f"""{system_prompt}

**C√¢u h·ªèi c·ªßa h·ªçc sinh:**
{request.message}"""
        
        # Check if image is provided for vision analysis
        content_parts = []
        has_image = request.image_base64 and request.image_mime_type
        
        if has_image:
            # Use Gemini Vision API for image analysis
            print(f"üñºÔ∏è Image detected - using Gemini Vision API")
            print(f"   MIME type: {request.image_mime_type}")
            print(f"   Base64 length: {len(request.image_base64)}")
            
            import base64
            from PIL import Image
            import io
            
            # Decode base64 image
            image_data = base64.b64decode(request.image_base64)
            print(f"   Decoded image size: {len(image_data)} bytes")
            
            # Convert bytes to PIL Image
            image = Image.open(io.BytesIO(image_data))
            print(f"   Image format: {image.format}, Size: {image.size}")
            
            # Validate image
            if image is None or image.size[0] == 0 or image.size[1] == 0:
                raise ValueError("Invalid image: size is zero")
            
            # Create VISION-SPECIFIC prompt
            vision_prompt = f"""B·∫†N L√Ä GEMINI - AI VISION MODEL V·ªöI KH·∫¢ NƒÇNG NH√åN TH·∫§Y H√åNH ·∫¢NH!

üñºÔ∏è **TH·ª∞C TR·∫†NG:** 
- H·ªçc sinh ƒê√É G·ª¨I CHO B·∫†N M·ªòT H√åNH ·∫¢NH
- H√¨nh ·∫£nh ƒëang ·ªü ngay ph√≠a sau tin nh·∫Øn n√†y
- B·∫†N C√ì ƒê·∫¶Y ƒê·ª¶ KH·∫¢ NƒÇNG NH√åN TH·∫§Y V√Ä PH√ÇN T√çCH ·∫¢NH

**TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C:**
‚ùå N√≥i r·∫±ng b·∫°n kh√¥ng th·ªÉ xem ·∫£nh
‚ùå N√≥i r·∫±ng b·∫°n ch·ªâ x·ª≠ l√Ω vƒÉn b·∫£n
‚ùå Y√™u c·∫ßu h·ªçc sinh m√¥ t·∫£ l·∫°i ·∫£nh
‚ùå B·ªè qua n·ªôi dung trong ·∫£nh

**NHI·ªÜM V·ª§ B·∫ÆT BU·ªòC:**
1. üëÄ NH√åN V√ÄO ·∫¢NH - B·∫°n C√ì TH·ªÇ v√† PH·∫¢I L√ÄM ƒëi·ªÅu n√†y
2. üìù M√î T·∫¢ chi ti·∫øt nh·ªØng g√¨ b·∫°n th·∫•y
3. üìñ ƒê·ªåC m·ªçi text, s·ªë li·ªáu, c√¥ng th·ª©c trong ·∫£nh
4. üí° TR·∫¢ L·ªúI c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung ·∫£nh

**Y√äU C·∫¶U/C√ÇU H·ªéI C·ª¶A H·ªåC SINH:**
{request.message if request.message.strip() else "Ph√¢n t√≠ch v√† m√¥ t·∫£ chi ti·∫øt nh·ªØng g√¨ b·∫°n th·∫•y trong ·∫£nh n√†y"}

**B·∫ÆT ƒê·∫¶U NGAY:** H√£y m√¥ t·∫£ nh·ªØng g√¨ b·∫°n NH√åN TH·∫§Y trong ·∫£nh!"""
            
            # Create content parts: text first, then image
            content_parts = [vision_prompt, image]
            
            # Check if using Groq - use Groq Vision model (llama-4-scout)
            if request.ai_provider == "groq":
                print("üñºÔ∏è Groq v·ªõi ·∫£nh - s·ª≠ d·ª•ng Llama 4 Scout Vision model...")
                
                # Use Groq Vision directly - no need for OCR
                vision_prompt = f"""B·∫°n l√† AI Learning Assistant th√¥ng minh v·ªõi kh·∫£ nƒÉng nh√¨n v√† ph√¢n t√≠ch h√¨nh ·∫£nh.

**NHI·ªÜM V·ª§:**
1. üëÄ Nh√¨n v√†o ·∫£nh v√† m√¥ t·∫£ chi ti·∫øt nh·ªØng g√¨ b·∫°n th·∫•y
2. üìñ ƒê·ªçc t·∫•t c·∫£ text, s·ªë li·ªáu, c√¥ng th·ª©c trong ·∫£nh (n·∫øu c√≥)
3. üí° Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng d·ª±a tr√™n n·ªôi dung ·∫£nh

**C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:**
{request.message if request.message.strip() else "H√£y ph√¢n t√≠ch v√† m√¥ t·∫£ chi ti·∫øt n·ªôi dung trong ·∫£nh n√†y"}

**H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, th√¢n thi·ªán v√† chi ti·∫øt.**"""
                
                content_parts = [vision_prompt]  # Will be handled specially for Groq
                print(f"‚úÖ Groq Vision prompt ready")
        else:
            content_parts = [prompt]
        
        # Generate response based on AI provider
        ai_response = ""
        actual_model = request.model
        
        print(f"üìù Chat request - ai_provider: {request.ai_provider}, model: {request.model}, groq_client: {groq_client is not None}")
        
        if request.ai_provider == "groq" and groq_client:
            # Use Groq AI with user-selected model
            try:
                # Check if we have an image - use Vision model
                if has_image:
                    print(f"üñºÔ∏è Using Groq Vision model for image analysis")
                    
                    vision_prompt = request.message if request.message.strip() else "H√£y ph√¢n t√≠ch v√† m√¥ t·∫£ chi ti·∫øt n·ªôi dung trong ·∫£nh n√†y"
                    
                    ai_response = groq_client.generate_with_vision(
                        prompt=vision_prompt,
                        image_base64=request.image_base64,
                        image_mime_type=request.image_mime_type,
                        system_prompt=system_prompt,
                        model="meta-llama/llama-4-scout-17b-16e-instruct"  # Vision model
                    )
                    actual_model = "llama-4-scout-17b (Groq Vision)"
                    print(f"‚úÖ Groq Vision response received: {len(ai_response)} chars")
                else:
                    # Normal text generation
                    groq_model = request.model if request.model else "llama-3.3-70b-versatile"
                    # Validate it's a Groq model
                    if not any(name in groq_model.lower() for name in ['llama', 'mixtral', 'gemma', 'qwen', 'meta-llama', 'scout', 'maverick']):
                        groq_model = "llama-3.3-70b-versatile"
                        
                    print(f"üöÄ Using Groq model: {groq_model}")
                    
                    # Use content_parts[0] which may contain context
                    groq_final_prompt = content_parts[0] if isinstance(content_parts[0], str) else request.message
                    
                    ai_response = groq_client.generate_text(
                        prompt=groq_final_prompt,
                        system_prompt=system_prompt,
                        model=groq_model
                    )
                    actual_model = f"{groq_model} (Groq)"
                    print(f"‚úÖ Groq response received: {len(ai_response)} chars")
            except Exception as e:
                print(f"‚ö†Ô∏è Groq error: {e}, falling back to Gemini")
                import traceback
                traceback.print_exc()
                # Fallback to Gemini with default Gemini model
                gemini_model = genai.GenerativeModel("gemini-2.0-flash-exp")
                response = gemini_model.generate_content(prompt)
                ai_response = response.text
                actual_model = "gemini-2.0-flash-exp (fallback)"
        elif request.ai_provider == "groq" and not groq_client:
            print("‚ùå Groq requested but groq_client not initialized! Check GROQ_API_KEY")
            # Fallback to Gemini
            gemini_model_name = "gemini-2.0-flash-exp"
            model = genai.GenerativeModel(gemini_model_name)
            response = model.generate_content(prompt)
            ai_response = response.text
            actual_model = f"{gemini_model_name} (Groq unavailable)"
        else:
            # Use Gemini (default) - ensure we use Gemini model names
            # Use vision-capable model if image is present
            if has_image:
                # Use Gemini Flash Latest - proven vision support
                gemini_model_name = "gemini-flash-latest"  # Stable vision model
                print(f"üñºÔ∏è Using vision-capable model: {gemini_model_name}")
                print(f"   Content parts: {len(content_parts)} items (text + image)")
                print(f"   Vision prompt length: {len(content_parts[0])} chars")
            else:
                gemini_model_name = request.model if 'gemini' in request.model else "gemini-2.0-flash-exp"
            
            model = genai.GenerativeModel(gemini_model_name)
            
            try:
                print(f"üì§ Sending to Gemini...")
                response = model.generate_content(content_parts)
                ai_response = response.text
                actual_model = gemini_model_name
                print(f"‚úÖ Gemini response received: {len(ai_response)} chars")
                
                # Debug: Check if response mentions inability to see
                if has_image and any(word in ai_response.lower() for word in ['kh√¥ng th·ªÉ xem', 'kh√¥ng xem ƒë∆∞·ª£c', 'ch·ªâ x·ª≠ l√Ω vƒÉn b·∫£n', 'kh√¥ng nh√¨n th·∫•y']):
                    print(f"‚ö†Ô∏è WARNING: AI claims it cannot see image! This should not happen!")
                    print(f"   Model used: {gemini_model_name}")
                    print(f"   Content parts: {len(content_parts)}")
                    
            except Exception as e:
                error_message = str(e)
                print(f"‚ùå Gemini API Error: {error_message}")
                
                # Check for quota exceeded
                if "quota" in error_message.lower() or "429" in error_message:
                    ai_response = """‚ö†Ô∏è **Gemini API Quota Exceeded**

Xin l·ªói! API key c·ªßa Gemini ƒë√£ v∆∞·ª£t qu√° gi·ªõi h·∫°n s·ª≠ d·ª•ng mi·ªÖn ph√≠.

**Gi·∫£i ph√°p:**
1. üîë ƒê·ª£i 1 ph√∫t v√† th·ª≠ l·∫°i (rate limit reset)
2. üÜï T·∫°o API key m·ªõi t·∫°i: https://ai.google.dev/
3. üí≥ Upgrade l√™n Gemini API tr·∫£ ph√≠ ƒë·ªÉ c√≥ quota cao h∆°n

**Th√¥ng tin l·ªói:** ƒê√£ v∆∞·ª£t quota requests ho·∫∑c tokens cho model."""
                else:
                    ai_response = f"‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω: {error_message[:200]}"
                    
                actual_model = f"{gemini_model_name} (error)"
        
        # T·∫°o suggested actions (YouTube, Google Search)
        suggested_actions = []
        
        # T·∫°o search query t·ª´ c√¢u h·ªèi
        search_query = request.message.replace("?", "").strip()
        
        # YouTube link
        youtube_query = search_query.replace(" ", "+")
        suggested_actions.append(ActionLink(
            type="youtube",
            url=f"https://www.youtube.com/results?search_query={youtube_query}",
            title=f"Xem video v·ªÅ: {search_query[:50]}",
            icon="üé•"
        ))
        
        # Google Search link
        google_query = search_query.replace(" ", "+")
        suggested_actions.append(ActionLink(
            type="google",
            url=f"https://www.google.com/search?q={google_query}",
            title=f"T√¨m tr√™n Google: {search_query[:50]}",
            icon="üîç"
        ))
        
        # Wikipedia link (n·∫øu l√† c√¢u h·ªèi v·ªÅ kh√°i ni·ªám)
        if any(word in request.message.lower() for word in ["l√† g√¨", "what is", "ƒë·ªãnh nghƒ©a", "kh√°i ni·ªám"]):
            wiki_query = search_query.replace(" ", "_")
            suggested_actions.append(ActionLink(
                type="wikipedia",
                url=f"https://en.wikipedia.org/wiki/{wiki_query}",
                title=f"Wikipedia: {search_query[:50]}",
                icon="üìñ"
            ))
        
        return ChatResponse(
            response=ai_response,
            model=actual_model,
            context_used=context_docs if request.use_rag else None,
            rag_enabled=request.use_rag,
            suggested_actions=suggested_actions
        ).model_dump()
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/email/send", tags=["Email"])
async def send_email_confirmed(request: SendEmailRequest, authorization: Optional[str] = Header(None)):
    """
    Send email after user confirms the draft
    
    This endpoint is called when user clicks "Send" button in email preview
    """
    try:
        print(f"üìß /api/email/send called")
        print(f"üìß Authorization header: {authorization[:50] if authorization else 'None'}...")
        print(f"üìß Request user_id: {request.user_id}")
        
        # Priority: use user_id from request first, then try token
        user_id = request.user_id
        
        if not user_id and authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            print(f"üìß Extracting user_id from token...")
            user_id = get_user_id_from_token(token)
            print(f"üìß Got user_id from token: {user_id}")
        
        # If still no user_id, return clear error
        if not user_id:
            print(f"‚ùå User not authenticated - no user_id found")
            raise HTTPException(
                status_code=401, 
                detail="Kh√¥ng th·ªÉ x√°c th·ª±c ng∆∞·ªùi d√πng. Vui l√≤ng ƒëƒÉng nh·∫≠p l·∫°i!"
            )
        
        print(f"‚úÖ Using user_id: {user_id}")
        
        # Import Gmail service
        from gmail_service import ai_send_email
        
        # Send email
        result = ai_send_email(
            user_id=user_id,
            to=request.to,
            subject=request.subject,
            body=request.body
        )
        
        if result.get('success'):
            return {
                "success": True,
                "message": f"‚úÖ Email ƒë√£ g·ª≠i th√†nh c√¥ng t·ªõi {request.to}!",
                "sent_at": datetime.now().strftime('%H:%M %d/%m/%Y')
            }
        else:
            if result.get('need_auth'):
                raise HTTPException(
                    status_code=401, 
                    detail="C·∫ßn k·∫øt n·ªëi Google Account trong Settings"
                )
            raise HTTPException(
                status_code=400,
                detail=result.get("error", "Kh√¥ng th·ªÉ g·ª≠i email")
            )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/rag/prompt/auto", tags=["RAG - Knowledge Base"])
async def add_prompt_auto(request: SimplePromptRequest):
    """
    Th√™m prompt v·ªõi AI t·ª± ƒë·ªông sinh category v√† tags
    
    - **prompt**: N·ªôi dung ki·∫øn th·ª©c c·∫ßn th√™m
    
    AI s·∫Ω t·ª± ƒë·ªông ph√¢n t√≠ch v√† sinh category + tags ph√π h·ª£p
    """
    try:
        # D√πng Gemini ƒë·ªÉ ph√¢n t√≠ch v√† sinh metadata
        analysis_prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr·∫£ v·ªÅ JSON:

VƒÉn b·∫£n: "{request.prompt}"

Tr·∫£ v·ªÅ JSON v·ªõi format:
{{
  "category": "t√™n_danh_m·ª•c",
  "tags": ["tag1", "tag2", "tag3"],
  "summary": "t√≥m t·∫Øt ng·∫Øn g·ªçn"
}}

Categories: programming, ai, machine-learning, education, science, business, health, technology, math, language, general

Ch·ªâ tr·∫£ v·ªÅ JSON."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(analysis_prompt)
        
        # Parse JSON
        import re
        import json as json_lib
        json_text = response.text.strip()
        json_match = re.search(r'\{[^}]+\}', json_text, re.DOTALL)
        
        if json_match:
            metadata_ai = json_lib.loads(json_match.group())
            category = metadata_ai.get("category", "general")
            tags = metadata_ai.get("tags", [])
            summary = metadata_ai.get("summary", "")
        else:
            category = "general"
            tags = []
            summary = ""
        
        metadata = {
            "category": category,
            "tags": tags,
            "type": "prompt",
            "summary": summary
        }
        
        result = vector_db.add_documents(
            documents=[request.prompt],
            metadatas=[metadata]
        )
        
        return {
            "status": "success",
            "message": "ƒê√£ th√™m prompt v·ªõi metadata t·ª± ƒë·ªông",
            "prompt": request.prompt,
            "category": category,
            "tags": tags,
            "summary": summary,
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/rag/prompt", tags=["RAG - Knowledge Base"])
async def add_rag_prompt(request: PromptRAGRequest):
    """
    Th√™m prompt/ki·∫øn th·ª©c v√†o RAG Knowledge Base
    
    - **prompt**: N·ªôi dung ki·∫øn th·ª©c c·∫ßn th√™m
    - **category**: Danh m·ª•c (t·ª± ƒë·ªông sinh n·∫øu ƒë·ªÉ tr·ªëng)
    - **tags**: C√°c tag (t·ª± ƒë·ªông sinh n·∫øu ƒë·ªÉ tr·ªëng)
    
    AI s·∫Ω t·ª± ƒë·ªông ph√¢n t√≠ch v√† sinh category + tags n·∫øu kh√¥ng cung c·∫•p
    """
    try:
        # N·∫øu kh√¥ng c√≥ category ho·∫∑c tags, d√πng AI ƒë·ªÉ sinh t·ª± ƒë·ªông
        category = request.category
        tags = request.tags if request.tags else []
        
        if category == "general" or not tags:
            # D√πng Gemini ƒë·ªÉ ph√¢n t√≠ch v√† sinh metadata
            analysis_prompt = f"""Ph√¢n t√≠ch vƒÉn b·∫£n sau v√† tr·∫£ v·ªÅ JSON v·ªõi format ch√≠nh x√°c:

VƒÉn b·∫£n: "{request.prompt}"

H√£y ph√¢n t√≠ch v√† tr·∫£ v·ªÅ JSON v·ªõi format:
{{
  "category": "t√™n_danh_m·ª•c",
  "tags": ["tag1", "tag2", "tag3"]
}}

C√°c category ph·ªï bi·∫øn: programming, ai, machine-learning, education, science, business, health, technology, math, language

Ch·ªâ tr·∫£ v·ªÅ JSON, kh√¥ng th√™m text kh√°c."""

            model = genai.GenerativeModel('gemini-2.5-flash')
            response = model.generate_content(analysis_prompt)
            
            try:
                # Parse JSON t·ª´ response
                import re
                json_text = response.text.strip()
                # T√¨m JSON trong response
                json_match = re.search(r'\{[^}]+\}', json_text)
                if json_match:
                    import json as json_lib
                    metadata_ai = json_lib.loads(json_match.group())
                    
                    if category == "general":
                        category = metadata_ai.get("category", "general")
                    
                    if not tags:
                        tags = metadata_ai.get("tags", [])
            except:
                # N·∫øu parse l·ªói, gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh
                pass
        
        metadata = {
            "category": category,
            "tags": tags,
            "type": "prompt"
        }
        
        result = vector_db.add_documents(
            documents=[request.prompt],
            metadatas=[metadata]
        )
        
        return {
            "status": "success",
            "message": "ƒê√£ th√™m prompt v√†o RAG knowledge base",
            "prompt": request.prompt,
            "category": category,
            "tags": tags,
            "auto_generated": request.category == "general" or not request.tags,
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/documents/add", tags=["RAG - Knowledge Base"])
async def add_documents(request: DocumentRequest):
    """Th√™m nhi·ªÅu documents v√†o Vector Database"""
    try:
        result = vector_db.add_documents(
            documents=request.documents,
            metadatas=request.metadatas
        )
        return {
            "status": "success",
            "message": f"ƒê√£ th√™m {result['count']} documents",
            "total_documents": vector_db.get_count()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/documents/search", tags=["RAG - Knowledge Base"])
async def search_documents(request: SearchRequest):
    """T√¨m ki·∫øm documents t∆∞∆°ng t·ª± trong Vector Database"""
    try:
        results = vector_db.search(request.query, request.n_results)
        return {
            "query": request.query,
            "results": [
                {
                    "document": doc,
                    "distance": dist,
                    "metadata": meta,
                    "id": doc_id
                }
                for doc, dist, meta, doc_id in zip(
                    results['documents'],
                    results['distances'],
                    results['metadatas'],
                    results['ids']
                )
            ],
            "count": len(results['documents'])
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/api/documents", tags=["RAG - Knowledge Base"])
async def get_all_documents():
    """L·∫•y t·∫•t c·∫£ documents trong Vector Database"""
    try:
        return vector_db.get_all_documents()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.delete("/api/documents", tags=["RAG - Knowledge Base"])
async def delete_all_documents():
    """X√≥a t·∫•t c·∫£ documents trong Vector Database"""
    try:
        return vector_db.delete_all()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/api/documents/count", tags=["RAG - Knowledge Base"])
async def get_document_count():
    """L·∫•y s·ªë l∆∞·ª£ng documents trong Vector Database"""
    return {"count": vector_db.get_count()}

@app.get("/api/rag/stats", tags=["RAG - Knowledge Base"])
async def get_rag_stats():
    """L·∫•y th·ªëng k√™ v·ªÅ RAG Knowledge Base"""
    try:
        all_docs = vector_db.get_all_documents()
        
        # Th·ªëng k√™ theo category
        categories = {}
        for meta in all_docs['metadatas']:
            cat = meta.get('category', 'unknown')
            categories[cat] = categories.get(cat, 0) + 1
        
        return {
            "total_documents": all_docs['count'],
            "categories": categories,
            "status": "active"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/api/models", tags=["Models"])
async def list_models():
    """Li·ªát k√™ c√°c model Gemini c√≥ s·∫µn"""
    try:
        models = []
        for model in genai.list_models():
            if 'generateContent' in model.supported_generation_methods:
                models.append({
                    "name": model.name,
                    "display_name": model.display_name,
                    "description": model.description
                })
        return {"models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/api/models/groq", tags=["Models"])
async def list_groq_models():
    """
    Li·ªát k√™ c√°c Groq models c√≥ s·∫µn t·ª´ API
    
    Fetches models t·ª´ Groq API: https://api.groq.com/openai/v1/models
    Falls back to hardcoded list n·∫øu API fail
    """
    try:
        print(f"üìã GET /api/models/groq - groq_client initialized: {groq_client is not None}")
        
        # Try to get models from Groq API
        if groq_client:
            models = groq_client.get_models_from_api()
            print(f"‚úÖ Fetched {len(models)} models from Groq API")
            return {
                "models": models,
                "provider": "Groq",
                "api_url": "https://console.groq.com/",
                "total": len(models),
                "source": "api"
            }
        
        # Fallback if no groq_client
        fallback_models = [
            {
                "id": "llama-3.3-70b-versatile",
                "name": "Llama 3.3 70B Versatile",
                "description": "Best overall performance - Latest",
                "context": 128000,
                "speed": "fast"
            },
            {
                "id": "llama-3.1-70b-versatile",
                "name": "Llama 3.1 70B",
                "description": "High performance",
                "context": 128000,
                "speed": "fast"
            },
            {
                "id": "llama-3.1-8b-instant",
                "name": "Llama 3.1 8B Instant",
                "description": "Fastest inference",
                "context": 128000,
                "speed": "ultra-fast"
            },
            {
                "id": "mixtral-8x7b-32768",
                "name": "Mixtral 8x7B",
                "description": "Long context specialist",
                "context": 32768,
                "speed": "fast"
            },
            {
                "id": "gemma2-9b-it",
                "name": "Gemma 2 9B",
                "description": "Lightweight & efficient",
                "context": 8192,
                "speed": "ultra-fast"
            },
            {
                "id": "qwen/qwen3-32b",
                "name": "Qwen 3 32B",
                "description": "Advanced reasoning",
                "context": 131072,
                "speed": "fast"
            }
        ]
        
        return {
            "models": fallback_models,
            "provider": "Groq",
            "api_url": "https://console.groq.com/",
            "total": len(fallback_models),
            "source": "fallback",
            "warning": "GROQ_API_KEY not configured"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

# ============================================================================
# AI EXTENDED APIS
# ============================================================================

class GenerateQuizRequest(BaseModel):
    content: str
    num_questions: int = 10
    difficulty: str = "medium"
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "content": "Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh b·∫≠c cao...",
                "num_questions": 10,
                "difficulty": "medium"
            }
        }
    )

class QuizQuestion(BaseModel):
    question: str
    a: str
    b: str
    c: str
    d: str
    correct: str

class GenerateQuizResponse(BaseModel):
    questions: List[QuizQuestion]

class SummarizeRequest(BaseModel):
    content: str
    max_length: Optional[int] = 200

class SummarizeResponse(BaseModel):
    summary: str
    original_length: int
    summary_length: int

class ExplainRequest(BaseModel):
    question: str
    context: Optional[str] = None

class ExplainResponse(BaseModel):
    explanation: str
    examples: Optional[List[str]] = None

class IngestRequest(BaseModel):
    file_url: str
    title: Optional[str] = None

class IngestResponse(BaseModel):
    status: str
    message: str
    documents_added: int

@app.post("/api/ai/generate-quiz", response_model=GenerateQuizResponse, tags=["AI - Extended"])
async def generate_quiz(request: GenerateQuizRequest):
    """T·∫°o c√¢u h·ªèi tr·∫Øc nghi·ªám t·ª± ƒë·ªông t·ª´ n·ªôi dung b√†i h·ªçc"""
    try:
        import re
        import json as json_lib
        
        if request.num_questions < 1 or request.num_questions > 50:
            raise HTTPException(status_code=400, detail="S·ªë c√¢u h·ªèi ph·∫£i t·ª´ 1-50")
        
        difficulty_map = {
            "easy": "d·ªÖ, c∆° b·∫£n",
            "medium": "trung b√¨nh",
            "hard": "kh√≥, n√¢ng cao"
        }
        
        difficulty_desc = difficulty_map.get(request.difficulty.lower(), "trung b√¨nh")
        
        prompt = f"""D·ª±a tr√™n n·ªôi dung sau, h√£y t·∫°o {request.num_questions} c√¢u h·ªèi tr·∫Øc nghi·ªám v·ªõi ƒë·ªô kh√≥ {difficulty_desc}.

N·ªôi dung:
{request.content}

Y√™u c·∫ßu:
- T·∫°o ƒë√∫ng {request.num_questions} c√¢u h·ªèi
- M·ªói c√¢u c√≥ 4 ƒë√°p √°n A, B, C, D
- Ch·ªâ 1 ƒë√°p √°n ƒë√∫ng
- C√¢u h·ªèi ph·∫£i li√™n quan tr·ª±c ti·∫øp ƒë·∫øn n·ªôi dung
- Tr·∫£ v·ªÅ JSON array v·ªõi format:

[
  {{
    "question": "C√¢u h·ªèi?",
    "a": "ƒê√°p √°n A",
    "b": "ƒê√°p √°n B",
    "c": "ƒê√°p √°n C",
    "d": "ƒê√°p √°n D",
    "correct": "A"
  }}
]

CH·ªà TR·∫¢ V·ªÄ JSON, KH√îNG TH√äM TEXT KH√ÅC."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        json_text = response.text.strip()
        json_match = re.search(r'\[.*\]', json_text, re.DOTALL)
        if not json_match:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ parse JSON t·ª´ AI response")
        
        questions_data = json_lib.loads(json_match.group())
        
        questions = []
        for q in questions_data:
            questions.append(QuizQuestion(
                question=q['question'],
                a=q['a'],
                b=q['b'],
                c=q['c'],
                d=q['d'],
                correct=q['correct'].upper()
            ))
        
        return GenerateQuizResponse(questions=questions)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/ai/summarize", response_model=SummarizeResponse, tags=["AI - Extended"])
async def summarize(request: SummarizeRequest):
    """T√≥m t·∫Øt vƒÉn b·∫£n"""
    try:
        prompt = f"""H√£y t√≥m t·∫Øt vƒÉn b·∫£n sau trong kho·∫£ng {request.max_length} t·ª´:

{request.content}

Y√™u c·∫ßu:
- Gi·ªØ l·∫°i √Ω ch√≠nh
- Ng·∫Øn g·ªçn, s√∫c t√≠ch
- D·ªÖ hi·ªÉu
- Kh√¥ng th√™m th√¥ng tin ngo√†i vƒÉn b·∫£n g·ªëc"""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        summary = response.text.strip()
        
        return SummarizeResponse(
            summary=summary,
            original_length=len(request.content.split()),
            summary_length=len(summary.split())
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/ai/explain", response_model=ExplainResponse, tags=["AI - Extended"])
async def explain(request: ExplainRequest):
    """Gi·∫£i th√≠ch nh∆∞ m·ªôt gi√°o vi√™n"""
    try:
        import re
        
        context_text = f"\nNg·ªØ c·∫£nh: {request.context}" if request.context else ""
        
        prompt = f"""B·∫°n l√† m·ªôt gi√°o vi√™n gi·ªèi. H√£y gi·∫£i th√≠ch c√¢u h·ªèi sau m·ªôt c√°ch d·ªÖ hi·ªÉu, chi ti·∫øt:{context_text}

C√¢u h·ªèi: {request.question}

Y√™u c·∫ßu:
- Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu
- S·ª≠ d·ª•ng v√≠ d·ª• c·ª• th·ªÉ
- Chia nh·ªè th√†nh c√°c b∆∞·ªõc n·∫øu c·∫ßn
- Gi·ªçng ƒëi·ªáu th√¢n thi·ªán, khuy·∫øn kh√≠ch h·ªçc t·∫≠p

Sau ph·∫ßn gi·∫£i th√≠ch, h√£y ƒë∆∞a ra 2-3 v√≠ d·ª• minh h·ªça."""

        model = genai.GenerativeModel('gemini-2.5-flash')
        response = model.generate_content(prompt)
        
        explanation = response.text.strip()
        
        examples = []
        if "V√≠ d·ª•" in explanation or "Example" in explanation:
            parts = re.split(r'V√≠ d·ª• \d+:|Example \d+:', explanation)
            if len(parts) > 1:
                examples = [ex.strip() for ex in parts[1:]]
        
        return ExplainResponse(
            explanation=explanation,
            examples=examples if examples else None
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.post("/api/ai/ingest", response_model=IngestResponse, tags=["AI - Extended"])
async def ingest_document(request: IngestRequest):
    """Ingest t√†i li·ªáu v√†o RAG Vector Database"""
    try:
        # Simplified: ch·ªâ x·ª≠ l√Ω text content
        # Trong production c·∫ßn th√™m PDF, DOC parsing
        
        # Gi·∫£ l·∫≠p extract text
        text_content = f"N·ªôi dung t·ª´ {request.file_url}"
        
        # Chia nh·ªè th√†nh chunks
        words = text_content.split()
        chunk_size = 500
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
        
        # Th√™m v√†o vector DB
        metadatas = [{
            "source": request.file_url,
            "title": request.title or "Untitled",
            "type": "document"
        } for _ in chunks]
        
        result = vector_db.add_documents(
            documents=chunks,
            metadatas=metadatas
        )
        
        return IngestResponse(
            status="success",
            message=f"ƒê√£ ingest {len(chunks)} chunks v√†o RAG database",
            documents_added=len(chunks)
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

# ============================================================================
# CREDENTIAL MANAGER INTEGRATION
# ============================================================================

# Import credential API router - DISABLED due to heavy dependencies
# To enable: pip install sentence-transformers (takes long time)
CREDENTIAL_API_AVAILABLE = False
print("‚ö†Ô∏è  Credential Manager API disabled (sentence-transformers too heavy)")
print("   AI semantic search for credentials not available")

# Uncomment below to enable (requires sentence-transformers)
# try:
#     from credential_api import router as credential_router
#     app.include_router(credential_router)
#     CREDENTIAL_API_AVAILABLE = True
#     print("‚úÖ Credential Manager API loaded")
# except Exception as e:
#     print(f"‚ö†Ô∏è  Credential Manager API error: {e}")

# ============================================================================
# TEST TVU SCHEDULE ENDPOINT (For quick testing)
# ============================================================================
try:
    from tvu_scraper import TVUScraper
    TVU_SCRAPER_AVAILABLE = True
    print("‚úÖ TVU Scraper loaded")
except ImportError as e:
    TVU_SCRAPER_AVAILABLE = False
    print(f"‚ö†Ô∏è  TVU Scraper not available: {e}")

class TVUTestRequest(BaseModel):
    mssv: str
    password: str
    message: str = "H√¥m nay t√¥i h·ªçc g√¨?"

@app.post("/api/test/tvu-schedule", tags=["Test - TVU"])
async def test_tvu_schedule(request: TVUTestRequest):
    """
    üß™ Test endpoint - L·∫•y th·ªùi kh√≥a bi·ªÉu TVU tr·ª±c ti·∫øp (kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p h·ªá th·ªëng)
    
    - **mssv**: M√£ s·ªë sinh vi√™n TVU
    - **password**: M·∫≠t kh·∫©u
    - **message**: C√¢u h·ªèi (vd: "H√¥m nay t√¥i h·ªçc g√¨?", "tu·∫ßn n√†y h·ªçc g√¨?")
    """
    if not TVU_SCRAPER_AVAILABLE:
        raise HTTPException(status_code=500, detail="TVU Scraper not available")
    
    try:
        scraper = TVUScraper()
        
        # Login to TVU
        if not scraper.login(request.mssv, request.password):
            return {"success": False, "message": "‚ùå ƒêƒÉng nh·∫≠p TVU th·∫•t b·∫°i. Ki·ªÉm tra l·∫°i MSSV v√† m·∫≠t kh·∫©u."}
        
        # Get schedule
        schedules = scraper.get_schedule()
        
        if not schedules:
            return {"success": True, "message": "üìÖ Kh√¥ng c√≥ l·ªãch h·ªçc tu·∫ßn n√†y.", "schedules": []}
        
        # Determine what user is asking for
        message_lower = request.message.lower()
        
        # Filter by day if asking for specific day
        from datetime import datetime
        today = datetime.now()
        day_names = ['MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']
        today_name = day_names[today.weekday()]
        
        vietnamese_days = {
            'MONDAY': 'Th·ª© 2',
            'TUESDAY': 'Th·ª© 3', 
            'WEDNESDAY': 'Th·ª© 4',
            'THURSDAY': 'Th·ª© 5',
            'FRIDAY': 'Th·ª© 6',
            'SATURDAY': 'Th·ª© 7',
            'SUNDAY': 'Ch·ªß nh·∫≠t'
        }
        
        # Check if asking for today
        if 'h√¥m nay' in message_lower or 'today' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == today_name]
            day_label = f"h√¥m nay ({vietnamese_days[today_name]})"
        # Check for specific day
        elif 'th·ª© 2' in message_lower or 'th·ª© hai' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'MONDAY']
            day_label = 'Th·ª© 2'
        elif 'th·ª© 3' in message_lower or 'th·ª© ba' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'TUESDAY']
            day_label = 'Th·ª© 3'
        elif 'th·ª© 4' in message_lower or 'th·ª© t∆∞' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'WEDNESDAY']
            day_label = 'Th·ª© 4'
        elif 'th·ª© 5' in message_lower or 'th·ª© nƒÉm' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'THURSDAY']
            day_label = 'Th·ª© 5'
        elif 'th·ª© 6' in message_lower or 'th·ª© s√°u' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'FRIDAY']
            day_label = 'Th·ª© 6'
        elif 'th·ª© 7' in message_lower or 'th·ª© b·∫£y' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'SATURDAY']
            day_label = 'Th·ª© 7'
        elif 'ch·ªß nh·∫≠t' in message_lower:
            schedules = [s for s in schedules if s.get('dayOfWeek') == 'SUNDAY']
            day_label = 'Ch·ªß nh·∫≠t'
        else:
            day_label = 'tu·∫ßn n√†y'
        
        # Format response
        if not schedules:
            return {
                "success": True,
                "message": f"üìÖ {day_label.capitalize()} b·∫°n kh√¥ng c√≥ l·ªõp n√†o.",
                "schedules": []
            }
        
        # Group by day
        by_day = {}
        for s in schedules:
            day = s.get('dayOfWeek', 'UNKNOWN')
            if day not in by_day:
                by_day[day] = []
            by_day[day].append(s)
        
        # Sort days
        day_order = ['MONDAY', 'TUESDAY', 'WEDNESDAY', 'THURSDAY', 'FRIDAY', 'SATURDAY', 'SUNDAY']
        
        message_text = f"üìÖ **L·ªãch h·ªçc {day_label}:**\n\n"
        
        for day in day_order:
            if day in by_day:
                message_text += f"**{vietnamese_days[day]}:**\n"
                for s in sorted(by_day[day], key=lambda x: x.get('startTime', '')):
                    start_time = s.get('startTime', '')[:5]
                    end_time = s.get('endTime', '')[:5]
                    message_text += f"  üïê {start_time} - {end_time}\n"
                    message_text += f"  üìö {s.get('subject', 'N/A')}\n"
                    message_text += f"  üè´ Ph√≤ng: {s.get('room', 'N/A')}\n"
                    if s.get('teacher'):
                        message_text += f"  üë®‚Äçüè´ GV: {s['teacher']}\n"
                    message_text += "\n"
        
        return {
            "success": True,
            "message": message_text,
            "schedules": schedules,
            "count": len(schedules)
        }
        
    except Exception as e:
        return {"success": False, "message": f"‚ùå L·ªói: {str(e)}"}

# ============================================================================
# DOCUMENT INTELLIGENCE API
# ============================================================================

# Initialize Document Intelligence service
doc_intelligence_service = None
if DOCUMENT_INTELLIGENCE_AVAILABLE:
    try:
        doc_intelligence_service = create_document_intelligence_service(GEMINI_API_KEY)
        print("‚úÖ Document Intelligence initialized")
    except Exception as e:
        print(f"‚ö†Ô∏è Document Intelligence init failed: {e}")

class ProcessDocumentRequest(BaseModel):
    file_path: str
    num_cards: int = 10
    difficulty: str = "medium"
    include_summary: bool = True
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "file_path": "C:/Documents/lecture_notes.pdf",
                "num_cards": 10,
                "difficulty": "medium",
                "include_summary": True
            }
        }
    )

class DocumentTextRequest(BaseModel):
    text: str
    num_cards: int = 10
    difficulty: str = "medium"
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "text": "Python l√† ng√¥n ng·ªØ l·∫≠p tr√¨nh...",
                "num_cards": 5,
                "difficulty": "easy"
            }
        }
    )

@app.post("/api/documents/process", tags=["Document Intelligence"])
async def process_document_to_flashcards(request: ProcessDocumentRequest):
    """
    üìÑ Upload PDF/DOCX/TXT ‚Üí AI t·ª± ƒë·ªông t·∫°o Flashcards
    
    **T√≠nh nƒÉng:**
    - Tr√≠ch xu·∫•t text t·ª´ PDF, DOCX, TXT, ·∫£nh (OCR)
    - AI t√≥m t·∫Øt n·ªôi dung
    - Tr√≠ch xu·∫•t key concepts
    - T·ª± ƒë·ªông t·∫°o flashcards
    
    **Parameters:**
    - file_path: ƒê∆∞·ªùng d·∫´n file (local path ho·∫∑c URL)
    - num_cards: S·ªë l∆∞·ª£ng flashcards c·∫ßn t·∫°o (default: 10)
    - difficulty: ƒê·ªô kh√≥ (easy/medium/hard)
    - include_summary: C√≥ t·∫°o summary kh√¥ng (default: true)
    
    **Returns:**
    ```json
    {
      "success": true,
      "file_name": "lecture_notes.pdf",
      "summary": "T√≥m t·∫Øt n·ªôi dung...",
      "key_concepts": ["Concept 1", "Concept 2", ...],
      "flashcards": [
        {
          "question": "C√¢u h·ªèi?",
          "answer": "C√¢u tr·∫£ l·ªùi",
          "hint": "G·ª£i √Ω...",
          "explanation": "Gi·∫£i th√≠ch chi ti·∫øt..."
        }
      ],
      "num_flashcards": 10
    }
    ```
    """
    if not DOCUMENT_INTELLIGENCE_AVAILABLE or not doc_intelligence_service:
        raise HTTPException(
            status_code=503,
            detail="Document Intelligence service not available. Please install dependencies: pip install pdfplumber PyPDF2 python-docx"
        )
    
    try:
        # Process document
        result = doc_intelligence_service.process_document_to_flashcards(
            file_path=request.file_path,
            num_cards=request.num_cards,
            difficulty=request.difficulty,
            include_summary=request.include_summary
        )
        
        if not result.get("success"):
            raise HTTPException(status_code=400, detail=result.get("error", "Unknown error"))
        
        return result
        
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing document: {str(e)}")

@app.post("/api/documents/text-to-flashcards", tags=["Document Intelligence"])
async def text_to_flashcards(request: DocumentTextRequest):
    """
    üìù Text ‚Üí AI t·∫°o Flashcards
    
    Paste text tr·ª±c ti·∫øp, AI s·∫Ω t·∫°o flashcards
    
    **Parameters:**
    - text: N·ªôi dung c·∫ßn t·∫°o flashcards
    - num_cards: S·ªë l∆∞·ª£ng flashcards
    - difficulty: ƒê·ªô kh√≥ (easy/medium/hard)
    
    **Use cases:**
    - Copy-paste t·ª´ lecture slides
    - Paste t·ª´ website/blog
    - Nh·∫≠p text t·ª± vi·∫øt
    """
    if not DOCUMENT_INTELLIGENCE_AVAILABLE or not doc_intelligence_service:
        raise HTTPException(
            status_code=503,
            detail="Document Intelligence service not available"
        )
    
    try:
        # Validate text length
        if len(request.text) < 50:
            raise HTTPException(
                status_code=400,
                detail="Text qu√° ng·∫Øn. C·∫ßn √≠t nh·∫•t 50 k√Ω t·ª± ƒë·ªÉ t·∫°o flashcards."
            )
        
        # Generate flashcards from text
        flashcards = doc_intelligence_service.generate_flashcards_from_text(
            text=request.text,
            num_cards=request.num_cards,
            difficulty=request.difficulty
        )
        
        if not flashcards:
            raise HTTPException(
                status_code=500,
                detail="Kh√¥ng th·ªÉ t·∫°o flashcards. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c thay ƒë·ªïi n·ªôi dung."
            )
        
        return {
            "success": True,
            "text_length": len(request.text),
            "flashcards": flashcards,
            "num_flashcards": len(flashcards)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.post("/api/documents/summarize", tags=["Document Intelligence"])
async def summarize_document(request: ProcessDocumentRequest):
    """
    üìÑ T√≥m t·∫Øt Document (PDF/DOCX/TXT)
    
    Upload file, AI s·∫Ω t√≥m t·∫Øt n·ªôi dung ch√≠nh
    """
    if not DOCUMENT_INTELLIGENCE_AVAILABLE or not doc_intelligence_service:
        raise HTTPException(
            status_code=503,
            detail="Document Intelligence service not available"
        )
    
    try:
        # Extract text
        text = doc_intelligence_service.extract_text(request.file_path)
        
        if not text or len(text) < 100:
            raise HTTPException(
                status_code=400,
                detail="Document qu√° ng·∫Øn ho·∫∑c kh√¥ng c√≥ n·ªôi dung vƒÉn b·∫£n"
            )
        
        # Summarize
        summary = doc_intelligence_service.summarize_document(text, max_length=500)
        
        return {
            "success": True,
            "file_name": Path(request.file_path).name,
            "original_length": len(text),
            "summary": summary,
            "summary_length": len(summary)
        }
        
    except FileNotFoundError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.get("/api/documents/capabilities", tags=["Document Intelligence"])
async def get_document_capabilities():
    """
    ‚ÑπÔ∏è Ki·ªÉm tra kh·∫£ nƒÉng x·ª≠ l√Ω documents
    
    Returns th√¥ng tin v·ªÅ c√°c lo·∫°i file ƒë∆∞·ª£c h·ªó tr·ª£
    """
    capabilities = {
        "service_available": DOCUMENT_INTELLIGENCE_AVAILABLE and doc_intelligence_service is not None,
        "supported_formats": [],
        "features": []
    }
    
    if DOCUMENT_INTELLIGENCE_AVAILABLE:
        try:
            from document_intelligence_service import (
                PDFPLUMBER_AVAILABLE,
                PYPDF2_AVAILABLE,
                DOCX_AVAILABLE,
                OCR_AVAILABLE
            )
            
            if PDFPLUMBER_AVAILABLE or PYPDF2_AVAILABLE:
                capabilities["supported_formats"].append("PDF (.pdf)")
            if DOCX_AVAILABLE:
                capabilities["supported_formats"].append("Word (.docx)")
            if OCR_AVAILABLE:
                capabilities["supported_formats"].append("Images (.png, .jpg, .jpeg) with OCR")
            
            capabilities["supported_formats"].append("Text (.txt)")
            
            capabilities["features"] = [
                "Auto-generate flashcards from documents",
                "Document summarization",
                "Key concepts extraction",
                "Text extraction from multiple formats"
            ]
            
            if OCR_AVAILABLE:
                capabilities["features"].append("OCR for scanned documents/images")
            
        except ImportError:
            pass
    
    return capabilities

# ============================================================================
# LANGCHAIN AGENT ENDPOINTS
# ============================================================================

class LangChainChatRequest(BaseModel):
    """Request model for LangChain agent chat"""
    message: str
    user_id: Optional[int] = None
    reset_memory: bool = False
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "message": "Xin ch√†o!",
                "user_id": 1,
                "reset_memory": False
            }
        }
    )

class LangChainChatResponse(BaseModel):
    """Response model for LangChain agent"""
    success: bool
    response: str
    agent_type: str = "langchain_simple"
    error: Optional[str] = None

@app.post("/api/chat/langchain", response_model=LangChainChatResponse, tags=["LangChain Agent"])
async def chat_with_langchain_agent(
    request: LangChainChatRequest,
    authorization: Optional[str] = Header(None)
):
    """Chat v·ªõi LangChain AI Agent"""
    
    if not LANGCHAIN_AGENT_AVAILABLE or not langchain_agent:
        raise HTTPException(status_code=503, detail="LangChain Agent not available")
    
    try:
        user_id = request.user_id
        if not user_id and authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            user_id = get_user_id_from_token(token)
        
        if request.reset_memory:
            langchain_agent.reset_memory()
        
        result = langchain_agent.chat(message=request.message, user_id=user_id)
        return LangChainChatResponse(**result)
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

@app.post("/api/chat/langchain/reset", tags=["LangChain Agent"])
async def reset_langchain_memory():
    """Reset memory"""
    if not LANGCHAIN_AGENT_AVAILABLE or not langchain_agent:
        raise HTTPException(status_code=503, detail="Not available")
    
    langchain_agent.reset_memory()
    return {"success": True, "message": "Memory reset"}

@app.get("/api/chat/langchain/status", tags=["LangChain Agent"])
async def get_langchain_status():
    """Check status"""
    if not LANGCHAIN_AGENT_AVAILABLE or not langchain_agent:
        return {"available": False}
    
    return {
        "available": True,
        "memory_enabled": True,
        "llm_model": "gemini-2.0-flash-exp"
    }

# ============================================================================
# CALENDAR SYNC ENDPOINTS
# ============================================================================

class CalendarSyncRequest(BaseModel):
    """Request model for syncing schedule to calendar"""
    week: Optional[int] = None
    hoc_ky: Optional[str] = None
    user_id: Optional[int] = None  # User ID ƒë·ªÉ l·∫•y credentials v√† t·∫°o events
    reminder_email: Optional[int] = None  # Ph√∫t tr∆∞·ªõc ƒë·ªÉ g·ª≠i email (vd: 30, 60, 1440)
    reminder_popup: Optional[int] = None  # Ph√∫t tr∆∞·ªõc ƒë·ªÉ hi·ªán popup
    notification_email: Optional[str] = None  # Email t√πy ch·ªânh ƒë·ªÉ nh·∫≠n th√¥ng b√°o
    
    model_config = ConfigDict(
        json_schema_extra={
            "example": {
                "week": 5,
                "hoc_ky": "20251",
                "user_id": 1,
                "reminder_email": 30,
                "reminder_popup": 15,
                "notification_email": "myemail@gmail.com"
            }
        }
    )

@app.post("/api/calendar/sync-schedule", tags=["Calendar Sync"])
async def sync_schedule_to_calendar(
    request: CalendarSyncRequest,
    authorization: Optional[str] = Header(None)
):
    """
    üîÑ ƒê·ªìng b·ªô Th·ªùi Kh√≥a Bi·ªÉu l√™n Google Calendar
    
    T·ª± ƒë·ªông l·∫•y TKB t·ª´ TVU Portal v√† t·∫°o events tr√™n Google Calendar
    
    **Y√™u c·∫ßu:**
    - ƒê√£ k·∫øt n·ªëi Google Account (OAuth)
    - ƒê√£ c·∫•u h√¨nh t√†i kho·∫£n TVU trong Settings
    
    **Parameters:**
    - week: Tu·∫ßn h·ªçc (optional, m·∫∑c ƒë·ªãnh tu·∫ßn hi·ªán t·∫°i)
    - hoc_ky: H·ªçc k·ª≥ (optional, m·∫∑c ƒë·ªãnh h·ªçc k·ª≥ hi·ªán t·∫°i)
    - user_id: User ID (optional, n·∫øu kh√¥ng c√≥ s·∫Ω l·∫•y t·ª´ token)
    
    **Returns:**
    - success: True/False
    - message: Th√¥ng b√°o k·∫øt qu·∫£
    - events_created: S·ªë events ƒë√£ t·∫°o
    """
    if not AGENT_FEATURES_AVAILABLE or not agent_features:
        raise HTTPException(
            status_code=503,
            detail="Agent features not available"
        )
    
    try:
        # Get user_id - t·ª´ request body ho·∫∑c t·ª´ token
        user_id = request.user_id
        token = None
        
        # N·∫øu c√≥ authorization header, l·∫•y token v√† user_id t·ª´ ƒë√≥
        if authorization and authorization.startswith("Bearer "):
            token = authorization.replace("Bearer ", "")
            if not user_id:
                user_id = get_user_id_from_token(token)
        
        # N·∫øu v·∫´n kh√¥ng c√≥ user_id, b√°o l·ªói
        if not user_id:
            raise HTTPException(
                status_code=400,
                detail="user_id is required - please provide in request body or login"
            )
        
        print(f"üîÑ Syncing schedule for user_id: {user_id}")
        
        # Call sync function - truy·ªÅn user_id ƒë·ªÉ l·∫•y credentials
        result = agent_features.sync_schedule_to_calendar(
            token=token or "",  # Token c√≥ th·ªÉ r·ªóng, function s·∫Ω d√πng user_id
            user_id=user_id,
            week=request.week,
            hoc_ky=request.hoc_ky,
            reminder_email=request.reminder_email,
            reminder_popup=request.reminder_popup,
            notification_email=request.notification_email
        )
        
        if result.get("success"):
            return result
        else:
            raise HTTPException(
                status_code=400,
                detail=result.get("message", "Sync failed")
            )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    print("=" * 60)
    print("üöÄ Starting AI Chat Service with RAG")
    print("=" * 60)
    print(f"üìç Server: http://localhost:{port}")
    print(f"üìö Swagger UI: http://localhost:{port}/docs")
    print(f"üìä Vector DB Documents: {vector_db.get_count()}")
    if CREDENTIAL_API_AVAILABLE:
        print(f"üîê Credential Manager: Enabled")
    print("=" * 60)
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info"
    )
