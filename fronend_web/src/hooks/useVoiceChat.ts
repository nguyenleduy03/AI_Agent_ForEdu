/**
 * Custom Hook for Voice Chat
 * - Speech-to-Text (ghi Ã¢m giá»ng nÃ³i)
 * - Text-to-Speech (Ä‘á»c response cá»§a AI)
 */
import { useState, useEffect, useRef } from 'react';
import toast from 'react-hot-toast';

interface UseVoiceChatProps {
  onTranscript: (text: string) => void;
  language?: string;
}

interface UseVoiceChatReturn {
  isListening: boolean;
  isSpeaking: boolean;
  isSupported: boolean;
  startListening: () => void;
  stopListening: () => void;
  speak: (text: string) => void;
  stopSpeaking: () => void;
  transcript: string;
}

export const useVoiceChat = ({ 
  onTranscript, 
  language = 'vi-VN' 
}: UseVoiceChatProps): UseVoiceChatReturn => {
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [isSupported, setIsSupported] = useState(false);
  
  const recognitionRef = useRef<any>(null);
  const synthRef = useRef<SpeechSynthesis | null>(null);

  // Check browser support
  useEffect(() => {
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
    const speechSynthesis = window.speechSynthesis;
    
    if (SpeechRecognition && speechSynthesis) {
      setIsSupported(true);
      
      // Initialize Speech Recognition
      const recognition = new SpeechRecognition();
      recognition.continuous = false; // Stop after one sentence
      recognition.interimResults = true; // Show interim results
      recognition.lang = language;
      
      recognition.onstart = () => {
        setIsListening(true);
        console.log('ðŸŽ¤ Voice recognition started');
      };
      
      recognition.onresult = (event: any) => {
        const current = event.resultIndex;
        const transcriptText = event.results[current][0].transcript;
        setTranscript(transcriptText);
        
        // If final result, send to parent
        if (event.results[current].isFinal) {
          console.log('âœ… Final transcript:', transcriptText);
          onTranscript(transcriptText);
        }
      };
      
      recognition.onerror = (event: any) => {
        console.error('âŒ Speech recognition error:', event.error);
        setIsListening(false);
        
        if (event.error === 'no-speech') {
          toast.error('KhÃ´ng nghe tháº¥y giá»ng nÃ³i. HÃ£y thá»­ láº¡i!');
        } else if (event.error === 'not-allowed') {
          toast.error('Vui lÃ²ng cho phÃ©p truy cáº­p microphone!');
        } else {
          toast.error(`Lá»—i: ${event.error}`);
        }
      };
      
      recognition.onend = () => {
        setIsListening(false);
        console.log('ðŸ›‘ Voice recognition ended');
      };
      
      recognitionRef.current = recognition;
      synthRef.current = speechSynthesis;
    } else {
      setIsSupported(false);
      console.warn('âš ï¸  Browser khÃ´ng há»— trá»£ Web Speech API');
    }
    
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      if (synthRef.current) {
        synthRef.current.cancel();
      }
    };
  }, [language, onTranscript]);

  const startListening = () => {
    if (!isSupported) {
      toast.error('TrÃ¬nh duyá»‡t khÃ´ng há»— trá»£ ghi Ã¢m giá»ng nÃ³i!');
      return;
    }
    
    if (recognitionRef.current && !isListening) {
      setTranscript('');
      recognitionRef.current.start();
      toast.success('ðŸŽ¤ Äang nghe... HÃ£y nÃ³i!', { duration: 2000 });
    }
  };

  const stopListening = () => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop();
    }
  };

  const speak = (text: string) => {
    if (!isSupported || !synthRef.current) {
      console.warn('Text-to-Speech khÃ´ng kháº£ dá»¥ng');
      return;
    }
    
    // Cancel any ongoing speech
    synthRef.current.cancel();
    
    // Create utterance
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = language;
    utterance.rate = 1.0; // Speed
    utterance.pitch = 1.0; // Pitch
    utterance.volume = 1.0; // Volume
    
    utterance.onstart = () => {
      setIsSpeaking(true);
      console.log('ðŸ”Š Speaking started');
    };
    
    utterance.onend = () => {
      setIsSpeaking(false);
      console.log('ðŸ”‡ Speaking ended');
    };
    
    utterance.onerror = (event) => {
      console.error('âŒ Speech synthesis error:', event);
      setIsSpeaking(false);
    };
    
    synthRef.current.speak(utterance);
  };

  const stopSpeaking = () => {
    if (synthRef.current) {
      synthRef.current.cancel();
      setIsSpeaking(false);
    }
  };

  return {
    isListening,
    isSpeaking,
    isSupported,
    startListening,
    stopListening,
    speak,
    stopSpeaking,
    transcript,
  };
};
